{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LSPH515 : M\u00e9thodes num\u00e9riques Cours con\u00e7u pour les \u00e9tudiants de L3 de l'Universit\u00e9 de Versailles Saint-Quentin (UVSQ) Pr\u00e9sentation du cours En Physique, les probl\u00e8mes \u00e0 r\u00e9soudre ne peuvent pas toujours l'\u00eatre de mani\u00e8re analytique. Dans les cas o\u00f9 une r\u00e9solution analytique est difficile, voire impossible, on approximera la solution \u00e0 l'aide de m\u00e9thodes num\u00e9riques . On parle alors de \"r\u00e9solution num\u00e9rique\" d'un probl\u00e8me. Une m\u00e9thode num\u00e9rique est une suite de proc\u00e9dures calculatoires ou d' algorithmes . Vous trouverez dans ce cours des m\u00e9thodes num\u00e9riques pour r\u00e9soudre diff\u00e9rent probl\u00e8mes classiques en Physique : Chapitre I : Introduction : une introduction aux m\u00e9thodes num\u00e9riques. Chapitre II : Recherche de racines : un panel de m\u00e9thodes pour approximer les racines d'une fonction. Chapitre III : Interpolation polynomiale : un panel de m\u00e9thodes pour interpoler des donn\u00e9es discr\u00e8tes \u00e0 l'aide d'un polyn\u00f4me. Chapitre IV : Int\u00e9gration num\u00e9rique : un panel de m\u00e9thodes pour approximer l'int\u00e9grale d'une fonction. Chapitre V : Syst\u00e8mes lin\u00e9aires : un panel de m\u00e9thodes pour approximer la solution d'un syst\u00e8me lin\u00e9aire d'\u00e9quations. L'objectif est qu'\u00e0 la fin de ce cours vous soyez capables de : Choisir une m\u00e9thode num\u00e9rique adapt\u00e9e \u00e0 un probl\u00e8me donn\u00e9, en se basant sur les hypoth\u00e8ses d' applicabilit\u00e9 des m\u00e9thodes, ainsi que sur leurs performances (simplicit\u00e9, vitesse, stabilit\u00e9, co\u00fbt en m\u00e9moire, etc.). Impl\u00e9menter les diff\u00e9rents algorithmes sous la forme de codes Python , avec des adaptations si besoin. Appliquer les m\u00e9thodes num\u00e9riques \u00e0 un probl\u00e8me donn\u00e9, en choisissant pertinemment les param\u00e8tres d'entr\u00e9e de l'algorithme (valeurs initiales, conditions d'arr\u00eat, etc.). Chaque chapitre sera accompagn\u00e9 d'un probl\u00e8me exemple , auquel seront appliqu\u00e9es les diff\u00e9rentes m\u00e9thodes vues dans ce cours. Credits \u00a9 Nicolas OUDART & Aymeric CHAZOTTES Remerciements \u00e0 Alice LE GALL","title":"Accueil"},{"location":"#lsph515-methodes-numeriques","text":"Cours con\u00e7u pour les \u00e9tudiants de L3 de l'Universit\u00e9 de Versailles Saint-Quentin (UVSQ)","title":"LSPH515 : M\u00e9thodes num\u00e9riques"},{"location":"#presentation-du-cours","text":"En Physique, les probl\u00e8mes \u00e0 r\u00e9soudre ne peuvent pas toujours l'\u00eatre de mani\u00e8re analytique. Dans les cas o\u00f9 une r\u00e9solution analytique est difficile, voire impossible, on approximera la solution \u00e0 l'aide de m\u00e9thodes num\u00e9riques . On parle alors de \"r\u00e9solution num\u00e9rique\" d'un probl\u00e8me. Une m\u00e9thode num\u00e9rique est une suite de proc\u00e9dures calculatoires ou d' algorithmes . Vous trouverez dans ce cours des m\u00e9thodes num\u00e9riques pour r\u00e9soudre diff\u00e9rent probl\u00e8mes classiques en Physique : Chapitre I : Introduction : une introduction aux m\u00e9thodes num\u00e9riques. Chapitre II : Recherche de racines : un panel de m\u00e9thodes pour approximer les racines d'une fonction. Chapitre III : Interpolation polynomiale : un panel de m\u00e9thodes pour interpoler des donn\u00e9es discr\u00e8tes \u00e0 l'aide d'un polyn\u00f4me. Chapitre IV : Int\u00e9gration num\u00e9rique : un panel de m\u00e9thodes pour approximer l'int\u00e9grale d'une fonction. Chapitre V : Syst\u00e8mes lin\u00e9aires : un panel de m\u00e9thodes pour approximer la solution d'un syst\u00e8me lin\u00e9aire d'\u00e9quations. L'objectif est qu'\u00e0 la fin de ce cours vous soyez capables de : Choisir une m\u00e9thode num\u00e9rique adapt\u00e9e \u00e0 un probl\u00e8me donn\u00e9, en se basant sur les hypoth\u00e8ses d' applicabilit\u00e9 des m\u00e9thodes, ainsi que sur leurs performances (simplicit\u00e9, vitesse, stabilit\u00e9, co\u00fbt en m\u00e9moire, etc.). Impl\u00e9menter les diff\u00e9rents algorithmes sous la forme de codes Python , avec des adaptations si besoin. Appliquer les m\u00e9thodes num\u00e9riques \u00e0 un probl\u00e8me donn\u00e9, en choisissant pertinemment les param\u00e8tres d'entr\u00e9e de l'algorithme (valeurs initiales, conditions d'arr\u00eat, etc.). Chaque chapitre sera accompagn\u00e9 d'un probl\u00e8me exemple , auquel seront appliqu\u00e9es les diff\u00e9rentes m\u00e9thodes vues dans ce cours.","title":"Pr\u00e9sentation du cours"},{"location":"#credits","text":"\u00a9 Nicolas OUDART & Aymeric CHAZOTTES Remerciements \u00e0 Alice LE GALL","title":"Credits"},{"location":"Chap1_Introduction/","text":"Chapitre I : Introduction aux m\u00e9thodes num\u00e9riques Ce chapitre est une introduction aux enjeux des m\u00e9thodes num\u00e9riques. Dans le c\u00e9l\u00e8bre roman de science-fiction humoristique anglais \"The Hitchhiker's Guide to the Galaxy\", \u00e9crit en 1978 par Douglas Adams, des extra-terrestres construisent un superordinateur du nom de \"Deep Thought\" et lui posent le probl\u00e8me suivant : \"O Deep Thought computer, [...] the task we have designed you to perform is this. We want you to tell us the Answer [...] to Life, the Universe and Everything\". L'ordinateur leur r\u00e9pond alors : \"I can do it [...] but I'll have to think about it. [...] Seven and half million years.\" 7,5 millions d'ann\u00e9es plus tard, les descendants des cr\u00e9ateurs de Deep Thought viennent consulter l'ordinateur pour obtenir leur r\u00e9ponse. Deep Thought dit qu'il a bien la r\u00e9ponse, mais les pr\u00e9vient : \"I don't think [...] that you're gonna like it\". Les extra-terrestres insistent pour avoir la r\u00e9ponse. Deep Thought s'ex\u00e9cute : \"The Answer to the Great Question [...] of Life, the Universe and Everything [...] is ... 42\". Les extra-terrestres sont en col\u00e8re face \u00e0 cette r\u00e9ponse, et demandent \u00e0 l'ordinateur s'il est bien s\u00fbr qu'il n'y a pas une erreur. Ce \u00e0 quoi Deep Thought r\u00e9pond qu'il est certain que la r\u00e9ponse est correcte, et que s'ils ne l'aiment pas, c'est parce que leur probl\u00e8me \u00e9tait mal pos\u00e9. Cette blague aujourd'hui pass\u00e9e \u00e0 la post\u00e9rit\u00e9 fait \u00e9cho aux enjeux des m\u00e9thodes num\u00e9riques que nous allons voir dans ce chapitre... Nous allons d\u00e9tailler dans ce chapitre les diff\u00e9rentes \u00e9tapes de r\u00e9solution d'un probl\u00e8me en Physique, avec un exemple simple. Le probl\u00e8me Un groupe d'\u00e9tudiants de l'UVSQ r\u00e9alise un projet de ballon sonde pour mesurer la vitesse du vent dans la stratosph\u00e8re au cours du temps . On consid\u00e8re qu'apr\u00e8s une phase d'ascension, leur ballon a atteint une altitude stable dans la stratosph\u00e8re, et que son d\u00e9placement est uniquement li\u00e9 au vent . La nacelle de leur ballon contient une balise GPS, qui leur permet de mesurer le d\u00e9placement du ballon au cours du temps . Le probl\u00e8me physique auquel les \u00e9tudiants sont confront\u00e9s est le suivant : estimer la vitesse du vent dans la stratosph\u00e8re au cours du temps \u00e0 partir de leurs mesures. Mod\u00e9lisation du probl\u00e8me La premi\u00e8re \u00e9tape est de traduire ce probl\u00e8me physique en un mod\u00e8le par le biais d'\u00e9quations math\u00e9matiques : Les \u00e9tudiants font l'hypoth\u00e8se que leur ballon stratosph\u00e9rique se d\u00e9place \u00e0 la vitesse du vent . Notons \\(p(t)\\) la fonction associant \u00e0 chaque instant \\(t\\) en [s] \u00e0 partir du d\u00e9but de la mesure le d\u00e9placement du ballon en [m]. La traduction math\u00e9matique du probl\u00e8me est donc que la vitesse du vent au cours du temps \\(W(t)\\) est li\u00e9e \u00e0 \\(p(t)\\) par l'\u00e9quation : \\(W(t) = \\frac{d}{dt} p(t)\\) Comme la fonction \\(p(t)\\) n'a pas d'expression analytique connue (il s'agit d'une mesure d'un capteur), les \u00e9tudiants devront utiliser une m\u00e9thode num\u00e9rique pour estimer la d\u00e9riv\u00e9e de \\(p(t)\\) , et donc la solution du probl\u00e8me. Discr\u00e9tisation du probl\u00e8me Un ordinateur ne pouvant g\u00e9rer des objets continus, l'application d'une m\u00e9thode num\u00e9rique n\u00e9cessite une discr\u00e9tisation . Dans le cas des \u00e9tudiants de l'UVSQ, la discr\u00e9tisation est r\u00e9alis\u00e9e au moment de l'\u00e9chantillonnage : Les \u00e9tudiants d\u00e9cident d'enregistrer une mesure de d\u00e9placement toutes les \\(h\\) secondes. Ils n'ont donc pas acc\u00e8s \u00e0 toutes les valeurs possibles de \\(p(t)\\) , mais \u00e0 des valeurs discr\u00e8tes r\u00e9guli\u00e8rement espac\u00e9es \\(p(t_i)\\) avec \\(t_i = 0, h, 2 \\times h, 3 \\times h, ... (N-1) \\times h\\) et \\(N\\) le nombre d'\u00e9chantillons. De m\u00eame, ils n'estimeront pas toutes les valeurs de \\(W(t)\\) , mais des valeurs discr\u00e8tes \\(W(t_i)\\) . On nomme \\(h\\) le pas de discr\u00e9tisation du probl\u00e8me. La solution existe-t-elle ? Est-elle unique ? Il faut toujours se poser ces questions avant d'essayer de r\u00e9soudre num\u00e9riquement un probl\u00e8me. Pour notre exemple, nous partirons du principe que la trajectoire de la nacelle \\(p(t)\\) est d\u00e9rivable, et donc que l' existence et l' unicit\u00e9 de la solution est triviale. Mais ce n'est pas toujours le cas . Par exemple, pour un probl\u00e8me impliquant la recherche de la racine d'une fonction sur un intervalle, nous verrons que l'existence et l'unicit\u00e9 ne sont pas \u00e9videntes et doivent \u00eatre d\u00e9montr\u00e9es. Choix d'une m\u00e9thode num\u00e9rique Nos \u00e9tudiants de l'UVSQ on besoin ici d'une m\u00e9thode num\u00e9rique d'estimation de la d\u00e9riv\u00e9e d'une fonction. Pour approcher la d\u00e9riv\u00e9e ils d\u00e9cident d'employer une m\u00e9thode de \" diff\u00e9rences d\u00e9centr\u00e9es \u00e0 droite \" : \\(W(t_i) = \\frac{d}{dt} p(t_i) \\approx \\frac{p(t_{i+1})-p(t_i)}{h}\\) avec \\(t_{i+1} = t_i + h\\) et \\(i = 0, 1, ..., N-2\\) Lorsque l'on choisi une m\u00e9thode num\u00e9rique pour r\u00e9pondre \u00e0 un probl\u00e8me, il convient de se poser les questions suivantes. La m\u00e9thode est-elle applicable ? Chaque m\u00e9thode num\u00e9rique a des conditions d' applicabilit\u00e9 , qui ne sont pas n\u00e9cessairement les m\u00eames pour un type de probl\u00e8me donn\u00e9. Dans notre exemple, il faut que la fonction \\(p(t)\\) soit doublement d\u00e9rivable sur chaque intervalle \\([t_i,t_{i+1}]\\) . Quelles sont les sources d'erreur, et quelle est l'erreur attendue ? La r\u00e9solution num\u00e9rique d'un probl\u00e8me physique est toujours entach\u00e9e d'erreurs . On appelle pr\u00e9cision d'une m\u00e9thode num\u00e9rique l'erreur entre la solution donn\u00e9e par la m\u00e9thode et la solution exacte du probl\u00e8me. Pour estimer cette pr\u00e9cision, on introduit les notions d'erreur absolue et d'erreur relative . Soit \\(\\hat{x}\\) la solution approch\u00e9e d'une valeure r\u00e9elle exacte \\(x\\) : L' erreur absolue est d\u00e9finie par \\(\\mid x - \\hat{x} \\mid\\) . L' erreur relative est d\u00e9finie par \\(\\frac{\\mid x - \\hat{x} \\mid}{\\mid x \\mid}\\) . (Il est \u00e0 noter que ces notions sont g\u00e9n\u00e9ralisables \u00e0 des vecteurs ou des matrices en rempla\u00e7ant la valeur absolue par une norme vectorielle / matricielle). En pratique, il est cependant difficile d'\u00e9valuer l'erreur absolue / relative, puisque la solution exacte est par d\u00e9finition l'inconnue que nous voulons approcher. Dans notre exemple, les \u00e9tudiants de l'UVSQ ne connaissent pas la valeur exacte de la vitesse du vent. On peut toutefois essayer de borner l'erreur . Les erreurs lors de la r\u00e9solution num\u00e9rique d'un probl\u00e8me ont diff\u00e9rentes sources qu'il convient d'identifier. Certaines sont li\u00e9es \u00e0 la m\u00e9thode choisie, d'autres non . Erreurs de mod\u00e9lisation et de donn\u00e9es Une partie de l'erreur sur la solution d'un probl\u00e8me physique provient toujours du mod\u00e8le : Pour commencer, tout mod\u00e8le fait des hypoth\u00e8ses simplificatrices, n\u00e9gligeant certains ph\u00e9nom\u00e8nes physiques. Ensuite, notre mod\u00e8le peut aussi avoir un domaine de validit\u00e9 limit\u00e9, qui ne soit pas respect\u00e9 pendant une partie de l'\u00e9tude. Et enfin, si notre mod\u00e8le s'appuie sur des donn\u00e9es mesur\u00e9es, ou sur des param\u00e8tres physiques estim\u00e9s, alors ces valeurs seront forc\u00e9ment inexactes. Dans le cas du projet de ballon des \u00e9tudiants de l'UVSQ, nous avons : L'hypoth\u00e8se simplificatrice que le mouvement du ballon n'est influenc\u00e9 que par le vent, et que la vitesse du ballon est directement \u00e9gale \u00e0 la vitesse du vent. Le validit\u00e9 de notre mod\u00e8le se limitant probablement \u00e0 des vents faibles aux variations lentes (pas des rafales). Les mesures du d\u00e9placement du ballon par GPS, qui seront entach\u00e9es d'erreurs de mesures. Erreurs de troncature La discr\u00e9tisation d'un probl\u00e8me physique par une m\u00e9thode num\u00e9rique induit n\u00e9cessairement des erreurs. Par exemple, la troncature d'une s\u00e9rie infinie convergeant vers la solution, ou l'arr\u00eat au bout d'un nombre d'it\u00e9rations finies d'une suite convergeant vers la solution, sont in\u00e9vitables. En effet, un ordinateur ne peut effectuer qu'un nombre fini d'op\u00e9rations . On parle alors d' erreurs de troncature . Il s'agit donc ici d'erreurs directement li\u00e9es \u00e0 la m\u00e9thode choisie. La m\u00e9thode choisie par nos \u00e9tudiants de l'UVSQ est bas\u00e9e sur le d\u00e9veloppement de Taylor d'ordre 2 suivant : \\(\\frac{d}{dt} p(t_i) = \\frac{p(t_i+h)-p(t_i)}{h} - \\frac{h}{2} \\frac{d^2}{dt^2} p(\\tau)\\) avec \\(\\tau \\in [t_i,t_i+h]\\) Dans ce cas, l'erreur de troncature est donc : \\(\\frac{h}{2} \\mid \\frac{d^2}{dt^2} p(\\tau) \\mid\\) . On note alors qu'il y a 2 moyens de r\u00e9duire cette erreur : Diminuer le pas de discr\u00e9tisation \\(h\\) (dans notre cas, augmenter la fr\u00e9quence d'\u00e9chantillonnage). Modifier la m\u00e9thode en r\u00e9alisant un d\u00e9veloppement de Taylor d'ordre sup\u00e9rieur. Erreurs d'arrondi Un ordinateur ne peut manipuler que des nombres en pr\u00e9cision finie : il y a un nombre fini de r\u00e9els qui peuvent \u00eatre repr\u00e9sent\u00e9s par la machine. Il y a donc un arrondi sur toutes les valeurs repr\u00e9sent\u00e9es par un ordinateur. On parle alors d' erreurs d'arrondi . Il s'agit donc ici d'erreurs directement li\u00e9es \u00e0 la pr\u00e9cision de la machine. Ces erreurs se propagent \u00e0 mesure que l'on applique des op\u00e9rations lors de la r\u00e9solution d'un probl\u00e8me num\u00e9rique. Si nos \u00e9tudiants de l'UVSQ choisissent une machine repr\u00e9sentant les r\u00e9els avec une pr\u00e9cision \\(\\delta\\) , alors : Pour chaque \\(t_i\\) l'\u00e9valuation de \\(p(t_i)\\) sera connue avec une pr\u00e9cision \\(\\delta\\) . Donc l'\u00e9valuation de \\(p(t_i+h)-p(t_i)\\) sera connue avec une pr\u00e9cision \\(2 \\delta\\) . Et par cons\u00e9quent l'\u00e9valuation de \\(\\frac{p(t_i+h)-p(t_i)}{h}\\) sera connue avec une pr\u00e9cision \\(\\frac{2 \\delta}{h}\\) . On en d\u00e9duit que l'erreur d'arrondi dans notre exemple est : \\(2 \\delta \\frac{p(t_i)}{h}\\) . On remarque que l'erreur d'arrondi sur \\(p(t_i)\\) a \u00e9t\u00e9 propag\u00e9e par les diff\u00e9rentes op\u00e9rations, avec en particulier un facteur 2 d\u00fb \u00e0 l'op\u00e9ration de soustraction. Nota Bene On observe que l'erreur d'arrondi est inversement proportionnelle \u00e0 \\(h\\) , alors que l'erreur de troncature est proportionnelle \u00e0 \\(h\\) . Le choix du pas de discr\u00e9tisation \\(h\\) a donc des effets antagonistes sur ces 2 erreurs : un compromis est n\u00e9cessaire. Dans le cas de notre exemple, on peut montrer que la valeur de \\(h\\) minimisant la somme de ces 2 erreurs est : \\(h = 2 \\sqrt{\\delta \\mid \\frac{p(t_i)}{\\frac{d^2}{dt^2} p(\\tau)} \\mid}\\) avec \\(\\tau \\in [t_i,t_i+h]\\) . Erreurs de repr\u00e9sentation des nombres Afin de stocker et manipuler des nombres, une machine va g\u00e9n\u00e9ralement les repr\u00e9senter en binaire (dans la base de 2). Les nombres sont physiquement rang\u00e9s en m\u00e9moire dans un nombre pr\u00e9d\u00e9fini de cellules-m\u00e9moires appel\u00e9es bits (\"binary digits\"). Un bit ne peut prendre que 2 valeurs : 0 ou 1 . Les nombres sont alors repr\u00e9sent\u00e9s par une combinaison binaire de 0 et de 1. Nous allons voir que cette repr\u00e9sentation des nombres par la machine peut aussi \u00eatre source d'erreurs en analyse num\u00e9rique. Repr\u00e9sentation des entiers : Un entier naturel \\(n\\) est repr\u00e9sent\u00e9 en binaire par : \\(n = a_{p-1} 2^{p-1} + a_{p-2} 2^{p-2} + ... + a_1 2^1 + a_0 2^0\\) avec \\(a_i\\) pour \\(0 \\leq i < p\\) les \\(p\\) bits, \u00e9gaux \u00e0 0 ou 1. \\(p\\) bits permettent alors de repr\u00e9senter exactement les entiers naturels entre \\(0\\) et \\(2^p-1\\) . Des op\u00e9rations \u00e9l\u00e9mentaires telles que l'addition peuvent \u00eatre appliqu\u00e9es sur le binaires bit \u00e0 bit . Repr\u00e9sentation des entiers sign\u00e9s : Pour repr\u00e9senter les entiers sign\u00e9s, on veut : (1) pouvoir identifier le signe avec un bit d\u00e9di\u00e9 nomm\u00e9 bit de poids fort , (2) que les r\u00e8gles d'addition soient toujours valides. La repr\u00e9sentation couramment utilis\u00e9e est celle du compl\u00e9ment \u00e0 2 : on code un nombre n\u00e9gatif en binaire par \\((2^p - |x|)_2\\) . Avec cette convention, un entier sign\u00e9 \\(n\\) est repr\u00e9sent\u00e9 par : \\(n = -a_{p-1} 2^{p-1} + a_{p-2} 2^{p-2} + ... + a_1 2^1 + a_0 2^0\\) avec \\(a_i\\) pour \\(0 \\leq i < p\\) les \\(p\\) bits, \u00e9gaux \u00e0 0 ou 1. \\(p\\) bits permettent alors de repr\u00e9senter exactement les entiers sign\u00e9s entre \\(-2^{p-1}\\) et \\(2^{p-1}-1\\) . D\u00e9passement de capacit\u00e9 (overflow) : Soient 2 entiers sign\u00e9s \\(n_1\\) et \\(n_2\\) repr\u00e9sent\u00e9s en format compl\u00e9ment \u00e0 2 sur \\(p\\) bits : Si \\(n_1 + n_2 \\geq 2^{p-1}\\) alors il y a un d\u00e9passement de capacit\u00e9 positif . Si \\(n_1 + n_2 < -2^{p-1}\\) alors il y a un d\u00e9passement de capacit\u00e9 n\u00e9gatif . On parle en anglais de probl\u00e8me d' overflow . Les op\u00e9rations arithm\u00e9tiques sur des entiers s'effectuent donc exactement, mais \u00e0 condition que le r\u00e9sultat soit repr\u00e9sentable par la machine . Sinon, la valeur de sortie sera \u00e9rron\u00e9e. Nota Bene - Lorsque \\(p=8\\) on parle d' octet . - Lorsque \\(p=16\\) on parle de simple pr\u00e9cision . - Lorsque \\(p=32\\) on parle de double pr\u00e9cision . Repr\u00e9sentation des r\u00e9els : Pour repr\u00e9senter des r\u00e9els en machine, on a recourt \u00e0 la convention de la virgule flottante . Avec cette convention, un r\u00e9el \\(r\\) sera repr\u00e9sent\u00e9 par : \\(r = (-1)^s M 2^e\\) avec \\(s\\) le signe, \\(M\\) la mantisse (un r\u00e9el positif), et \\(e\\) l'exposant (un entier sign\u00e9). Suivant le r\u00e9el \u00e0 repr\u00e9senter, et le choix de la mantisse et de l'exposant, des probl\u00e8mes d'arrondi apparaissent . Aussi, il n'y a pas unicit\u00e9 de la repr\u00e9sentation d'un nombre avec cette convention. C'est pourquoi on va fixer en plus des r\u00e8gles sur la mantisse et l'exposant. Norme IEEE 754 et erreurs : La norme IEEE 754 a \u00e9t\u00e9 introduite en 1985 pour standardiser la repr\u00e9sentation des r\u00e9els en virgule flottante. En simple pr\u00e9cision , un r\u00e9el sera repr\u00e9sent\u00e9 sur 32 bits : 1 bit pour le signe , 8 bits pour l' exposant , et 23 bits pour la mantisse . On peut avec cette norme repr\u00e9senter les r\u00e9els compris entre \\(2^{-126}\\) et environ \\(2^{128}\\) . En double pr\u00e9cision , un r\u00e9el sera repr\u00e9sent\u00e9 sur 64 bits : 1 bit pour le signe , 11 bits pour l' exposant , et 52 bits pour la mantisse . On peut avec cette norme repr\u00e9senter les r\u00e9els compris entre \\(2^{-1022}\\) et environ \\(2^{1024}\\) . D\u00e9finition : le epsilon machine On appelle epsilon machine (ou \"macheps\") le nombre positif \\(\\epsilon\\) le plus petit tel que \\(1 + \\epsilon > 1\\) . On peut montrer que l'erreur relative sur la repr\u00e9sentation virgule flottante d'un nombre est major\u00e9e par \\(\\epsilon\\) . Pour la norme IEEE 754 : en simple pr\u00e9cision \\(\\epsilon = 2^{-23}\\) , en double pr\u00e9cision \\(\\epsilon = 2^{-52}\\) . Nota Bene En norme IEEE 754, les situation d'overflow ne provoquent pas d'arr\u00eat des calculs. Il faut donc \u00eatre vigilant, car des valeurs erron\u00e9es seront alors obtenues. Les bonnes pratiques : Lors de la r\u00e9solution de leur probl\u00e8me, on recommande \u00e0 nos \u00e9tudiants de l'UVSQ de suivre les 3 conseils suivants : (1) choisir une pr\u00e9cision pertinente pour la repr\u00e9sentation des entiers et des r\u00e9els, (2) arrondir les nombres au nombre de d\u00e9cimales requis par le calcul, (3) normaliser les valeurs pour \u00e9viter les probl\u00e8mes d'overflow. La m\u00e9thode converge -t-elle vers la solution ? Avec quelle vitesse ? Une m\u00e9thode num\u00e9rique est dites convergente si l'\u00e9cart entre la solution approch\u00e9e et la solution exacte tend vers 0 quand le pas de discr\u00e9tisation \\(h\\) tend vers 0. Si de plus, l'erreur absolue \\(e\\) peut \u00eatre major\u00e9e : \\(e \\leq C h^p\\) avec \\(h\\) le pas de discr\u00e9tisation, \\(p\\) un nombre positif et \\(C\\) une constante alors la m\u00e9thode est dite convergente d'ordre \\(p\\) . Si \\(p = 2, 3\\) ou \\(4\\) , on dit la convergence quadratique, cubique ou quartique. Dans notre exemple, on peut montrer que pour chaque \\(W(t_i)\\) , l'erreur est major\u00e9e par \\(\\frac{h}{2} sup_{t \\in [t_i,t_i+h]} \\mid \\frac{d^2}{dt^2} p(t)\\mid\\) . La m\u00e9thode converge donc vers la solution lorsque que le pas de discr\u00e9tisation \\(h\\) diminue. Cette convergence est \"d'ordre 1\". La solution est-elle stable ? Lors de la r\u00e9solution num\u00e9rique d'un probl\u00e8me, il convient de se poser la question de la stabilit\u00e9 de la solution. Cette notion de stabilit\u00e9 peut s'appliquer \u00e0 3 niveaux : Stabilit\u00e9 du probl\u00e8me physique Certains probl\u00e8mes en Physique sont par nature chaotiques : une petite variation des conditions initiales entraine une variation tellement importante des r\u00e9sultats qu'elle rend toute approximation de la solution impossible. Ce n'est pas le cas du probl\u00e8me auquels nos \u00e9tudiants de l'UVSQ sont confront\u00e9s, mais on peut citer des ph\u00e9nom\u00e8nes chaotiques c\u00e9l\u00e8bres en Physique : le double-pendule, le probl\u00e8me \u00e0 N corps, et la turbulence des fluides. Cette instabilit\u00e9 \u00e9tant directement li\u00e9e au probl\u00e8me, et donc ind\u00e9pendante de nos choix de mod\u00e8le ou de m\u00e9thode, nous n'avons aucun moyen d'y rem\u00e9dier. Stabilit\u00e9 du mod\u00e8le math\u00e9matique Il est possible qu'un mod\u00e8le math\u00e9matique soit mal conditionn\u00e9 : une petite variation des entr\u00e9es ou des param\u00e8tres du mod\u00e8le entraine une grande variation du r\u00e9sultat. Pour mesurer cette sensibilit\u00e9 du mod\u00e8le aux entr\u00e9es / param\u00e8tres, on introduit souvent un indicateur num\u00e9rique appell\u00e9 conditionnement (not\u00e9 \\(\\kappa\\) ), qu'il convient d'\u00e9valuer. Nous verrons dans ce cours que cette notion est particuli\u00e8rement utilis\u00e9e pour la r\u00e9solution de syst\u00e8mes d'\u00e9quations lin\u00e9aires. Si le mod\u00e8le math\u00e9matique propos\u00e9 s'av\u00e8re \u00eatre mal conditionn\u00e9, il vaut mieux essayer d'en trouver un autre, bien conditionn\u00e9. Stabilit\u00e9 de la m\u00e9thode num\u00e9rique Pour une m\u00e9thode num\u00e9rique, on parle d' instabilit\u00e9 lorsque les erreurs de troncature et d'arrondi sont propag\u00e9es et amplifi\u00e9es par les diff\u00e9rentes op\u00e9rations de l'algorithme. Cette instabilit\u00e9 va d\u00e9pendre du nombre et de la nature des op\u00e9rations r\u00e9alis\u00e9es par une m\u00e9thode donn\u00e9e. Dans le cas de la m\u00e9thode employ\u00e9e par nos \u00e9tudiants de l'UVSQ, nous avons vu que l'erreur d'arrondi est propag\u00e9e par l'op\u00e9ration de soustraction \\(f(x_i+h)-f(x_i)\\) . Cette instabilit\u00e9 \u00e9tant directement li\u00e9e \u00e0 la m\u00e9thode num\u00e9rique choisie, il convient de choisir la m\u00e9thode la plus stable possible pour r\u00e9soudre un probl\u00e8me donn\u00e9. Quelles sont les demandes de ressources informatique de la m\u00e9thode : temps de calcul et m\u00e9moire ? L'analyse de la quantit\u00e9 de ressources informatique n\u00e9cessaire pour faire tourner un algorithme est essentiel dans le choix d'une m\u00e9thode d'analyse num\u00e9rique. Il est en effet important de choisir une m\u00e9thode qui peut tourner sur une machine donn\u00e9e, et de savoir en combien de temps elle devra tourner pour donner un r\u00e9sultat. On parle d'\"analyse de la complexit\u00e9 \" d'un algorithme. Il y a 2 types de complexit\u00e9 : La complexit\u00e9 en temps Il s'agit de d\u00e9compter le nombre d'op\u00e9rations \u00e9l\u00e9mentaires r\u00e9alis\u00e9es par l'algorithme afin d'estimer le temps de calcul qu'il n\u00e9cessite sur une machine donn\u00e9e. Il est parfois difficile d'estimer le nombre exact d'op\u00e9rations d'un algorithme dans tous les cas. On utilise donc souvent la notation \\(O\\) de Landau pour donner une majoration du nombre d'op\u00e9rations : Nom Complexit\u00e9 Constant \\(O(1)\\) Logarithmique \\(O(log(n))\\) Lin\u00e9aire \\(O(n)\\) Lin\u00e9arithmique \\(O(n log(n))\\) Quadratique \\(O(n^2)\\) Cubique \\(O(n^3)\\) Exponentiel \\(2^{O(n)}\\) Dans le cas de la m\u00e9thode choisie par nos \u00e9tudiants de l'UVSQ, on va r\u00e9aliser \\(N-1\\) soustractions, et \\(N-1\\) divisions, soit \\(2(N-1)\\) op\u00e9rations arithm\u00e9tiques au total. La complexit\u00e9 en espace Il s'agit de calculer la quantit\u00e9 maximale de m\u00e9moire utilis\u00e9e au cours de l'algorithme pour stocker les variables n\u00e9cessaire \u00e0 son ex\u00e9cution. Comme nous l'avons vu, les variables sont stock\u00e9es en binaire, on exprimer en g\u00e9n\u00e9ral la complexit\u00e9 en espace d'un algorithme en octets (multiplet de 8 bits). Dans le cas des \u00e9tudiants de l'UVSQ, on doit stocker les entr\u00e9es, un vecteur de taille \\(N-1\\) et un scalaire, et la sortie, un vecteur de taille \\(N-1\\) . Au total, on doit donc stocker \\(2N-1\\) valeurs. Si ces valeurs sont encod\u00e9es sur 32 bits, un espace m\u00e9moire de maximum \\(64N-32\\) bits sera alors n\u00e9cessaire pour faire tourner l'algorithme, soit \\(8N-4\\) octets. Impl\u00e9mentation de l'algorithme Algorithmique On appelle algorithme l'\u00e9nonc\u00e9 d'une suite d'instructions / d'op\u00e9rations \u00e9l\u00e9mentaires permettant de r\u00e9soudre un probl\u00e8me math\u00e9matique. La notion d'algorithme remonte au moins au Moyen-Age, bien avant l'invention de l'ordinateur. Il n'y a pas unicit\u00e9 d'algorithme pour r\u00e9soudre un probl\u00e8me donn\u00e9. L'algorithme peut effectuer des t\u00e2ches en les unes apr\u00e8s les autres ou simultan\u00e9ment, on dira alors qu'il est respectivement s\u00e9quentiel ou parall\u00e8le . Voici par exemple un algorithme en \"pseudo-code\" \u00e9crit par nos \u00e9tudiants de l'UVSQ par estimer la d\u00e9riv\u00e9e d'une fonction : FONCTION difference_decentree_droite(VECTEUR REELS f, REEL h) ENTIER n <- LONGUEUR(f) VECTEUR REELS df POUR i DE 0 JUSQUE n-1 FAIRE df[i] <- (f[i+1]-f[i])/h FIN_POUR RETOURNER df FIN_FONCTION Le pseudo-code est une mani\u00e8re d'\u00e9crire un algorithme en langage naturel, mais sa syntaxe ne fait pas l'objet d'un consensus. Ecrire un algorithme en langage naturel avant impl\u00e9mentation sur ordinateur est souvent une bonne aide pour structurer ses id\u00e9es. Programmation On appelle programme la traduction d'un algorithme dans un langage de programmation , dans le but d'\u00eatre interpr\u00e9t\u00e9 puis execut\u00e9 par un ordinateur. La notion de programmation remonte au XIX\u00e8me si\u00e8cle, bien qu'elle ne soit formellement th\u00e9oris\u00e9e que dans les ann\u00e9es 1930 par Alan Turing. Il existe aujourd'hui une grande vari\u00e9t\u00e9 de langages et de paradigmes de programmation suivant les applications. Les m\u00e9thodes num\u00e9riques pr\u00e9sent\u00e9es dans ce cours seront toutes impl\u00e9ment\u00e9es sous forme de programmes en langage Python . Python est un langage interpr\u00e9t\u00e9, multi-paradigmes, et multi-plateforme, invent\u00e9 en 1991. Ce langage est un des plus populaires au monde, probablement parce que : Il s'agit d'un langage de haut niveau (loin du langage machine), proposant des outils avanc\u00e9s. Sa syntaxe est plut\u00f4t simple, et son typage dynamique fort (le type des variables est assign\u00e9 automatiquement). Sa licence est libre, et il dispose d'une grande communaut\u00e9 open-source, d\u00e9veloppant des biblioth\u00e8ques pour de nombreuses applications. Concernant les \" biblioth\u00e8ques \", il s'agit d'un ensemble de fonctionnalit\u00e9s d\u00e9j\u00e0 programm\u00e9es par d'autres que vous pouvez importer et utiliser dans votre propre programme. Nous utiliserons lors de ce cours les biblioth\u00e8ques : Numpy : pour la manipulation de vecteurs et de matrices. Matplotlib : pour l'affichage graphique de figures. Scipy : pour certaines m\u00e9thodes num\u00e9riques de r\u00e9f\u00e9rence. Voici le programme Python \u00e9crit par les \u00e9tudiants de l'UVSQ \u00e0 partir de leur algorithme : #Importer la librairie Numpy sous le nom \"np\" : import numpy as np #D\u00e9finition de la m\u00e9thode sous la forme d'une fonction : def difference_decentree_droite(f,h): #On s'assure que f contient des r\u00e9els encod\u00e9s sur 64 bits : f.astype(dtype=np.float64) #On r\u00e9cup\u00e8re le nombre d'\u00e9l\u00e9ments N de f : N = len(f) #On initialise un vecteur df de taille N-1 ne contenant que des z\u00e9ros, #qui contiendra les valeurs de la d\u00e9riv\u00e9e de f : df = np.zeros(N-1,dtype=np.float64) #On fait une boucle sur les diff\u00e9rentes valeurs de d\u00e9riv\u00e9e \u00e0 calculer : for i in range(N-1): #On approxime la d\u00e9riv\u00e9e en un point par la formule de la diff\u00e9rence d\u00e9centr\u00e9e \u00e0 droite : df[i] = (f[i+1]-f[i])/h #Renvoyer le vecteur des valeurs de la d\u00e9riv\u00e9e de f : return df Il s'agit d'une \"fonction\" Python prenant en entr\u00e9e : f : un vecteur Numpy contenant les valeurs de la fonction math\u00e9matique \u00e0 d\u00e9river, suppos\u00e9s r\u00e9guli\u00e8rement espac\u00e9s. h : un nombre r\u00e9el \\(h\\) correspondant au pas de discr\u00e9tisation de \\(f\\) . Et retournant en sortie : df : un vecteur Numpy correspondant aux valeurs de la d\u00e9riv\u00e9e de \\(f\\) . Les \u00e9tudiants pourront appeller cette fonction, avec en entr\u00e9e : leurs points de mesures du d\u00e9placement dans \\(f\\) , et leur pas de discr\u00e9tisation dans \\(h\\) . Ils obtiendront alors les valeurs estim\u00e9es de la vitesse du vent en sortie. Execution et analyse du r\u00e9sultat Il ne reste plus qu'\u00e0 executer le programme et \u00e0 analyser les r\u00e9sultats ! Admentons que nos \u00e9tudiants aient choisi de mesurer le d\u00e9placement de leur ballon dans la stratosph\u00e8re toutes les 10 secondes ( \\(h = 10 s\\) ), pendant 100 secondes ( \\(N = 10\\) ). Voici l'application de la m\u00e9thode des diff\u00e9rences d\u00e9centr\u00e9es \u00e0 droite aux donn\u00e9es mesur\u00e9es par le ballon, afin d'estimer la vitesse du vent dans la stratosph\u00e8re : (Les courbes th\u00e9oriques du d\u00e9placement du ballon et de la vitesse du vent sont en pointill\u00e9s. On part bien entendu du principe que les \u00e9tudiants n'ont pas acc\u00e8s \u00e0 ces informations). On voit que le r\u00e9sultat est entach\u00e9 d'erreurs relativement importantes. Les axes d'am\u00e9lioration possibles pour obtenir une solution plus proche de la r\u00e9alit\u00e9 sont nombreux : R\u00e9duire le pas de discr\u00e9tisation \\(h\\) . Tronquer plus loin le d\u00e9veloppement de Taylor de la m\u00e9thode. Passer d'une m\u00e9thode \"d\u00e9centr\u00e9e\" \u00e0 une m\u00e9thode \"centr\u00e9e\". Etc. Cet exemple \u00e9tait tr\u00e8s simple, voire caricatural, car il n'avait pour but que de vous faire d\u00e9couvrir les probl\u00e9matiques li\u00e9es aux m\u00e9thodes num\u00e9riques. Dans la suite de ce cours, nous verrons des m\u00e9thodes num\u00e9riques plus pouss\u00e9es, sur des exemples issus de la Physique plus complexes et r\u00e9alistes. Nous irons aussi plus loin dans la description de certaines des notions introduites dans ce chapitre.","title":"I. Introduction"},{"location":"Chap1_Introduction/#chapitre-i-introduction-aux-methodes-numeriques","text":"Ce chapitre est une introduction aux enjeux des m\u00e9thodes num\u00e9riques. Dans le c\u00e9l\u00e8bre roman de science-fiction humoristique anglais \"The Hitchhiker's Guide to the Galaxy\", \u00e9crit en 1978 par Douglas Adams, des extra-terrestres construisent un superordinateur du nom de \"Deep Thought\" et lui posent le probl\u00e8me suivant : \"O Deep Thought computer, [...] the task we have designed you to perform is this. We want you to tell us the Answer [...] to Life, the Universe and Everything\". L'ordinateur leur r\u00e9pond alors : \"I can do it [...] but I'll have to think about it. [...] Seven and half million years.\" 7,5 millions d'ann\u00e9es plus tard, les descendants des cr\u00e9ateurs de Deep Thought viennent consulter l'ordinateur pour obtenir leur r\u00e9ponse. Deep Thought dit qu'il a bien la r\u00e9ponse, mais les pr\u00e9vient : \"I don't think [...] that you're gonna like it\". Les extra-terrestres insistent pour avoir la r\u00e9ponse. Deep Thought s'ex\u00e9cute : \"The Answer to the Great Question [...] of Life, the Universe and Everything [...] is ... 42\". Les extra-terrestres sont en col\u00e8re face \u00e0 cette r\u00e9ponse, et demandent \u00e0 l'ordinateur s'il est bien s\u00fbr qu'il n'y a pas une erreur. Ce \u00e0 quoi Deep Thought r\u00e9pond qu'il est certain que la r\u00e9ponse est correcte, et que s'ils ne l'aiment pas, c'est parce que leur probl\u00e8me \u00e9tait mal pos\u00e9. Cette blague aujourd'hui pass\u00e9e \u00e0 la post\u00e9rit\u00e9 fait \u00e9cho aux enjeux des m\u00e9thodes num\u00e9riques que nous allons voir dans ce chapitre... Nous allons d\u00e9tailler dans ce chapitre les diff\u00e9rentes \u00e9tapes de r\u00e9solution d'un probl\u00e8me en Physique, avec un exemple simple.","title":"Chapitre I : Introduction aux m\u00e9thodes num\u00e9riques"},{"location":"Chap1_Introduction/#le-probleme","text":"Un groupe d'\u00e9tudiants de l'UVSQ r\u00e9alise un projet de ballon sonde pour mesurer la vitesse du vent dans la stratosph\u00e8re au cours du temps . On consid\u00e8re qu'apr\u00e8s une phase d'ascension, leur ballon a atteint une altitude stable dans la stratosph\u00e8re, et que son d\u00e9placement est uniquement li\u00e9 au vent . La nacelle de leur ballon contient une balise GPS, qui leur permet de mesurer le d\u00e9placement du ballon au cours du temps . Le probl\u00e8me physique auquel les \u00e9tudiants sont confront\u00e9s est le suivant : estimer la vitesse du vent dans la stratosph\u00e8re au cours du temps \u00e0 partir de leurs mesures.","title":"Le probl\u00e8me"},{"location":"Chap1_Introduction/#modelisation-du-probleme","text":"La premi\u00e8re \u00e9tape est de traduire ce probl\u00e8me physique en un mod\u00e8le par le biais d'\u00e9quations math\u00e9matiques : Les \u00e9tudiants font l'hypoth\u00e8se que leur ballon stratosph\u00e9rique se d\u00e9place \u00e0 la vitesse du vent . Notons \\(p(t)\\) la fonction associant \u00e0 chaque instant \\(t\\) en [s] \u00e0 partir du d\u00e9but de la mesure le d\u00e9placement du ballon en [m]. La traduction math\u00e9matique du probl\u00e8me est donc que la vitesse du vent au cours du temps \\(W(t)\\) est li\u00e9e \u00e0 \\(p(t)\\) par l'\u00e9quation : \\(W(t) = \\frac{d}{dt} p(t)\\) Comme la fonction \\(p(t)\\) n'a pas d'expression analytique connue (il s'agit d'une mesure d'un capteur), les \u00e9tudiants devront utiliser une m\u00e9thode num\u00e9rique pour estimer la d\u00e9riv\u00e9e de \\(p(t)\\) , et donc la solution du probl\u00e8me.","title":"Mod\u00e9lisation du probl\u00e8me"},{"location":"Chap1_Introduction/#discretisation-du-probleme","text":"Un ordinateur ne pouvant g\u00e9rer des objets continus, l'application d'une m\u00e9thode num\u00e9rique n\u00e9cessite une discr\u00e9tisation . Dans le cas des \u00e9tudiants de l'UVSQ, la discr\u00e9tisation est r\u00e9alis\u00e9e au moment de l'\u00e9chantillonnage : Les \u00e9tudiants d\u00e9cident d'enregistrer une mesure de d\u00e9placement toutes les \\(h\\) secondes. Ils n'ont donc pas acc\u00e8s \u00e0 toutes les valeurs possibles de \\(p(t)\\) , mais \u00e0 des valeurs discr\u00e8tes r\u00e9guli\u00e8rement espac\u00e9es \\(p(t_i)\\) avec \\(t_i = 0, h, 2 \\times h, 3 \\times h, ... (N-1) \\times h\\) et \\(N\\) le nombre d'\u00e9chantillons. De m\u00eame, ils n'estimeront pas toutes les valeurs de \\(W(t)\\) , mais des valeurs discr\u00e8tes \\(W(t_i)\\) . On nomme \\(h\\) le pas de discr\u00e9tisation du probl\u00e8me.","title":"Discr\u00e9tisation du probl\u00e8me"},{"location":"Chap1_Introduction/#la-solution-existe-t-elle-est-elle-unique","text":"Il faut toujours se poser ces questions avant d'essayer de r\u00e9soudre num\u00e9riquement un probl\u00e8me. Pour notre exemple, nous partirons du principe que la trajectoire de la nacelle \\(p(t)\\) est d\u00e9rivable, et donc que l' existence et l' unicit\u00e9 de la solution est triviale. Mais ce n'est pas toujours le cas . Par exemple, pour un probl\u00e8me impliquant la recherche de la racine d'une fonction sur un intervalle, nous verrons que l'existence et l'unicit\u00e9 ne sont pas \u00e9videntes et doivent \u00eatre d\u00e9montr\u00e9es.","title":"La solution existe-t-elle ? Est-elle unique ?"},{"location":"Chap1_Introduction/#choix-dune-methode-numerique","text":"Nos \u00e9tudiants de l'UVSQ on besoin ici d'une m\u00e9thode num\u00e9rique d'estimation de la d\u00e9riv\u00e9e d'une fonction. Pour approcher la d\u00e9riv\u00e9e ils d\u00e9cident d'employer une m\u00e9thode de \" diff\u00e9rences d\u00e9centr\u00e9es \u00e0 droite \" : \\(W(t_i) = \\frac{d}{dt} p(t_i) \\approx \\frac{p(t_{i+1})-p(t_i)}{h}\\) avec \\(t_{i+1} = t_i + h\\) et \\(i = 0, 1, ..., N-2\\) Lorsque l'on choisi une m\u00e9thode num\u00e9rique pour r\u00e9pondre \u00e0 un probl\u00e8me, il convient de se poser les questions suivantes.","title":"Choix d'une m\u00e9thode num\u00e9rique"},{"location":"Chap1_Introduction/#la-methode-est-elle-applicable","text":"Chaque m\u00e9thode num\u00e9rique a des conditions d' applicabilit\u00e9 , qui ne sont pas n\u00e9cessairement les m\u00eames pour un type de probl\u00e8me donn\u00e9. Dans notre exemple, il faut que la fonction \\(p(t)\\) soit doublement d\u00e9rivable sur chaque intervalle \\([t_i,t_{i+1}]\\) .","title":"La m\u00e9thode est-elle applicable ?"},{"location":"Chap1_Introduction/#quelles-sont-les-sources-derreur-et-quelle-est-lerreur-attendue","text":"La r\u00e9solution num\u00e9rique d'un probl\u00e8me physique est toujours entach\u00e9e d'erreurs . On appelle pr\u00e9cision d'une m\u00e9thode num\u00e9rique l'erreur entre la solution donn\u00e9e par la m\u00e9thode et la solution exacte du probl\u00e8me. Pour estimer cette pr\u00e9cision, on introduit les notions d'erreur absolue et d'erreur relative . Soit \\(\\hat{x}\\) la solution approch\u00e9e d'une valeure r\u00e9elle exacte \\(x\\) : L' erreur absolue est d\u00e9finie par \\(\\mid x - \\hat{x} \\mid\\) . L' erreur relative est d\u00e9finie par \\(\\frac{\\mid x - \\hat{x} \\mid}{\\mid x \\mid}\\) . (Il est \u00e0 noter que ces notions sont g\u00e9n\u00e9ralisables \u00e0 des vecteurs ou des matrices en rempla\u00e7ant la valeur absolue par une norme vectorielle / matricielle). En pratique, il est cependant difficile d'\u00e9valuer l'erreur absolue / relative, puisque la solution exacte est par d\u00e9finition l'inconnue que nous voulons approcher. Dans notre exemple, les \u00e9tudiants de l'UVSQ ne connaissent pas la valeur exacte de la vitesse du vent. On peut toutefois essayer de borner l'erreur . Les erreurs lors de la r\u00e9solution num\u00e9rique d'un probl\u00e8me ont diff\u00e9rentes sources qu'il convient d'identifier. Certaines sont li\u00e9es \u00e0 la m\u00e9thode choisie, d'autres non .","title":"Quelles sont les sources d'erreur, et quelle est l'erreur attendue ?"},{"location":"Chap1_Introduction/#erreurs-de-modelisation-et-de-donnees","text":"Une partie de l'erreur sur la solution d'un probl\u00e8me physique provient toujours du mod\u00e8le : Pour commencer, tout mod\u00e8le fait des hypoth\u00e8ses simplificatrices, n\u00e9gligeant certains ph\u00e9nom\u00e8nes physiques. Ensuite, notre mod\u00e8le peut aussi avoir un domaine de validit\u00e9 limit\u00e9, qui ne soit pas respect\u00e9 pendant une partie de l'\u00e9tude. Et enfin, si notre mod\u00e8le s'appuie sur des donn\u00e9es mesur\u00e9es, ou sur des param\u00e8tres physiques estim\u00e9s, alors ces valeurs seront forc\u00e9ment inexactes. Dans le cas du projet de ballon des \u00e9tudiants de l'UVSQ, nous avons : L'hypoth\u00e8se simplificatrice que le mouvement du ballon n'est influenc\u00e9 que par le vent, et que la vitesse du ballon est directement \u00e9gale \u00e0 la vitesse du vent. Le validit\u00e9 de notre mod\u00e8le se limitant probablement \u00e0 des vents faibles aux variations lentes (pas des rafales). Les mesures du d\u00e9placement du ballon par GPS, qui seront entach\u00e9es d'erreurs de mesures.","title":"Erreurs de mod\u00e9lisation et de donn\u00e9es"},{"location":"Chap1_Introduction/#erreurs-de-troncature","text":"La discr\u00e9tisation d'un probl\u00e8me physique par une m\u00e9thode num\u00e9rique induit n\u00e9cessairement des erreurs. Par exemple, la troncature d'une s\u00e9rie infinie convergeant vers la solution, ou l'arr\u00eat au bout d'un nombre d'it\u00e9rations finies d'une suite convergeant vers la solution, sont in\u00e9vitables. En effet, un ordinateur ne peut effectuer qu'un nombre fini d'op\u00e9rations . On parle alors d' erreurs de troncature . Il s'agit donc ici d'erreurs directement li\u00e9es \u00e0 la m\u00e9thode choisie. La m\u00e9thode choisie par nos \u00e9tudiants de l'UVSQ est bas\u00e9e sur le d\u00e9veloppement de Taylor d'ordre 2 suivant : \\(\\frac{d}{dt} p(t_i) = \\frac{p(t_i+h)-p(t_i)}{h} - \\frac{h}{2} \\frac{d^2}{dt^2} p(\\tau)\\) avec \\(\\tau \\in [t_i,t_i+h]\\) Dans ce cas, l'erreur de troncature est donc : \\(\\frac{h}{2} \\mid \\frac{d^2}{dt^2} p(\\tau) \\mid\\) . On note alors qu'il y a 2 moyens de r\u00e9duire cette erreur : Diminuer le pas de discr\u00e9tisation \\(h\\) (dans notre cas, augmenter la fr\u00e9quence d'\u00e9chantillonnage). Modifier la m\u00e9thode en r\u00e9alisant un d\u00e9veloppement de Taylor d'ordre sup\u00e9rieur.","title":"Erreurs de troncature"},{"location":"Chap1_Introduction/#erreurs-darrondi","text":"Un ordinateur ne peut manipuler que des nombres en pr\u00e9cision finie : il y a un nombre fini de r\u00e9els qui peuvent \u00eatre repr\u00e9sent\u00e9s par la machine. Il y a donc un arrondi sur toutes les valeurs repr\u00e9sent\u00e9es par un ordinateur. On parle alors d' erreurs d'arrondi . Il s'agit donc ici d'erreurs directement li\u00e9es \u00e0 la pr\u00e9cision de la machine. Ces erreurs se propagent \u00e0 mesure que l'on applique des op\u00e9rations lors de la r\u00e9solution d'un probl\u00e8me num\u00e9rique. Si nos \u00e9tudiants de l'UVSQ choisissent une machine repr\u00e9sentant les r\u00e9els avec une pr\u00e9cision \\(\\delta\\) , alors : Pour chaque \\(t_i\\) l'\u00e9valuation de \\(p(t_i)\\) sera connue avec une pr\u00e9cision \\(\\delta\\) . Donc l'\u00e9valuation de \\(p(t_i+h)-p(t_i)\\) sera connue avec une pr\u00e9cision \\(2 \\delta\\) . Et par cons\u00e9quent l'\u00e9valuation de \\(\\frac{p(t_i+h)-p(t_i)}{h}\\) sera connue avec une pr\u00e9cision \\(\\frac{2 \\delta}{h}\\) . On en d\u00e9duit que l'erreur d'arrondi dans notre exemple est : \\(2 \\delta \\frac{p(t_i)}{h}\\) . On remarque que l'erreur d'arrondi sur \\(p(t_i)\\) a \u00e9t\u00e9 propag\u00e9e par les diff\u00e9rentes op\u00e9rations, avec en particulier un facteur 2 d\u00fb \u00e0 l'op\u00e9ration de soustraction. Nota Bene On observe que l'erreur d'arrondi est inversement proportionnelle \u00e0 \\(h\\) , alors que l'erreur de troncature est proportionnelle \u00e0 \\(h\\) . Le choix du pas de discr\u00e9tisation \\(h\\) a donc des effets antagonistes sur ces 2 erreurs : un compromis est n\u00e9cessaire. Dans le cas de notre exemple, on peut montrer que la valeur de \\(h\\) minimisant la somme de ces 2 erreurs est : \\(h = 2 \\sqrt{\\delta \\mid \\frac{p(t_i)}{\\frac{d^2}{dt^2} p(\\tau)} \\mid}\\) avec \\(\\tau \\in [t_i,t_i+h]\\) .","title":"Erreurs d'arrondi"},{"location":"Chap1_Introduction/#erreurs-de-representation-des-nombres","text":"Afin de stocker et manipuler des nombres, une machine va g\u00e9n\u00e9ralement les repr\u00e9senter en binaire (dans la base de 2). Les nombres sont physiquement rang\u00e9s en m\u00e9moire dans un nombre pr\u00e9d\u00e9fini de cellules-m\u00e9moires appel\u00e9es bits (\"binary digits\"). Un bit ne peut prendre que 2 valeurs : 0 ou 1 . Les nombres sont alors repr\u00e9sent\u00e9s par une combinaison binaire de 0 et de 1. Nous allons voir que cette repr\u00e9sentation des nombres par la machine peut aussi \u00eatre source d'erreurs en analyse num\u00e9rique. Repr\u00e9sentation des entiers : Un entier naturel \\(n\\) est repr\u00e9sent\u00e9 en binaire par : \\(n = a_{p-1} 2^{p-1} + a_{p-2} 2^{p-2} + ... + a_1 2^1 + a_0 2^0\\) avec \\(a_i\\) pour \\(0 \\leq i < p\\) les \\(p\\) bits, \u00e9gaux \u00e0 0 ou 1. \\(p\\) bits permettent alors de repr\u00e9senter exactement les entiers naturels entre \\(0\\) et \\(2^p-1\\) . Des op\u00e9rations \u00e9l\u00e9mentaires telles que l'addition peuvent \u00eatre appliqu\u00e9es sur le binaires bit \u00e0 bit . Repr\u00e9sentation des entiers sign\u00e9s : Pour repr\u00e9senter les entiers sign\u00e9s, on veut : (1) pouvoir identifier le signe avec un bit d\u00e9di\u00e9 nomm\u00e9 bit de poids fort , (2) que les r\u00e8gles d'addition soient toujours valides. La repr\u00e9sentation couramment utilis\u00e9e est celle du compl\u00e9ment \u00e0 2 : on code un nombre n\u00e9gatif en binaire par \\((2^p - |x|)_2\\) . Avec cette convention, un entier sign\u00e9 \\(n\\) est repr\u00e9sent\u00e9 par : \\(n = -a_{p-1} 2^{p-1} + a_{p-2} 2^{p-2} + ... + a_1 2^1 + a_0 2^0\\) avec \\(a_i\\) pour \\(0 \\leq i < p\\) les \\(p\\) bits, \u00e9gaux \u00e0 0 ou 1. \\(p\\) bits permettent alors de repr\u00e9senter exactement les entiers sign\u00e9s entre \\(-2^{p-1}\\) et \\(2^{p-1}-1\\) . D\u00e9passement de capacit\u00e9 (overflow) : Soient 2 entiers sign\u00e9s \\(n_1\\) et \\(n_2\\) repr\u00e9sent\u00e9s en format compl\u00e9ment \u00e0 2 sur \\(p\\) bits : Si \\(n_1 + n_2 \\geq 2^{p-1}\\) alors il y a un d\u00e9passement de capacit\u00e9 positif . Si \\(n_1 + n_2 < -2^{p-1}\\) alors il y a un d\u00e9passement de capacit\u00e9 n\u00e9gatif . On parle en anglais de probl\u00e8me d' overflow . Les op\u00e9rations arithm\u00e9tiques sur des entiers s'effectuent donc exactement, mais \u00e0 condition que le r\u00e9sultat soit repr\u00e9sentable par la machine . Sinon, la valeur de sortie sera \u00e9rron\u00e9e. Nota Bene - Lorsque \\(p=8\\) on parle d' octet . - Lorsque \\(p=16\\) on parle de simple pr\u00e9cision . - Lorsque \\(p=32\\) on parle de double pr\u00e9cision . Repr\u00e9sentation des r\u00e9els : Pour repr\u00e9senter des r\u00e9els en machine, on a recourt \u00e0 la convention de la virgule flottante . Avec cette convention, un r\u00e9el \\(r\\) sera repr\u00e9sent\u00e9 par : \\(r = (-1)^s M 2^e\\) avec \\(s\\) le signe, \\(M\\) la mantisse (un r\u00e9el positif), et \\(e\\) l'exposant (un entier sign\u00e9). Suivant le r\u00e9el \u00e0 repr\u00e9senter, et le choix de la mantisse et de l'exposant, des probl\u00e8mes d'arrondi apparaissent . Aussi, il n'y a pas unicit\u00e9 de la repr\u00e9sentation d'un nombre avec cette convention. C'est pourquoi on va fixer en plus des r\u00e8gles sur la mantisse et l'exposant. Norme IEEE 754 et erreurs : La norme IEEE 754 a \u00e9t\u00e9 introduite en 1985 pour standardiser la repr\u00e9sentation des r\u00e9els en virgule flottante. En simple pr\u00e9cision , un r\u00e9el sera repr\u00e9sent\u00e9 sur 32 bits : 1 bit pour le signe , 8 bits pour l' exposant , et 23 bits pour la mantisse . On peut avec cette norme repr\u00e9senter les r\u00e9els compris entre \\(2^{-126}\\) et environ \\(2^{128}\\) . En double pr\u00e9cision , un r\u00e9el sera repr\u00e9sent\u00e9 sur 64 bits : 1 bit pour le signe , 11 bits pour l' exposant , et 52 bits pour la mantisse . On peut avec cette norme repr\u00e9senter les r\u00e9els compris entre \\(2^{-1022}\\) et environ \\(2^{1024}\\) . D\u00e9finition : le epsilon machine On appelle epsilon machine (ou \"macheps\") le nombre positif \\(\\epsilon\\) le plus petit tel que \\(1 + \\epsilon > 1\\) . On peut montrer que l'erreur relative sur la repr\u00e9sentation virgule flottante d'un nombre est major\u00e9e par \\(\\epsilon\\) . Pour la norme IEEE 754 : en simple pr\u00e9cision \\(\\epsilon = 2^{-23}\\) , en double pr\u00e9cision \\(\\epsilon = 2^{-52}\\) . Nota Bene En norme IEEE 754, les situation d'overflow ne provoquent pas d'arr\u00eat des calculs. Il faut donc \u00eatre vigilant, car des valeurs erron\u00e9es seront alors obtenues.","title":"Erreurs de repr\u00e9sentation des nombres"},{"location":"Chap1_Introduction/#les-bonnes-pratiques","text":"Lors de la r\u00e9solution de leur probl\u00e8me, on recommande \u00e0 nos \u00e9tudiants de l'UVSQ de suivre les 3 conseils suivants : (1) choisir une pr\u00e9cision pertinente pour la repr\u00e9sentation des entiers et des r\u00e9els, (2) arrondir les nombres au nombre de d\u00e9cimales requis par le calcul, (3) normaliser les valeurs pour \u00e9viter les probl\u00e8mes d'overflow.","title":"Les bonnes pratiques :"},{"location":"Chap1_Introduction/#la-methode-converge-t-elle-vers-la-solution-avec-quelle-vitesse","text":"Une m\u00e9thode num\u00e9rique est dites convergente si l'\u00e9cart entre la solution approch\u00e9e et la solution exacte tend vers 0 quand le pas de discr\u00e9tisation \\(h\\) tend vers 0. Si de plus, l'erreur absolue \\(e\\) peut \u00eatre major\u00e9e : \\(e \\leq C h^p\\) avec \\(h\\) le pas de discr\u00e9tisation, \\(p\\) un nombre positif et \\(C\\) une constante alors la m\u00e9thode est dite convergente d'ordre \\(p\\) . Si \\(p = 2, 3\\) ou \\(4\\) , on dit la convergence quadratique, cubique ou quartique. Dans notre exemple, on peut montrer que pour chaque \\(W(t_i)\\) , l'erreur est major\u00e9e par \\(\\frac{h}{2} sup_{t \\in [t_i,t_i+h]} \\mid \\frac{d^2}{dt^2} p(t)\\mid\\) . La m\u00e9thode converge donc vers la solution lorsque que le pas de discr\u00e9tisation \\(h\\) diminue. Cette convergence est \"d'ordre 1\".","title":"La m\u00e9thode converge-t-elle vers la solution ? Avec quelle vitesse ?"},{"location":"Chap1_Introduction/#la-solution-est-elle-stable","text":"Lors de la r\u00e9solution num\u00e9rique d'un probl\u00e8me, il convient de se poser la question de la stabilit\u00e9 de la solution. Cette notion de stabilit\u00e9 peut s'appliquer \u00e0 3 niveaux :","title":"La solution est-elle stable ?"},{"location":"Chap1_Introduction/#stabilite-du-probleme-physique","text":"Certains probl\u00e8mes en Physique sont par nature chaotiques : une petite variation des conditions initiales entraine une variation tellement importante des r\u00e9sultats qu'elle rend toute approximation de la solution impossible. Ce n'est pas le cas du probl\u00e8me auquels nos \u00e9tudiants de l'UVSQ sont confront\u00e9s, mais on peut citer des ph\u00e9nom\u00e8nes chaotiques c\u00e9l\u00e8bres en Physique : le double-pendule, le probl\u00e8me \u00e0 N corps, et la turbulence des fluides. Cette instabilit\u00e9 \u00e9tant directement li\u00e9e au probl\u00e8me, et donc ind\u00e9pendante de nos choix de mod\u00e8le ou de m\u00e9thode, nous n'avons aucun moyen d'y rem\u00e9dier.","title":"Stabilit\u00e9 du probl\u00e8me physique"},{"location":"Chap1_Introduction/#stabilite-du-modele-mathematique","text":"Il est possible qu'un mod\u00e8le math\u00e9matique soit mal conditionn\u00e9 : une petite variation des entr\u00e9es ou des param\u00e8tres du mod\u00e8le entraine une grande variation du r\u00e9sultat. Pour mesurer cette sensibilit\u00e9 du mod\u00e8le aux entr\u00e9es / param\u00e8tres, on introduit souvent un indicateur num\u00e9rique appell\u00e9 conditionnement (not\u00e9 \\(\\kappa\\) ), qu'il convient d'\u00e9valuer. Nous verrons dans ce cours que cette notion est particuli\u00e8rement utilis\u00e9e pour la r\u00e9solution de syst\u00e8mes d'\u00e9quations lin\u00e9aires. Si le mod\u00e8le math\u00e9matique propos\u00e9 s'av\u00e8re \u00eatre mal conditionn\u00e9, il vaut mieux essayer d'en trouver un autre, bien conditionn\u00e9.","title":"Stabilit\u00e9 du mod\u00e8le math\u00e9matique"},{"location":"Chap1_Introduction/#stabilite-de-la-methode-numerique","text":"Pour une m\u00e9thode num\u00e9rique, on parle d' instabilit\u00e9 lorsque les erreurs de troncature et d'arrondi sont propag\u00e9es et amplifi\u00e9es par les diff\u00e9rentes op\u00e9rations de l'algorithme. Cette instabilit\u00e9 va d\u00e9pendre du nombre et de la nature des op\u00e9rations r\u00e9alis\u00e9es par une m\u00e9thode donn\u00e9e. Dans le cas de la m\u00e9thode employ\u00e9e par nos \u00e9tudiants de l'UVSQ, nous avons vu que l'erreur d'arrondi est propag\u00e9e par l'op\u00e9ration de soustraction \\(f(x_i+h)-f(x_i)\\) . Cette instabilit\u00e9 \u00e9tant directement li\u00e9e \u00e0 la m\u00e9thode num\u00e9rique choisie, il convient de choisir la m\u00e9thode la plus stable possible pour r\u00e9soudre un probl\u00e8me donn\u00e9.","title":"Stabilit\u00e9 de la m\u00e9thode num\u00e9rique"},{"location":"Chap1_Introduction/#quelles-sont-les-demandes-de-ressources-informatique-de-la-methode-temps-de-calcul-et-memoire","text":"L'analyse de la quantit\u00e9 de ressources informatique n\u00e9cessaire pour faire tourner un algorithme est essentiel dans le choix d'une m\u00e9thode d'analyse num\u00e9rique. Il est en effet important de choisir une m\u00e9thode qui peut tourner sur une machine donn\u00e9e, et de savoir en combien de temps elle devra tourner pour donner un r\u00e9sultat. On parle d'\"analyse de la complexit\u00e9 \" d'un algorithme. Il y a 2 types de complexit\u00e9 :","title":"Quelles sont les demandes de ressources informatique de la m\u00e9thode : temps de calcul et m\u00e9moire ?"},{"location":"Chap1_Introduction/#la-complexite-en-temps","text":"Il s'agit de d\u00e9compter le nombre d'op\u00e9rations \u00e9l\u00e9mentaires r\u00e9alis\u00e9es par l'algorithme afin d'estimer le temps de calcul qu'il n\u00e9cessite sur une machine donn\u00e9e. Il est parfois difficile d'estimer le nombre exact d'op\u00e9rations d'un algorithme dans tous les cas. On utilise donc souvent la notation \\(O\\) de Landau pour donner une majoration du nombre d'op\u00e9rations : Nom Complexit\u00e9 Constant \\(O(1)\\) Logarithmique \\(O(log(n))\\) Lin\u00e9aire \\(O(n)\\) Lin\u00e9arithmique \\(O(n log(n))\\) Quadratique \\(O(n^2)\\) Cubique \\(O(n^3)\\) Exponentiel \\(2^{O(n)}\\) Dans le cas de la m\u00e9thode choisie par nos \u00e9tudiants de l'UVSQ, on va r\u00e9aliser \\(N-1\\) soustractions, et \\(N-1\\) divisions, soit \\(2(N-1)\\) op\u00e9rations arithm\u00e9tiques au total.","title":"La complexit\u00e9 en temps"},{"location":"Chap1_Introduction/#la-complexite-en-espace","text":"Il s'agit de calculer la quantit\u00e9 maximale de m\u00e9moire utilis\u00e9e au cours de l'algorithme pour stocker les variables n\u00e9cessaire \u00e0 son ex\u00e9cution. Comme nous l'avons vu, les variables sont stock\u00e9es en binaire, on exprimer en g\u00e9n\u00e9ral la complexit\u00e9 en espace d'un algorithme en octets (multiplet de 8 bits). Dans le cas des \u00e9tudiants de l'UVSQ, on doit stocker les entr\u00e9es, un vecteur de taille \\(N-1\\) et un scalaire, et la sortie, un vecteur de taille \\(N-1\\) . Au total, on doit donc stocker \\(2N-1\\) valeurs. Si ces valeurs sont encod\u00e9es sur 32 bits, un espace m\u00e9moire de maximum \\(64N-32\\) bits sera alors n\u00e9cessaire pour faire tourner l'algorithme, soit \\(8N-4\\) octets.","title":"La complexit\u00e9 en espace"},{"location":"Chap1_Introduction/#implementation-de-lalgorithme","text":"","title":"Impl\u00e9mentation de l'algorithme"},{"location":"Chap1_Introduction/#algorithmique","text":"On appelle algorithme l'\u00e9nonc\u00e9 d'une suite d'instructions / d'op\u00e9rations \u00e9l\u00e9mentaires permettant de r\u00e9soudre un probl\u00e8me math\u00e9matique. La notion d'algorithme remonte au moins au Moyen-Age, bien avant l'invention de l'ordinateur. Il n'y a pas unicit\u00e9 d'algorithme pour r\u00e9soudre un probl\u00e8me donn\u00e9. L'algorithme peut effectuer des t\u00e2ches en les unes apr\u00e8s les autres ou simultan\u00e9ment, on dira alors qu'il est respectivement s\u00e9quentiel ou parall\u00e8le . Voici par exemple un algorithme en \"pseudo-code\" \u00e9crit par nos \u00e9tudiants de l'UVSQ par estimer la d\u00e9riv\u00e9e d'une fonction : FONCTION difference_decentree_droite(VECTEUR REELS f, REEL h) ENTIER n <- LONGUEUR(f) VECTEUR REELS df POUR i DE 0 JUSQUE n-1 FAIRE df[i] <- (f[i+1]-f[i])/h FIN_POUR RETOURNER df FIN_FONCTION Le pseudo-code est une mani\u00e8re d'\u00e9crire un algorithme en langage naturel, mais sa syntaxe ne fait pas l'objet d'un consensus. Ecrire un algorithme en langage naturel avant impl\u00e9mentation sur ordinateur est souvent une bonne aide pour structurer ses id\u00e9es.","title":"Algorithmique"},{"location":"Chap1_Introduction/#programmation","text":"On appelle programme la traduction d'un algorithme dans un langage de programmation , dans le but d'\u00eatre interpr\u00e9t\u00e9 puis execut\u00e9 par un ordinateur. La notion de programmation remonte au XIX\u00e8me si\u00e8cle, bien qu'elle ne soit formellement th\u00e9oris\u00e9e que dans les ann\u00e9es 1930 par Alan Turing. Il existe aujourd'hui une grande vari\u00e9t\u00e9 de langages et de paradigmes de programmation suivant les applications. Les m\u00e9thodes num\u00e9riques pr\u00e9sent\u00e9es dans ce cours seront toutes impl\u00e9ment\u00e9es sous forme de programmes en langage Python . Python est un langage interpr\u00e9t\u00e9, multi-paradigmes, et multi-plateforme, invent\u00e9 en 1991. Ce langage est un des plus populaires au monde, probablement parce que : Il s'agit d'un langage de haut niveau (loin du langage machine), proposant des outils avanc\u00e9s. Sa syntaxe est plut\u00f4t simple, et son typage dynamique fort (le type des variables est assign\u00e9 automatiquement). Sa licence est libre, et il dispose d'une grande communaut\u00e9 open-source, d\u00e9veloppant des biblioth\u00e8ques pour de nombreuses applications. Concernant les \" biblioth\u00e8ques \", il s'agit d'un ensemble de fonctionnalit\u00e9s d\u00e9j\u00e0 programm\u00e9es par d'autres que vous pouvez importer et utiliser dans votre propre programme. Nous utiliserons lors de ce cours les biblioth\u00e8ques : Numpy : pour la manipulation de vecteurs et de matrices. Matplotlib : pour l'affichage graphique de figures. Scipy : pour certaines m\u00e9thodes num\u00e9riques de r\u00e9f\u00e9rence. Voici le programme Python \u00e9crit par les \u00e9tudiants de l'UVSQ \u00e0 partir de leur algorithme : #Importer la librairie Numpy sous le nom \"np\" : import numpy as np #D\u00e9finition de la m\u00e9thode sous la forme d'une fonction : def difference_decentree_droite(f,h): #On s'assure que f contient des r\u00e9els encod\u00e9s sur 64 bits : f.astype(dtype=np.float64) #On r\u00e9cup\u00e8re le nombre d'\u00e9l\u00e9ments N de f : N = len(f) #On initialise un vecteur df de taille N-1 ne contenant que des z\u00e9ros, #qui contiendra les valeurs de la d\u00e9riv\u00e9e de f : df = np.zeros(N-1,dtype=np.float64) #On fait une boucle sur les diff\u00e9rentes valeurs de d\u00e9riv\u00e9e \u00e0 calculer : for i in range(N-1): #On approxime la d\u00e9riv\u00e9e en un point par la formule de la diff\u00e9rence d\u00e9centr\u00e9e \u00e0 droite : df[i] = (f[i+1]-f[i])/h #Renvoyer le vecteur des valeurs de la d\u00e9riv\u00e9e de f : return df Il s'agit d'une \"fonction\" Python prenant en entr\u00e9e : f : un vecteur Numpy contenant les valeurs de la fonction math\u00e9matique \u00e0 d\u00e9river, suppos\u00e9s r\u00e9guli\u00e8rement espac\u00e9s. h : un nombre r\u00e9el \\(h\\) correspondant au pas de discr\u00e9tisation de \\(f\\) . Et retournant en sortie : df : un vecteur Numpy correspondant aux valeurs de la d\u00e9riv\u00e9e de \\(f\\) . Les \u00e9tudiants pourront appeller cette fonction, avec en entr\u00e9e : leurs points de mesures du d\u00e9placement dans \\(f\\) , et leur pas de discr\u00e9tisation dans \\(h\\) . Ils obtiendront alors les valeurs estim\u00e9es de la vitesse du vent en sortie.","title":"Programmation"},{"location":"Chap1_Introduction/#execution-et-analyse-du-resultat","text":"Il ne reste plus qu'\u00e0 executer le programme et \u00e0 analyser les r\u00e9sultats ! Admentons que nos \u00e9tudiants aient choisi de mesurer le d\u00e9placement de leur ballon dans la stratosph\u00e8re toutes les 10 secondes ( \\(h = 10 s\\) ), pendant 100 secondes ( \\(N = 10\\) ). Voici l'application de la m\u00e9thode des diff\u00e9rences d\u00e9centr\u00e9es \u00e0 droite aux donn\u00e9es mesur\u00e9es par le ballon, afin d'estimer la vitesse du vent dans la stratosph\u00e8re : (Les courbes th\u00e9oriques du d\u00e9placement du ballon et de la vitesse du vent sont en pointill\u00e9s. On part bien entendu du principe que les \u00e9tudiants n'ont pas acc\u00e8s \u00e0 ces informations). On voit que le r\u00e9sultat est entach\u00e9 d'erreurs relativement importantes. Les axes d'am\u00e9lioration possibles pour obtenir une solution plus proche de la r\u00e9alit\u00e9 sont nombreux : R\u00e9duire le pas de discr\u00e9tisation \\(h\\) . Tronquer plus loin le d\u00e9veloppement de Taylor de la m\u00e9thode. Passer d'une m\u00e9thode \"d\u00e9centr\u00e9e\" \u00e0 une m\u00e9thode \"centr\u00e9e\". Etc. Cet exemple \u00e9tait tr\u00e8s simple, voire caricatural, car il n'avait pour but que de vous faire d\u00e9couvrir les probl\u00e9matiques li\u00e9es aux m\u00e9thodes num\u00e9riques. Dans la suite de ce cours, nous verrons des m\u00e9thodes num\u00e9riques plus pouss\u00e9es, sur des exemples issus de la Physique plus complexes et r\u00e9alistes. Nous irons aussi plus loin dans la description de certaines des notions introduites dans ce chapitre.","title":"Execution et analyse du r\u00e9sultat"},{"location":"Chap2_Recherche_de_racines/","text":"Chapitre II : Recherche de racines Ce chapitre porte sur les m\u00e9thodes num\u00e9riques pour la recherche de racines d'une fonction. Position du probl\u00e8me Motivation D\u00e9finition Soit f une fonction d\u00e9finie et continue de \\(\\mathbb{R}\\) dans \\(\\mathbb{R}\\) . Les racines ou z\u00e9ros de cette fonction sont les valeurs \\(x\\) qui v\u00e9rifient l'\u00e9quation \\(f(x)=0\\) Le principal int\u00e9r\u00eat des m\u00e9thodes de recherche de racines est de pouvoir r\u00e9soudre des \u00e9quations . En effet, trouver \\(x \\in \\mathbb{R}\\) tel que \\(f(x)=c\\) revient \u00e0 chercher les racines de la fonction \\(f(x)-c\\) . Dans certains cas, on peut trouver analytiquement les racines d'une fonction. Toute \u00e9quation polynomiale de degr\u00e9 \\(n\\) a exactement \\(n\\) solutions dans \\(\\mathbb{C}\\) et au plus \\(n\\) solutions dans \\(\\mathbb{R}\\) . Mais d'apr\u00e8s la th\u00e9orie d'Evariste Galois, \u00e0 partir du degr\u00e9 5 il n'existe plus de formule g\u00e9n\u00e9rale de r\u00e9solution . Si l'\u00e9quation n'est pas polynomiale, sa r\u00e9solution analytique est encore moins probable. D'o\u00f9 l'int\u00e9r\u00eat d'utiliser des m\u00e9thodes de r\u00e9solution num\u00e9riques , afin d'approcher les valeurs des racines. Le principe est le suivant : Localiser grossi\u00e8rement les racines en proc\u00e9dant \u00e0 des \u00e9valuations graphiques. Construire une suite qui converge vers chaque racine. Ce chapitre pr\u00e9sentera un panel de m\u00e9thodes num\u00e9riques de recherche de racines , que nous appliquerons \u00e0 un m\u00eame exemple pour illustration. Existence et localisation des racines Que l'approche soit analytique ou num\u00e9rique, la 1\u00e8re \u00e9tape consiste g\u00e9n\u00e9ralement \u00e0 localiser les solutions de l'\u00e9quation. Pour ce faire, on d\u00e9termine les intervalles \\([a,b]\\) contenant une unique racine . C'est ce que l'on appelle la s\u00e9paration des racines . Cette d\u00e9termination se fait graphiquement et/ou analytiquement de la mani\u00e8re suivante : Etude des variations de la fonction \\(f\\) . Trouver les intervalles ne contenant qu'une seule racine en s'appuyant sur les th\u00e9or\u00e8mes suivant. Th\u00e9or\u00e8me des valeurs interm\u00e9diaires Soit \\(f\\) une fonction continue sur un intervalle \\(I=[a,b]\\) de \\(\\mathbb{R}\\) . \\(f\\) atteint toutes les valeurs interm\u00e9diaires entre \\(f(a)\\) et \\(f(b)\\) . Autrement dit : - Si \\(f(a) \\leq f(b)\\) alors pour tout \\(d \\in [f(a),f(b)]\\) il existe un \\(c \\in [a,b]\\) tel que \\(f(c)=d\\) . - Si \\(f(a) \\geq f(b)\\) alors pour tout \\(d \\in [f(b),f(a)]\\) il existe un \\(c \\in [a,b]\\) tel que \\(f(c)=d\\) . D'o\u00f9 le corollaire : Th\u00e9or\u00e8me de Bolzano Soit une fonction continue \\(f:[a,b] \\longrightarrow \\mathbb{R}\\) Si \\(f(a)f(b)<0\\) alors il existe au moins un \\(c \\in ]a,b[\\) tel que \\(f(c)=0\\) . Le th\u00e9or\u00e8me de Bolzano garantit l'existence d'au moins une racine mais pas son unicit\u00e9 dans \\([a,b]\\) . Pour assurer l'unicit\u00e9 d'une racine dans \\([a,b]\\) , on essaiera d'appliquer le th\u00e9or\u00e8me de la bijection avec le th\u00e9or\u00e8me des valeurs interm\u00e9diaires . Th\u00e9or\u00e8me de la bijection + Th\u00e9or\u00e8me des valeurs interm\u00e9diaires Soit \\(f\\) une fonction continue et strictement monotone sur \\(I=[a,b]\\) de \\(\\mathbb{R}\\) alors \\(f\\) induit une bijection de \\(I\\) dans \\(f(I)\\) . Si de plus \\(f(a)f(b)<0\\) alors il existe un unique \\(c \\in ]a,b[\\) tel que \\(f(c)=0\\) . Autrement dit, la fonction \\(f\\) s'annule une seule fois dans \\(]a,b[\\) . L'\u00e9tude pr\u00e9alable de la fonction doit donc avoir pour but de s\u00e9parer les racines en isolant des intervalles sur lesquels la fonction est strictement monotone et change de signe . M\u00e9thodes num\u00e9riques et convergence Comme \u00e9voqu\u00e9 pr\u00e9c\u00e9demment, l'id\u00e9e est d'approximer la racine d'une fonction lorsque l'on est incapables de trouver la solution analytiquement. Les m\u00e9thodes num\u00e9riques de recherche de racines sont en g\u00e9n\u00e9ral des m\u00e9thodes it\u00e9ratives . Il s'agit de construire une suite \\(x_{n+1} = g(x_n)\\) telle que \\(\\lim\\limits_{n \\to \\infty} x_n = c\\) o\u00f9 \\(c\\) est la racine \u00e0 approcher. La performance de ces m\u00e9thodes est \u00e9valu\u00e9e par leur vitesse de convergence : D\u00e9finition Soit \\(c\\) la racine recherch\u00e9e. Posons \\(e_n = x_n - c\\) l' erreur absolue \u00e0 l'it\u00e9ration \\(n\\) . La suite est dite convergente d'ordre \\(p \\geq 1\\) si il existe une constante \\(K>0\\) tel que : \\(\\lim\\limits_{n \\to \\infty} \\frac{\\mid e_{n+1} \\mid}{\\mid e_n \\mid^p} \\leq K\\) La convergence est d'autant plus rapide que la valeur de \\(p\\) est grande. \\(K\\) est le facteur de convergence de la suite. Voici comment on qualifie la convergence en fonction de \\(p\\) et \\(K\\) : Valeur de \\(p\\) Valeur de \\(K\\) Convergence \\(1\\) \\(0\\) Super-lin\u00e9aire \\(1\\) \\(]0,1[\\) Lin\u00e9aire \\(1\\) \\(1\\) Logarithmique \\(1\\) \\(>1\\) Sous-lin\u00e9aire (non-convergence) \\(2\\) Quadratique \\(3\\) Cubique \\(4\\) Quartique Dans la pratique, la racine \u00e9tant inconnue, nous ne pouvons pas calculer l'erreur \\(e_n\\) . C'est pourquoi, \u00e0 chaque it\u00e9ration, on calcule plut\u00f4t le r\u00e9sidu \\(r_n = f(x_n)\\) . On consid\u00e8re que la suite est suffisamment proche de la racine si \\(r_n < \\varepsilon\\) avec \\(\\varepsilon\\) la pr\u00e9cision choisie. La convergence des m\u00e9thodes it\u00e9ratives de recherche de racines d\u00e9pend en g\u00e9n\u00e9ral du choix de la donn\u00e9e initiale \\(x_0\\) : Une m\u00e9thode qui converge quelque soit \\(x_0\\) est dite globalement convergente . Une m\u00e9thode qui converge seulement lorque \\(x_0\\) est au voisinage de la racine est dite localement convergente . De mani\u00e8re g\u00e9n\u00e9rale, les m\u00e9thodes localement convergentes on un ordre de convergence plus grand que les m\u00e9thodes globalement convergentes. Exemple de probl\u00e8me Au cours de ce chapitre, nous appliquerons les diff\u00e9rentes m\u00e9thodes num\u00e9riques de recherche de racines \u00e0 un m\u00eame exemple : l'estimation de \\(\\sqrt{2}\\) . \\(\\sqrt{2}\\) est un nombre irrationel, dont l'approximation est un probl\u00e8me depuis l'antiquit\u00e9, notamment parce qu'il correspond \u00e0 l'hypoth\u00e9nuse d'un carr\u00e9 de c\u00f4t\u00e9 1. En Physique, \\(\\sqrt{2}\\) est impliqu\u00e9 dans de nombreuses formules. Par exemple, le calcul de la valeur efficace d'une tension sinuso\u00efdale. Une valeur approch\u00e9e de \\(\\sqrt{2}\\) \u00e0 \\(10^{-9}\\) pr\u00e8s est : 1.414213562. Par d\u00e9finition, \\(\\sqrt{2}\\) et \\(-\\sqrt{2}\\) sont les solutions de l'\u00e9quation \\(x^2 = 2\\) . R\u00e9soudre cette \u00e9quation revient \u00e0 r\u00e9soudre \\(x^2 - 2 = 0\\) . Pour calculer une approximation de \\(\\sqrt{2}\\) , on peut donc chercher les racines de la fonction \\(f(x) = x^2 - 2\\) de \\(\\mathbb{R}\\) dans \\(\\mathbb{R}\\) . La voici sous la forme d'une fonction Python : def f(x): return (x**2)-2 Cette fonction est continue et d\u00e9rivable sur \\(\\mathbb{R}\\) , et sa d\u00e9riv\u00e9e est \\(f'(x) = 2x\\) . On peut en d\u00e9duire ses variations : Th\u00e9or\u00e8me des valeurs interm\u00e9diaires : \\(f(1)=-1\\) et \\(f(2)=2\\) , d'o\u00f9 \\(f(1)f(2)<0\\) . Th\u00e9or\u00e8me de la bijection : \\(f\\) est continue et strictement monotone sur \\(I=[1,2]\\) , donc \\(f\\) induit une bijection de \\(I\\) dans \\(f(I)\\) . Donc, il existe une seule racine de \\(f\\) dans \\(]1,2[\\) , et nous savons que cette racine est \\(\\sqrt{2}\\) . C'est pourquoi dans la suite de ce chapitre, sauf indication contraire, nous chercherons la racine de \\(f\\) se trouvant sur l'intervalle \\(]1,2[\\) . M\u00e9thode de la dichotomie Algorithme La m\u00e9thode de la dichotomie est inspir\u00e9e du th\u00e9or\u00e8me des valeurs interm\u00e9diaires. Elle est aussi connue sous le nom de \"m\u00e9thode de la bissection\" . Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . L'id\u00e9e va \u00eatre de d\u00e9couper l'intervalle en 2, de chercher dans quelle moiti\u00e9 la racine se trouve, puis de d\u00e9finir le nouvel intervalle comme \u00e9tant cette moiti\u00e9. Et ainsi de suite pour converger vers la racine. On appelle \\([a_n,b_n]\\) l'intervalle de centre \\(x_n\\) \u00e0 l'it\u00e9ration \\(n\\) : Si \\(f(a_n)f(x_n)<0\\) alors la racine se trouve dans \\(]a_n,x_n[\\) . Si \\(f(x_n)f(b_n)<0\\) alors la racine se trouve dans \\(]x_n,b_n[\\) . On r\u00e9cup\u00e8re \\(x_n\\) \u00e0 la fin de l'algorithme. Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. a et b les bornes de l'intervalle de recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine. a_n et b_n les bornes de l'intervalle de recherche. r_n le r\u00e9sidu. def dichotomie(f,a,b,n_max,e): #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n = (a+b)/2 #Estimation de la racine a_n = a #Borne inf\u00e9rieure de l'intervalle de recherche b_n = b #Borne sup\u00e9rieure de l'intervalle de recherche r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme de dichotomie #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(a_n-b_n)>e)and(abs(r_n)>e): #Si la racine est dans ]a_n,x_n[ alors on remplace b_n par x_n: if (f(a_n)*f(x_n))<0: b_n = x_n #Si la racine est dans ]x_n,b_n[ alors on remplace a_n par x_n: if (f(x_n)*f(b_n))<0: a_n = x_n #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour l'estimation de la racine et le r\u00e9sidu : x_n = (a_n+b_n)/2 r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n Convergence La longueur de l'intervalle de recherche est divis\u00e9e par 2 \u00e0 chaque it\u00e9ration : \\(I_n = \\mid b_n-a_n \\mid = \\frac{\\mid b-a \\mid}{2^n}\\) Donc, l'erreur absolue \u00e0 l'it\u00e9ration \\(n \\geq 0\\) , \\(e_n=x_n-c\\) \\(\\mid e_n \\mid \\leq \\frac{I_n}{2} = \\frac{\\mid b-a \\mid}{2^{n+1}}\\) Ce qui entraine : \\(\\lim\\limits_{n \\to \\infty} \\mid e_n \\mid = 0\\) et \\(\\frac{\\mid e_{n+1} \\mid}{\\mid e_n \\mid} \\leq \\frac{1}{2}\\) La m\u00e9thode de la dichotomie est donc globalement convergente : elle converge quelque soit le point de d\u00e9part. (Si la \\(f\\) a plusieurs racines dans \\([a,b]\\) , la racine trouv\u00e9e d\u00e9pendra de l'intervalle). Sa convergence est lin\u00e9aire , donc elle est relativement lente . C'est pourquoi on utilise souvent cette m\u00e9thode juste pour initialiser une m\u00e9thode plus rapide. Pr\u00e9cision En fonction de la pr\u00e9cision souhait\u00e9e \\(\\varepsilon\\) , on peut calculer le nombre d'it\u00e9rations \\(m\\) pour approcher la racine. On cherche \\(m \\in \\mathbb{N}\\) tel que : \\(\\mid e_{m-1} \\mid \\leq \\frac{\\mid b-a \\mid}{2^m} \\leq \\varepsilon\\) Donc tel que : \\(2^m \\geq \\frac{\\mid b-a \\mid}{\\varepsilon}\\) Soit : \\(m \\geq log_2{\\frac{\\mid b-a \\mid}{\\varepsilon}} = \\frac{ln{\\frac{\\mid b-a \\mid}{\\varepsilon}}}{ln(2)} \\approx 1.4427 ln{\\frac{\\mid b-a \\mid}{\\varepsilon}}\\) Exemple Voici les 4 premi\u00e8res it\u00e9rations de la m\u00e9thode de la dichotomie appliqu\u00e9e \u00e0 notre probl\u00e8me exemple, pour un intervalle initial \\([1,2]\\) : Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode de la dichotomie, avec un intervalle initial \\([1,2]\\) , estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Retrouvez-vous bien le nombre d'it\u00e9rations th\u00e9orique ? Avant-propos : les m\u00e9thodes lin\u00e9aris\u00e9es Les m\u00e9thodes qui seront pr\u00e9sent\u00e9es dans la suite de ce chapitre sont des m\u00e9thodes lin\u00e9aris\u00e9es . Ce type de m\u00e9thodes s'appuit sur le d\u00e9veloppement de Taylor de \\(f\\) autour de sa racine \\(c\\) : \\(f(c) = 0 = f(x') + (c-x')f'(\\xi)\\) o\u00f9 \\(\\xi \\in [x',c]\\) et \\(f(x') \\neq 0\\) D'o\u00f9 \\(c = x' - \\frac{f(x')}{f'(\\xi)}\\) Donc, si on connait \\(\\xi\\) , on peut d\u00e9terminer \\(c\\) \u00e0 partir de \\(x'\\) . D'un point de vue g\u00e9om\u00e9trique, la racine \\(c\\) est \u00e0 l'intersection entre la droite passant par le point \\((x',f'(x'))\\) et de pente \\(f'(\\xi)\\) et donc d'\u00e9quation : \\(y = f'(\\xi) x + f(x') - f'(\\xi) x'\\) et l'axe \\((Ox)\\) donc d'\u00e9quation \\(y = 0\\) . D'o\u00f9 la m\u00e9thode it\u00e9rative suivante : \\(f(x_n) + (x_{n+1} - x_n) q_n = 0\\) ou encore l' \u00e9quation de r\u00e9currence : \\(x_{n+1} = x_n - \\frac{f(x_n)}{q_n}\\) o\u00f9 \\(q_n\\) est une approximation de \\(f'(\\xi)\\) . L'id\u00e9e des m\u00e9thodes lin\u00e9aris\u00e9es est donc : D'approcher \u00e0 chaque it\u00e9ration la fonction par une droite de pente \\(q_n\\) passant par le point \\((x_n,f(x_n))\\) . D\u00e9terminer \u00e0 chaque it\u00e9ration \\(x_{n+1}\\) comme l'intersection entre l'axe \\((Ox)\\) et cette droite. Les m\u00e9thodes lin\u00e9aris\u00e9es (m\u00e9thode de la s\u00e9cante, m\u00e9thode de la fausse position, m\u00e9thode de Newton, etc.) se diff\u00e9rentient par le choix de \\(q_n\\) . M\u00e9thode de la s\u00e9cante Algorithme La m\u00e9thode de la s\u00e9cante est une m\u00e9thode lin\u00e9aris\u00e9e pour laquelle : \\(q_n = \\frac{f(x_n)-f(x_{n-1})}{x_n-x_{n-1}}\\) Cette suite correspond \u00e0 la droite passant par les points \\((x_n,f(x_n))\\) et \\((x_{n-1},f(x_{n-1}))\\) . Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. a et b les bornes de l'intervalle de recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine \u00e0 l'it\u00e9ration n. x_n_old l'estimation de la racine \u00e0 l'it\u00e9ration n-1. r_n le r\u00e9sidu. def secante(f,a,b,n_max,e): #Initialisation des variables : n = 1 #Nombre d'it\u00e9rations x_n = a #Estimation de la racine \u00e0 l'it\u00e9ration n x_n_old = b #Estimation de la racine \u00e0 l'it\u00e9ration n-1 r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme de la s\u00e9cante #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(x_n-x_n_old)>e)and(abs(r_n)>e): #Calculer la pente de la droite : q_n = (f(x_n)-f(x_n_old))/(x_n-x_n_old) #Mettre \u00e0 jour l'estimation de la racine : x_n_old = x_n #Iteration n x_n = x_n - f(x_n)/q_n #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour le r\u00e9sidu : r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n Convergence La m\u00e9thode de la s\u00e9cante est convergente localement . Si \\(f'(c) \\neq 0\\) , alors on peut d\u00e9montrer qu'elle converge avec un ordre \\(p = \\frac{1+\\sqrt(5)}{2}\\) . Cette valeur est connue sous le nom de \"nombre d'or\". On en d\u00e9duit que sous ces conditions, la convergence de la m\u00e9thode est lin\u00e9aire . Exemple Voici les 5 premi\u00e8res it\u00e9rations de la m\u00e9thode de la s\u00e9cante appliqu\u00e9e \u00e0 notre probl\u00e8me exemple. L'intervalle initial est ici de [0,2] pour des raisons de lisibilit\u00e9 : Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode de la s\u00e9cante, avec un intervalle initial \\([1,2]\\) , estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Comparez cette valeur \u00e0 celle obtenue pour la m\u00e9thode de la dichotomie. M\u00e9thode de la fausse position Algorithme La m\u00e9thode de la fausse position est une m\u00e9thode lin\u00e9aris\u00e9e pour laquelle : \\(q_n = \\frac{f(x_n)-f(x_m)}{x_n-x_m}\\) Cette suite correspond \u00e0 la droite passant par les points \\((x_n,f(x_n))\\) et \\((x_m,f(x_m))\\) , o\u00f9 \\(m\\) est le plus grand indice inf\u00e9rieur \u00e0 \\(n\\) tel que \\(f(x_n)f(x_m)<0\\) . Il s'agit d'un m\u00e9lange entre la m\u00e9thode de la dichotomie et la m\u00e9thode de la s\u00e9cante. On l'appelle aussi m\u00e9thode de regula falsi ou m\u00e9thode de Lagrange . Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. a et b les bornes de l'intervalle de recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine. a_n et b_n les bornes de l'intervalle de recherche. r_n le r\u00e9sidu. def fausse_position(f,a,b,n_max,e): #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations a_n = a #Borne inf\u00e9rieure de l'intervalle de recherche b_n = b #Borne sup\u00e9rieure de l'intervalle de recherche q_n = (f(b_n)-f(a_n))/(b_n-a_n) #Initialiser la pente de la droite x_n = a_n-f(a_n)/q_n #Estimation de la racine r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme de la fausse-position #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(a_n-b_n)>e)and(abs(r_n)>e): #Si la racine est dans ]a_n,x_n[ alors on remplace b_n par x_n: if (f(a_n)*f(x_n))<0: b_n = x_n #Si la racine est dans ]x_n,b_n[ alors on remplace a_n par x_n: if (f(x_n)*f(b_n))<0: a_n = x_n #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Calcul de la nouvelle pente de la droite : q_n = (f(b_n)-f(a_n))/(b_n-a_n) #Mettre \u00e0 jour l'estimation de la racine et le r\u00e9sidu : x_n = x_n-f(x_n)/q_n r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n Convergence La m\u00e9thode de la fausse position est globalement convergente : Si \\(f\\) est \u00e0 concavit\u00e9 constante avec \\(f\"<0\\) (concave), la suite converge vers la racine avec \\(f(x_n)\\) croissant. Si \\(f\\) est \u00e0 concavit\u00e9 constante avec \\(f\">0\\) (convexe), la suite converge vers la racine avec \\(f(x_n)\\) d\u00e9croissant. Elle converge avec un ordre \\(p\\) d'au moins 1 de fa\u00e7on super-lin\u00e9aire . Exemple Voici les 5 premi\u00e8res it\u00e9rations de la m\u00e9thode de la fausse position appliqu\u00e9e \u00e0 notre probl\u00e8me exemple. L'intervalle initial est ici de [0,2] pour des raisons de lisibilit\u00e9 : Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode de la fausse position, avec un intervalle initial \\([1,2]\\) , estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Comparez cette valeur \u00e0 celle obtenue pour la m\u00e9thode de la s\u00e9cante. M\u00e9thode de Newton Algorithme La m\u00e9thode de Newton est une m\u00e9thode lin\u00e9aris\u00e9e pour laquelle : \\(q_n = f'(x_n)\\) Cette suite correspond \u00e0 la pente de la tangente \u00e0 la fonction \\(f\\) en \\(x_n\\) . On l'appelle aussi la m\u00e9thode de Newton-Raphson . Cette m\u00e9thode n\u00e9cessite l'\u00e9valuation de \\(f\\) et de sa d\u00e9riv\u00e9e \\(f'\\) . Dans le cas de notre probl\u00e8me exemple, on d\u00e9finira : def f_derivee(x): return 2*x Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . On choisi d'initialiser la m\u00e9thode avec \\(x_0 \\in [a,b]\\) . Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. f_derivee la d\u00e9riv\u00e9e de la fonction dont on cherche les racines. x_0 point de d\u00e9part de la recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine \u00e0 l'it\u00e9ration n. x_n_old l'estimation de la racine \u00e0 l'it\u00e9ration n-1. r_n le r\u00e9sidu. def newton(f,f_derivee,x_0,n_max,e): #V\u00e9rifier que le point de d\u00e9part de la recherche est possible : if f_derivee(x_0)==0: raise ValueError(\"Mauvaise initialisation, f'(x_0) = 0\") #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = x_0 #Estimation de la racine \u00e0 l'it\u00e9ration n-1 x_n = x_n_old-f(x_n_old)/f_derivee(x_n_old) #Estimation de la racine \u00e0 l'it\u00e9ration n r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme de Newton #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(x_n-x_n_old)>e)and(abs(r_n)>e): #Mettre \u00e0 jour l'estimation de la racine : x_n_old = x_n #It\u00e9ration n x_n = x_n-f(x_n)/f_derivee(x_n) #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour le r\u00e9sidu : r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n Convergence La m\u00e9thode de Newton est convergente localement . Si \\(x_0\\) est assez proche de la racine \\(c\\) , et \\(f'(c) \\neq 0\\) , on peut montrer que la m\u00e9thode converge avec un ordre \\(p=2\\) . La convergence est donc quadratique . En pratique, on utilise souvent la m\u00e9thode de la dichotomie pour trouver un point de d\u00e9part \\(x_0\\) suffisamment proche de \\(c\\) avant d'utiliser la m\u00e9thode de Newton. Reste un incov\u00e9nient, la m\u00e9thode de Newton n\u00e9cessite le calcul de d\u00e9riv\u00e9es, et est donc plus co\u00fbteuse en calcul. La m\u00e9thode de Newton est s\u00fbrement convergente dans le cas d'une fonction \\(f\\) strictement monotone et ne pr\u00e9sentant pas de points d'inflexion dans l'intervalle \\([a,b]\\) (i.e. \\(f\"(x)\\) ne change pas de signe donc \\(f\\) a une concavit\u00e9 constante) si le point de d\u00e9part \\(x_0\\) est tel que : \\(f(x_0)f\"(x_0)>0\\) Pour choisir un point de d\u00e9part \\(x_0\\) qui ne risque pas de faire diverger la m\u00e9thode de Newton, il faut donc s'assurer de respecter cette condition. Exemple Voici les 4 premi\u00e8res it\u00e9rations de la m\u00e9thode de Newton appliqu\u00e9e \u00e0 notre probl\u00e8me exemple. Le point de d\u00e9part de la recherche choisi est de 2.5 pour des raisons de lisibilit\u00e9 : On note que le choix de point de d\u00e9part est correct, car \\(f\\) est \u00e0 concavit\u00e9 constante positive ( \\(f\"(x)=2>0\\) ) et \\(f(x_0)=4.25>0\\) , ce qui signifie que \\(f(x_0)f\"(x_0)>0\\) est bien respect\u00e9e. Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode de Newton, avec un point de d\u00e9part de votre choix, estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Comparez cette valeur \u00e0 celle obtenue pour les m\u00e9thodes pr\u00e9c\u00e9dentes. Quelle est la m\u00e9thode lin\u00e9aris\u00e9e la plus rapide ? NB: La m\u00e9thode de Newton appliqu\u00e9e au cas de l'estimation de \\(\\sqrt{2}\\) est un cas particulier de la c\u00e9l\u00e8bre m\u00e9thode d'H\u00e9ron d'Alexandrie. M\u00e9thode du point fixe D\u00e9finitions Id\u00e9e Transformer l'\u00e9quation \\(f(x)=0\\) en une \u00e9quation \u00e9quivalente \\(g(x)=x\\) o\u00f9 \\(g\\) est une fonction auxiliaire bien choisie. La racine \\(c\\) est alors un point fixe de \\(g\\) et approcher les z\u00e9ros de \\(f\\) revient \u00e0 approcher les points fixes de \\(g\\) . Cette transformation est toujours possible mais pas unique . Dans la pratique, la m\u00e9thode du point fixe consiste \u00e0 transformer le probl\u00e8me \\(f(x)=0\\) o\u00f9 \\(f:[a,b] \\rightarrow \\mathbb{R}\\) en un probl\u00e8me \u00e9quivalent \\(x=g(x)\\) , en passant par un sch\u00e9ma it\u00e9ratif \\(x_{n+1}=g(x_n)\\) o\u00f9 \\(g\\) est une fonction bien choisie. L'it\u00e9ration est dite de point fixe , et la fonction \\(g\\) est la fonction d'it\u00e9ration associ\u00e9e. Th\u00e9or\u00e8me du point fixe Soit \\(g\\) une fonction continue et \\((x_n)\\) une suite g\u00e9n\u00e9r\u00e9e par l'it\u00e9ration de point fixe \\(x_{n+1}=g(x_n)\\) . Si \\(\\lim\\limits_{n \\to \\infty} x_n = c\\) alors par continuit\u00e9 de \\(g\\) : \\(\\lim\\limits_{n \\to \\infty} g(x_n) = g(c) = c\\) Donc \\(c\\) est un point fixe de \\(g\\) . Les m\u00e9thodes lin\u00e9aris\u00e9es sont des m\u00e9thodes de point fixe : on obtient \\(x_{n+1}\\) \u00e0 partir de \\(x_n\\) en \u00e9valuant toujours la m\u00eame expression \\(x_{n+1}=g(x_n)\\) . Par exemple, dans notre probl\u00e8me de l'approximation de \\(\\sqrt{2}\\) , r\u00e9soudre \\(x^2-2=0\\) revient \u00e0 trouver le point fixe de \\(g(x)=x-\\frac{f(x)}{f'(x)}=\\frac{x+\\frac{2}{x}}{2}\\) . Algorithme Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . On choisi d'initialiser la m\u00e9thode avec \\(x_0 \\in [a,b]\\) . Cette m\u00e9thode n\u00e9cessite la s\u00e9lection d'une fonction d'it\u00e9ration de point fixe \\(g\\) . Dans le cas de notre probl\u00e8me exemple, on pourra choisir : def g(x): return x/2+1/x Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. g la fonction d'it\u00e9ration choisie. x_0 point de d\u00e9part de la recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine \u00e0 l'it\u00e9ration n. x_n_old l'estimation de la racine \u00e0 l'it\u00e9ration n-1. r_n le r\u00e9sidu. def point_fixe(f,g,x_0,n_max,e): #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = x_0 #Estimation de la racine \u00e0 l'it\u00e9ration n-1 x_n = g(x_n_old) #Estimation de la racine \u00e0 l'it\u00e9ration n r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme du point fixe #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(x_n-x_n_old)>e)and(abs(r_n)>e): #Mettre \u00e0 jour l'estimation de la racine : x_n_old = x_n #It\u00e9ration n x_n = g(x_n) #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour le r\u00e9sidu : r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n Convergence Th\u00e9or\u00e8me de la convergence globale des it\u00e9rations de point fixe Soit \\(g\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . 1. Hypoth\u00e8se d'inclusion (ou de stabilit\u00e9) : Si \\(\\forall x \\in [a,b]\\) , \\(g(x) \\in [a,b]\\) alors \\(g\\) admet un point fixe dans \\([a,b]\\) . 2. Hypoth\u00e8se de contraction stricte : Si de plus, \\(\\exists K \\in ]0,1[\\) tel que \\(\\mid g(x)-g(y) \\mid \\leq K \\mid x-y \\mid \\forall x,y \\in [a,b]\\) (on dit que \\(g\\) est strictement contractante ) alors \\(g\\) admet un point fixe unique not\u00e9 \\(c\\) dans \\([a,b]\\) et la suite \\(x_{n+1}=g(x_n)\\) converge vers \\(c\\) pour toute valeur de d\u00e9part \\(x_0\\) dans \\([a,b]\\) . On appelle alors \\(c\\) un point attracteur . 2 r\u00e8gles pratiques pour v\u00e9rifier l'hypoth\u00e8se de contraction : Th\u00e9or\u00e8me Soit \\(g\\) une fonction continue et d\u00e9rivable de \\([a,b]\\) dans \\(\\mathbb{R}\\) . Si \\(\\forall x\\in [a,b]\\) , \\(\\mid g'(x) \\mid <1\\) alors \\(g\\) est strictement contractante sur \\([a,b]\\) . Si \\(\\forall x\\in [a,b]\\) , \\(\\mid g'(x) \\mid >1\\) alors la suite diverge si \\(x_0 \\neq c\\) . On parle alors de point r\u00e9pulsif En pratique, il est souvent difficile de montrer la convergence globale. D'o\u00f9 l'utilit\u00e9 d'une \u00e9tude de convergence locale : Th\u00e9or\u00e8me de la convergence locale des it\u00e9rations de point fixe Soit \\(c\\) un point fixe d'une fonction \\(g\\) d\u00e9rivable au voisinage de \\(c\\) . Si \\(\\mid g'(c) \\mid < 1\\) alors il existe \\(\\delta >0\\) tel que la suite \\(x_{n+1}=g(x_n)\\) converge vers \\(c\\) pour tout \\(x_0\\) tel que \\(\\mid x_0-c \\mid < \\delta\\) . Si \\(\\mid g'(c) \\mid > 1\\) la convergence est impossible. Si \\(\\mid g'(c) \\mid = 1\\) on peut avoir convergence ou divergence. Exemple Consid\u00e9rons \u00e0 nouveau notre probl\u00e8me d'approximation de \\(\\sqrt{2}\\) par la recherche des racines de \\(f(x)=x^2-2\\) . Nous proposons d'essayer les fonctions d'it\u00e9ration suivantes : \\(g_1(x)=\\frac{2}{x}\\) \\(g_2(x)=2x-\\frac{2}{x}\\) \\(g_3(x)=\\frac{x}{2}+\\frac{1}{x}\\) \\(g_1'(x)=-\\frac{2}{x^2}\\) \\(g_2'(x)=2+\\frac{2}{x^2}\\) \\(g_3'(x)=\\frac{1}{2}-\\frac{1}{x^2}\\) \\(g_1'(c)=-1\\) \\(g_2'(c)=3\\) \\(g_3'(c)=0\\) On s'attend donc \u00e0 ce que \\(g_1\\) et \\(g_2\\) divergent, et \u00e0 ce que \\(g_3\\) converge. V\u00e9rifions avec un point de d\u00e9part de 2 pour les 3 fonctions. Voici les 4 premi\u00e8res it\u00e9rations pour \\(g_1(x)=\\frac{2}{x}\\) : La valeur de \\(x_n\\) oscille entre 1 et 2 sans jamais s'arr\u00eater. La suite ne converge donc pas. Voici les 4 premi\u00e8res it\u00e9rations pour \\(g_2(x)=2x-\\frac{2}{x}\\) : On observe que \\(x_n\\) diverge en escalier. Voici les 4 premi\u00e8res it\u00e9rations pour \\(g_3(x)=\\frac{x}{2}+\\frac{1}{x}\\) : Cette fois-ci, \\(x_n\\) converge rapidement vers \\(\\sqrt{2}\\) . On retrouve donc bien les r\u00e9sultats attendus. Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode du point fixe, avec le m\u00eame point de d\u00e9part que vous avez choisi pour la m\u00e9thode de Newton, estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Comparez cette valeur \u00e0 celle obtenue pour la m\u00e9thode de Newton. Expliquez pourquoi ces r\u00e9sultats sont identiques. Vitesse de convergence des m\u00e9thodes M\u00e9thode de la dichotomie On rappelle que la m\u00e9thode de la dichotomie converge de mani\u00e8re lin\u00e9aire . M\u00e9thodes de point fixe Supposons que \\(g\\) est d\u00e9rivable \\(p \\geq 1\\) fois dans \\([a,b]\\) contenant le point fixe \\(c\\) . La formule de Taylor au voisinage de \\(c\\) , \u00e0 l'it\u00e9ration \\(n\\) est : \\(g(x_n) = g(c) + g'(c)(x_n-c) + g\"(c)\\frac{(x_n-c)^2}{2!} + ... + g^{(p)}(c)\\frac{(x_n-c)^p}{p!}\\) D'o\u00f9 l'erreur absolue \u00e0 l'it\u00e9ration \\(n+1\\) : \\(e_{n+1} = x_{n+1}-c = g(x_n)-g(c) = g'(c)e_n + g\"(c)\\frac{e_n^2}{2!} + ... + g^{(p)}(c)\\frac{e_n^p}{p!}\\) Th\u00e9or\u00e8me La m\u00e9thode du point fixe est d'ordre \\(p>1\\) si et seulement si : \\(g^{(i)}(c)=0\\) \\(\\forall 1 \\leq i \\leq p-1\\) et \\(g^{(p)}(c) \\neq 0\\) La m\u00e9thode est convergente du 1er ordre (i.e. lin\u00e9aire ) si \\(g'(c) \\neq 0\\) et \\(\\lim\\limits_{n \\to \\infty} \\frac{\\mid e_{n+1} \\mid}{\\mid e_n \\mid} = \\mid g'(c) \\mid < 1\\) . (C'est ce que l'on appelle le \"facteur de r\u00e9duction\" de l'erreur). La m\u00e9thode est convergente d'ordre 2 (i.e. quadratique ) si \\(g'(c)=0\\) et \\(g\"(c) \\neq 0\\) et \\(\\lim\\limits_{n \\to \\infty} \\frac{\\mid e_{n+1} \\mid}{\\mid e_n \\mid^2} = \\frac{1}{2} \\mid g\"(c) \\mid\\) . M\u00e9thode de la s\u00e9cante On rappelle que dans le cas de la m\u00e9thode de la s\u00e9cante : \\(g(x) = x_0-f(x_0)\\frac{x-x_0}{f(x)-f(x_0)}\\) Donc \\(g'(x) = -f(x_0) \\frac{f(x)-f(x_0)-(x-x_0)f'(x)}{(f(x)-f(x_0))^2}\\) D'o\u00f9 \\(g'(c) = \\frac{f(x_0)+f'(c)(c-x_0)}{f(x_0)}\\) D'apr\u00e8s la formule de Taylor, il existe au moins un \\(c \\in ]a,x_0[\\) tel que : \\(f(x_0)+f'(c)(c-x_0) = f\"(c)\\frac{(c-x_0)^2}{2!}\\) Donc s'il n'y a pas de point d'inflexion, \\(g'(c) \\neq 0\\) et la m\u00e9thode converge lin\u00e9airement . M\u00e9thode de Newton On rappelle que dans le cas de la m\u00e9thode de Newton : \\(g(x) = x-\\frac{f(x)}{f'(x)}\\) Donc \\(g'(x) = 1 - \\frac{f'(x)f'(x)-f(x)f\"(x)}{f'(x)^2} = \\frac{f(x)f\"(x)}{f'(x)^2}\\) D'o\u00f9 \\(g'(c) = \\frac{f(c)f\"(c)}{f'(c)^2} = 0\\) De plus, \\(g\"(x) = \\frac{(f'(x)f\"(x)+f(x)f\"'(x))f'(x)^2-f(x)f\"(x)(2f\"(x)f'(x))}{f'(x)^4}\\) D'o\u00f9 \\(g\"(c)=\\frac{f\"(c)}{f'(c)}\\) Par cons\u00e9quent, si \\(c\\) est un z\u00e9ro simple de la fonction \\(f\\) (i.e. \\(f\"(c) \\neq 0\\) ) alors la m\u00e9thode converge quadratiquement . Si \\(f\"(c) = 0\\) , la convergence est d'ordre sup\u00e9rieur \u00e0 2 . Conclusions La m\u00e9thode de la dichotomie est simple mais lente car elle ne prend pas en compte le comportement de \\(f\\) . Elle est n\u00e9anmoins utile pour initialiser des m\u00e9thodes plus rapides . La m\u00e9thode de Newton est convergente d'ordre au moins 2 mais n\u00e9cessite le calcul de d\u00e9riv\u00e9es et un bon choix d'initialisation . Les m\u00e9thodes d'it\u00e9rations de point fixe convergent sous des conditions portant sur la fonction d'it\u00e9ration \\(g\\) et sa d\u00e9riv\u00e9e \\(g'\\) . Leur convergence est g\u00e9n\u00e9ralement lin\u00e9aire , mais devient quadratique quand \\(g'(c)=0\\) .","title":"II. Recherche de racines"},{"location":"Chap2_Recherche_de_racines/#chapitre-ii-recherche-de-racines","text":"Ce chapitre porte sur les m\u00e9thodes num\u00e9riques pour la recherche de racines d'une fonction.","title":"Chapitre II : Recherche de racines"},{"location":"Chap2_Recherche_de_racines/#position-du-probleme","text":"","title":"Position du probl\u00e8me"},{"location":"Chap2_Recherche_de_racines/#motivation","text":"D\u00e9finition Soit f une fonction d\u00e9finie et continue de \\(\\mathbb{R}\\) dans \\(\\mathbb{R}\\) . Les racines ou z\u00e9ros de cette fonction sont les valeurs \\(x\\) qui v\u00e9rifient l'\u00e9quation \\(f(x)=0\\) Le principal int\u00e9r\u00eat des m\u00e9thodes de recherche de racines est de pouvoir r\u00e9soudre des \u00e9quations . En effet, trouver \\(x \\in \\mathbb{R}\\) tel que \\(f(x)=c\\) revient \u00e0 chercher les racines de la fonction \\(f(x)-c\\) . Dans certains cas, on peut trouver analytiquement les racines d'une fonction. Toute \u00e9quation polynomiale de degr\u00e9 \\(n\\) a exactement \\(n\\) solutions dans \\(\\mathbb{C}\\) et au plus \\(n\\) solutions dans \\(\\mathbb{R}\\) . Mais d'apr\u00e8s la th\u00e9orie d'Evariste Galois, \u00e0 partir du degr\u00e9 5 il n'existe plus de formule g\u00e9n\u00e9rale de r\u00e9solution . Si l'\u00e9quation n'est pas polynomiale, sa r\u00e9solution analytique est encore moins probable. D'o\u00f9 l'int\u00e9r\u00eat d'utiliser des m\u00e9thodes de r\u00e9solution num\u00e9riques , afin d'approcher les valeurs des racines. Le principe est le suivant : Localiser grossi\u00e8rement les racines en proc\u00e9dant \u00e0 des \u00e9valuations graphiques. Construire une suite qui converge vers chaque racine. Ce chapitre pr\u00e9sentera un panel de m\u00e9thodes num\u00e9riques de recherche de racines , que nous appliquerons \u00e0 un m\u00eame exemple pour illustration.","title":"Motivation"},{"location":"Chap2_Recherche_de_racines/#existence-et-localisation-des-racines","text":"Que l'approche soit analytique ou num\u00e9rique, la 1\u00e8re \u00e9tape consiste g\u00e9n\u00e9ralement \u00e0 localiser les solutions de l'\u00e9quation. Pour ce faire, on d\u00e9termine les intervalles \\([a,b]\\) contenant une unique racine . C'est ce que l'on appelle la s\u00e9paration des racines . Cette d\u00e9termination se fait graphiquement et/ou analytiquement de la mani\u00e8re suivante : Etude des variations de la fonction \\(f\\) . Trouver les intervalles ne contenant qu'une seule racine en s'appuyant sur les th\u00e9or\u00e8mes suivant. Th\u00e9or\u00e8me des valeurs interm\u00e9diaires Soit \\(f\\) une fonction continue sur un intervalle \\(I=[a,b]\\) de \\(\\mathbb{R}\\) . \\(f\\) atteint toutes les valeurs interm\u00e9diaires entre \\(f(a)\\) et \\(f(b)\\) . Autrement dit : - Si \\(f(a) \\leq f(b)\\) alors pour tout \\(d \\in [f(a),f(b)]\\) il existe un \\(c \\in [a,b]\\) tel que \\(f(c)=d\\) . - Si \\(f(a) \\geq f(b)\\) alors pour tout \\(d \\in [f(b),f(a)]\\) il existe un \\(c \\in [a,b]\\) tel que \\(f(c)=d\\) . D'o\u00f9 le corollaire : Th\u00e9or\u00e8me de Bolzano Soit une fonction continue \\(f:[a,b] \\longrightarrow \\mathbb{R}\\) Si \\(f(a)f(b)<0\\) alors il existe au moins un \\(c \\in ]a,b[\\) tel que \\(f(c)=0\\) . Le th\u00e9or\u00e8me de Bolzano garantit l'existence d'au moins une racine mais pas son unicit\u00e9 dans \\([a,b]\\) . Pour assurer l'unicit\u00e9 d'une racine dans \\([a,b]\\) , on essaiera d'appliquer le th\u00e9or\u00e8me de la bijection avec le th\u00e9or\u00e8me des valeurs interm\u00e9diaires . Th\u00e9or\u00e8me de la bijection + Th\u00e9or\u00e8me des valeurs interm\u00e9diaires Soit \\(f\\) une fonction continue et strictement monotone sur \\(I=[a,b]\\) de \\(\\mathbb{R}\\) alors \\(f\\) induit une bijection de \\(I\\) dans \\(f(I)\\) . Si de plus \\(f(a)f(b)<0\\) alors il existe un unique \\(c \\in ]a,b[\\) tel que \\(f(c)=0\\) . Autrement dit, la fonction \\(f\\) s'annule une seule fois dans \\(]a,b[\\) . L'\u00e9tude pr\u00e9alable de la fonction doit donc avoir pour but de s\u00e9parer les racines en isolant des intervalles sur lesquels la fonction est strictement monotone et change de signe .","title":"Existence et localisation des racines"},{"location":"Chap2_Recherche_de_racines/#methodes-numeriques-et-convergence","text":"Comme \u00e9voqu\u00e9 pr\u00e9c\u00e9demment, l'id\u00e9e est d'approximer la racine d'une fonction lorsque l'on est incapables de trouver la solution analytiquement. Les m\u00e9thodes num\u00e9riques de recherche de racines sont en g\u00e9n\u00e9ral des m\u00e9thodes it\u00e9ratives . Il s'agit de construire une suite \\(x_{n+1} = g(x_n)\\) telle que \\(\\lim\\limits_{n \\to \\infty} x_n = c\\) o\u00f9 \\(c\\) est la racine \u00e0 approcher. La performance de ces m\u00e9thodes est \u00e9valu\u00e9e par leur vitesse de convergence : D\u00e9finition Soit \\(c\\) la racine recherch\u00e9e. Posons \\(e_n = x_n - c\\) l' erreur absolue \u00e0 l'it\u00e9ration \\(n\\) . La suite est dite convergente d'ordre \\(p \\geq 1\\) si il existe une constante \\(K>0\\) tel que : \\(\\lim\\limits_{n \\to \\infty} \\frac{\\mid e_{n+1} \\mid}{\\mid e_n \\mid^p} \\leq K\\) La convergence est d'autant plus rapide que la valeur de \\(p\\) est grande. \\(K\\) est le facteur de convergence de la suite. Voici comment on qualifie la convergence en fonction de \\(p\\) et \\(K\\) : Valeur de \\(p\\) Valeur de \\(K\\) Convergence \\(1\\) \\(0\\) Super-lin\u00e9aire \\(1\\) \\(]0,1[\\) Lin\u00e9aire \\(1\\) \\(1\\) Logarithmique \\(1\\) \\(>1\\) Sous-lin\u00e9aire (non-convergence) \\(2\\) Quadratique \\(3\\) Cubique \\(4\\) Quartique Dans la pratique, la racine \u00e9tant inconnue, nous ne pouvons pas calculer l'erreur \\(e_n\\) . C'est pourquoi, \u00e0 chaque it\u00e9ration, on calcule plut\u00f4t le r\u00e9sidu \\(r_n = f(x_n)\\) . On consid\u00e8re que la suite est suffisamment proche de la racine si \\(r_n < \\varepsilon\\) avec \\(\\varepsilon\\) la pr\u00e9cision choisie. La convergence des m\u00e9thodes it\u00e9ratives de recherche de racines d\u00e9pend en g\u00e9n\u00e9ral du choix de la donn\u00e9e initiale \\(x_0\\) : Une m\u00e9thode qui converge quelque soit \\(x_0\\) est dite globalement convergente . Une m\u00e9thode qui converge seulement lorque \\(x_0\\) est au voisinage de la racine est dite localement convergente . De mani\u00e8re g\u00e9n\u00e9rale, les m\u00e9thodes localement convergentes on un ordre de convergence plus grand que les m\u00e9thodes globalement convergentes.","title":"M\u00e9thodes num\u00e9riques et convergence"},{"location":"Chap2_Recherche_de_racines/#exemple-de-probleme","text":"Au cours de ce chapitre, nous appliquerons les diff\u00e9rentes m\u00e9thodes num\u00e9riques de recherche de racines \u00e0 un m\u00eame exemple : l'estimation de \\(\\sqrt{2}\\) . \\(\\sqrt{2}\\) est un nombre irrationel, dont l'approximation est un probl\u00e8me depuis l'antiquit\u00e9, notamment parce qu'il correspond \u00e0 l'hypoth\u00e9nuse d'un carr\u00e9 de c\u00f4t\u00e9 1. En Physique, \\(\\sqrt{2}\\) est impliqu\u00e9 dans de nombreuses formules. Par exemple, le calcul de la valeur efficace d'une tension sinuso\u00efdale. Une valeur approch\u00e9e de \\(\\sqrt{2}\\) \u00e0 \\(10^{-9}\\) pr\u00e8s est : 1.414213562. Par d\u00e9finition, \\(\\sqrt{2}\\) et \\(-\\sqrt{2}\\) sont les solutions de l'\u00e9quation \\(x^2 = 2\\) . R\u00e9soudre cette \u00e9quation revient \u00e0 r\u00e9soudre \\(x^2 - 2 = 0\\) . Pour calculer une approximation de \\(\\sqrt{2}\\) , on peut donc chercher les racines de la fonction \\(f(x) = x^2 - 2\\) de \\(\\mathbb{R}\\) dans \\(\\mathbb{R}\\) . La voici sous la forme d'une fonction Python : def f(x): return (x**2)-2 Cette fonction est continue et d\u00e9rivable sur \\(\\mathbb{R}\\) , et sa d\u00e9riv\u00e9e est \\(f'(x) = 2x\\) . On peut en d\u00e9duire ses variations : Th\u00e9or\u00e8me des valeurs interm\u00e9diaires : \\(f(1)=-1\\) et \\(f(2)=2\\) , d'o\u00f9 \\(f(1)f(2)<0\\) . Th\u00e9or\u00e8me de la bijection : \\(f\\) est continue et strictement monotone sur \\(I=[1,2]\\) , donc \\(f\\) induit une bijection de \\(I\\) dans \\(f(I)\\) . Donc, il existe une seule racine de \\(f\\) dans \\(]1,2[\\) , et nous savons que cette racine est \\(\\sqrt{2}\\) . C'est pourquoi dans la suite de ce chapitre, sauf indication contraire, nous chercherons la racine de \\(f\\) se trouvant sur l'intervalle \\(]1,2[\\) .","title":"Exemple de probl\u00e8me"},{"location":"Chap2_Recherche_de_racines/#methode-de-la-dichotomie","text":"","title":"M\u00e9thode de la dichotomie"},{"location":"Chap2_Recherche_de_racines/#algorithme","text":"La m\u00e9thode de la dichotomie est inspir\u00e9e du th\u00e9or\u00e8me des valeurs interm\u00e9diaires. Elle est aussi connue sous le nom de \"m\u00e9thode de la bissection\" . Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . L'id\u00e9e va \u00eatre de d\u00e9couper l'intervalle en 2, de chercher dans quelle moiti\u00e9 la racine se trouve, puis de d\u00e9finir le nouvel intervalle comme \u00e9tant cette moiti\u00e9. Et ainsi de suite pour converger vers la racine. On appelle \\([a_n,b_n]\\) l'intervalle de centre \\(x_n\\) \u00e0 l'it\u00e9ration \\(n\\) : Si \\(f(a_n)f(x_n)<0\\) alors la racine se trouve dans \\(]a_n,x_n[\\) . Si \\(f(x_n)f(b_n)<0\\) alors la racine se trouve dans \\(]x_n,b_n[\\) . On r\u00e9cup\u00e8re \\(x_n\\) \u00e0 la fin de l'algorithme. Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. a et b les bornes de l'intervalle de recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine. a_n et b_n les bornes de l'intervalle de recherche. r_n le r\u00e9sidu. def dichotomie(f,a,b,n_max,e): #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n = (a+b)/2 #Estimation de la racine a_n = a #Borne inf\u00e9rieure de l'intervalle de recherche b_n = b #Borne sup\u00e9rieure de l'intervalle de recherche r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme de dichotomie #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(a_n-b_n)>e)and(abs(r_n)>e): #Si la racine est dans ]a_n,x_n[ alors on remplace b_n par x_n: if (f(a_n)*f(x_n))<0: b_n = x_n #Si la racine est dans ]x_n,b_n[ alors on remplace a_n par x_n: if (f(x_n)*f(b_n))<0: a_n = x_n #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour l'estimation de la racine et le r\u00e9sidu : x_n = (a_n+b_n)/2 r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n","title":"Algorithme"},{"location":"Chap2_Recherche_de_racines/#convergence","text":"La longueur de l'intervalle de recherche est divis\u00e9e par 2 \u00e0 chaque it\u00e9ration : \\(I_n = \\mid b_n-a_n \\mid = \\frac{\\mid b-a \\mid}{2^n}\\) Donc, l'erreur absolue \u00e0 l'it\u00e9ration \\(n \\geq 0\\) , \\(e_n=x_n-c\\) \\(\\mid e_n \\mid \\leq \\frac{I_n}{2} = \\frac{\\mid b-a \\mid}{2^{n+1}}\\) Ce qui entraine : \\(\\lim\\limits_{n \\to \\infty} \\mid e_n \\mid = 0\\) et \\(\\frac{\\mid e_{n+1} \\mid}{\\mid e_n \\mid} \\leq \\frac{1}{2}\\) La m\u00e9thode de la dichotomie est donc globalement convergente : elle converge quelque soit le point de d\u00e9part. (Si la \\(f\\) a plusieurs racines dans \\([a,b]\\) , la racine trouv\u00e9e d\u00e9pendra de l'intervalle). Sa convergence est lin\u00e9aire , donc elle est relativement lente . C'est pourquoi on utilise souvent cette m\u00e9thode juste pour initialiser une m\u00e9thode plus rapide.","title":"Convergence"},{"location":"Chap2_Recherche_de_racines/#precision","text":"En fonction de la pr\u00e9cision souhait\u00e9e \\(\\varepsilon\\) , on peut calculer le nombre d'it\u00e9rations \\(m\\) pour approcher la racine. On cherche \\(m \\in \\mathbb{N}\\) tel que : \\(\\mid e_{m-1} \\mid \\leq \\frac{\\mid b-a \\mid}{2^m} \\leq \\varepsilon\\) Donc tel que : \\(2^m \\geq \\frac{\\mid b-a \\mid}{\\varepsilon}\\) Soit : \\(m \\geq log_2{\\frac{\\mid b-a \\mid}{\\varepsilon}} = \\frac{ln{\\frac{\\mid b-a \\mid}{\\varepsilon}}}{ln(2)} \\approx 1.4427 ln{\\frac{\\mid b-a \\mid}{\\varepsilon}}\\)","title":"Pr\u00e9cision"},{"location":"Chap2_Recherche_de_racines/#exemple","text":"Voici les 4 premi\u00e8res it\u00e9rations de la m\u00e9thode de la dichotomie appliqu\u00e9e \u00e0 notre probl\u00e8me exemple, pour un intervalle initial \\([1,2]\\) : Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode de la dichotomie, avec un intervalle initial \\([1,2]\\) , estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Retrouvez-vous bien le nombre d'it\u00e9rations th\u00e9orique ?","title":"Exemple"},{"location":"Chap2_Recherche_de_racines/#avant-propos-les-methodes-linearisees","text":"Les m\u00e9thodes qui seront pr\u00e9sent\u00e9es dans la suite de ce chapitre sont des m\u00e9thodes lin\u00e9aris\u00e9es . Ce type de m\u00e9thodes s'appuit sur le d\u00e9veloppement de Taylor de \\(f\\) autour de sa racine \\(c\\) : \\(f(c) = 0 = f(x') + (c-x')f'(\\xi)\\) o\u00f9 \\(\\xi \\in [x',c]\\) et \\(f(x') \\neq 0\\) D'o\u00f9 \\(c = x' - \\frac{f(x')}{f'(\\xi)}\\) Donc, si on connait \\(\\xi\\) , on peut d\u00e9terminer \\(c\\) \u00e0 partir de \\(x'\\) . D'un point de vue g\u00e9om\u00e9trique, la racine \\(c\\) est \u00e0 l'intersection entre la droite passant par le point \\((x',f'(x'))\\) et de pente \\(f'(\\xi)\\) et donc d'\u00e9quation : \\(y = f'(\\xi) x + f(x') - f'(\\xi) x'\\) et l'axe \\((Ox)\\) donc d'\u00e9quation \\(y = 0\\) . D'o\u00f9 la m\u00e9thode it\u00e9rative suivante : \\(f(x_n) + (x_{n+1} - x_n) q_n = 0\\) ou encore l' \u00e9quation de r\u00e9currence : \\(x_{n+1} = x_n - \\frac{f(x_n)}{q_n}\\) o\u00f9 \\(q_n\\) est une approximation de \\(f'(\\xi)\\) . L'id\u00e9e des m\u00e9thodes lin\u00e9aris\u00e9es est donc : D'approcher \u00e0 chaque it\u00e9ration la fonction par une droite de pente \\(q_n\\) passant par le point \\((x_n,f(x_n))\\) . D\u00e9terminer \u00e0 chaque it\u00e9ration \\(x_{n+1}\\) comme l'intersection entre l'axe \\((Ox)\\) et cette droite. Les m\u00e9thodes lin\u00e9aris\u00e9es (m\u00e9thode de la s\u00e9cante, m\u00e9thode de la fausse position, m\u00e9thode de Newton, etc.) se diff\u00e9rentient par le choix de \\(q_n\\) .","title":"Avant-propos : les m\u00e9thodes lin\u00e9aris\u00e9es"},{"location":"Chap2_Recherche_de_racines/#methode-de-la-secante","text":"","title":"M\u00e9thode de la s\u00e9cante"},{"location":"Chap2_Recherche_de_racines/#algorithme_1","text":"La m\u00e9thode de la s\u00e9cante est une m\u00e9thode lin\u00e9aris\u00e9e pour laquelle : \\(q_n = \\frac{f(x_n)-f(x_{n-1})}{x_n-x_{n-1}}\\) Cette suite correspond \u00e0 la droite passant par les points \\((x_n,f(x_n))\\) et \\((x_{n-1},f(x_{n-1}))\\) . Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. a et b les bornes de l'intervalle de recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine \u00e0 l'it\u00e9ration n. x_n_old l'estimation de la racine \u00e0 l'it\u00e9ration n-1. r_n le r\u00e9sidu. def secante(f,a,b,n_max,e): #Initialisation des variables : n = 1 #Nombre d'it\u00e9rations x_n = a #Estimation de la racine \u00e0 l'it\u00e9ration n x_n_old = b #Estimation de la racine \u00e0 l'it\u00e9ration n-1 r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme de la s\u00e9cante #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(x_n-x_n_old)>e)and(abs(r_n)>e): #Calculer la pente de la droite : q_n = (f(x_n)-f(x_n_old))/(x_n-x_n_old) #Mettre \u00e0 jour l'estimation de la racine : x_n_old = x_n #Iteration n x_n = x_n - f(x_n)/q_n #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour le r\u00e9sidu : r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n","title":"Algorithme"},{"location":"Chap2_Recherche_de_racines/#convergence_1","text":"La m\u00e9thode de la s\u00e9cante est convergente localement . Si \\(f'(c) \\neq 0\\) , alors on peut d\u00e9montrer qu'elle converge avec un ordre \\(p = \\frac{1+\\sqrt(5)}{2}\\) . Cette valeur est connue sous le nom de \"nombre d'or\". On en d\u00e9duit que sous ces conditions, la convergence de la m\u00e9thode est lin\u00e9aire .","title":"Convergence"},{"location":"Chap2_Recherche_de_racines/#exemple_1","text":"Voici les 5 premi\u00e8res it\u00e9rations de la m\u00e9thode de la s\u00e9cante appliqu\u00e9e \u00e0 notre probl\u00e8me exemple. L'intervalle initial est ici de [0,2] pour des raisons de lisibilit\u00e9 : Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode de la s\u00e9cante, avec un intervalle initial \\([1,2]\\) , estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Comparez cette valeur \u00e0 celle obtenue pour la m\u00e9thode de la dichotomie.","title":"Exemple"},{"location":"Chap2_Recherche_de_racines/#methode-de-la-fausse-position","text":"","title":"M\u00e9thode de la fausse position"},{"location":"Chap2_Recherche_de_racines/#algorithme_2","text":"La m\u00e9thode de la fausse position est une m\u00e9thode lin\u00e9aris\u00e9e pour laquelle : \\(q_n = \\frac{f(x_n)-f(x_m)}{x_n-x_m}\\) Cette suite correspond \u00e0 la droite passant par les points \\((x_n,f(x_n))\\) et \\((x_m,f(x_m))\\) , o\u00f9 \\(m\\) est le plus grand indice inf\u00e9rieur \u00e0 \\(n\\) tel que \\(f(x_n)f(x_m)<0\\) . Il s'agit d'un m\u00e9lange entre la m\u00e9thode de la dichotomie et la m\u00e9thode de la s\u00e9cante. On l'appelle aussi m\u00e9thode de regula falsi ou m\u00e9thode de Lagrange . Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. a et b les bornes de l'intervalle de recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine. a_n et b_n les bornes de l'intervalle de recherche. r_n le r\u00e9sidu. def fausse_position(f,a,b,n_max,e): #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations a_n = a #Borne inf\u00e9rieure de l'intervalle de recherche b_n = b #Borne sup\u00e9rieure de l'intervalle de recherche q_n = (f(b_n)-f(a_n))/(b_n-a_n) #Initialiser la pente de la droite x_n = a_n-f(a_n)/q_n #Estimation de la racine r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme de la fausse-position #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(a_n-b_n)>e)and(abs(r_n)>e): #Si la racine est dans ]a_n,x_n[ alors on remplace b_n par x_n: if (f(a_n)*f(x_n))<0: b_n = x_n #Si la racine est dans ]x_n,b_n[ alors on remplace a_n par x_n: if (f(x_n)*f(b_n))<0: a_n = x_n #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Calcul de la nouvelle pente de la droite : q_n = (f(b_n)-f(a_n))/(b_n-a_n) #Mettre \u00e0 jour l'estimation de la racine et le r\u00e9sidu : x_n = x_n-f(x_n)/q_n r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n","title":"Algorithme"},{"location":"Chap2_Recherche_de_racines/#convergence_2","text":"La m\u00e9thode de la fausse position est globalement convergente : Si \\(f\\) est \u00e0 concavit\u00e9 constante avec \\(f\"<0\\) (concave), la suite converge vers la racine avec \\(f(x_n)\\) croissant. Si \\(f\\) est \u00e0 concavit\u00e9 constante avec \\(f\">0\\) (convexe), la suite converge vers la racine avec \\(f(x_n)\\) d\u00e9croissant. Elle converge avec un ordre \\(p\\) d'au moins 1 de fa\u00e7on super-lin\u00e9aire .","title":"Convergence"},{"location":"Chap2_Recherche_de_racines/#exemple_2","text":"Voici les 5 premi\u00e8res it\u00e9rations de la m\u00e9thode de la fausse position appliqu\u00e9e \u00e0 notre probl\u00e8me exemple. L'intervalle initial est ici de [0,2] pour des raisons de lisibilit\u00e9 : Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode de la fausse position, avec un intervalle initial \\([1,2]\\) , estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Comparez cette valeur \u00e0 celle obtenue pour la m\u00e9thode de la s\u00e9cante.","title":"Exemple"},{"location":"Chap2_Recherche_de_racines/#methode-de-newton","text":"","title":"M\u00e9thode de Newton"},{"location":"Chap2_Recherche_de_racines/#algorithme_3","text":"La m\u00e9thode de Newton est une m\u00e9thode lin\u00e9aris\u00e9e pour laquelle : \\(q_n = f'(x_n)\\) Cette suite correspond \u00e0 la pente de la tangente \u00e0 la fonction \\(f\\) en \\(x_n\\) . On l'appelle aussi la m\u00e9thode de Newton-Raphson . Cette m\u00e9thode n\u00e9cessite l'\u00e9valuation de \\(f\\) et de sa d\u00e9riv\u00e9e \\(f'\\) . Dans le cas de notre probl\u00e8me exemple, on d\u00e9finira : def f_derivee(x): return 2*x Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . On choisi d'initialiser la m\u00e9thode avec \\(x_0 \\in [a,b]\\) . Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. f_derivee la d\u00e9riv\u00e9e de la fonction dont on cherche les racines. x_0 point de d\u00e9part de la recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine \u00e0 l'it\u00e9ration n. x_n_old l'estimation de la racine \u00e0 l'it\u00e9ration n-1. r_n le r\u00e9sidu. def newton(f,f_derivee,x_0,n_max,e): #V\u00e9rifier que le point de d\u00e9part de la recherche est possible : if f_derivee(x_0)==0: raise ValueError(\"Mauvaise initialisation, f'(x_0) = 0\") #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = x_0 #Estimation de la racine \u00e0 l'it\u00e9ration n-1 x_n = x_n_old-f(x_n_old)/f_derivee(x_n_old) #Estimation de la racine \u00e0 l'it\u00e9ration n r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme de Newton #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(x_n-x_n_old)>e)and(abs(r_n)>e): #Mettre \u00e0 jour l'estimation de la racine : x_n_old = x_n #It\u00e9ration n x_n = x_n-f(x_n)/f_derivee(x_n) #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour le r\u00e9sidu : r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n","title":"Algorithme"},{"location":"Chap2_Recherche_de_racines/#convergence_3","text":"La m\u00e9thode de Newton est convergente localement . Si \\(x_0\\) est assez proche de la racine \\(c\\) , et \\(f'(c) \\neq 0\\) , on peut montrer que la m\u00e9thode converge avec un ordre \\(p=2\\) . La convergence est donc quadratique . En pratique, on utilise souvent la m\u00e9thode de la dichotomie pour trouver un point de d\u00e9part \\(x_0\\) suffisamment proche de \\(c\\) avant d'utiliser la m\u00e9thode de Newton. Reste un incov\u00e9nient, la m\u00e9thode de Newton n\u00e9cessite le calcul de d\u00e9riv\u00e9es, et est donc plus co\u00fbteuse en calcul. La m\u00e9thode de Newton est s\u00fbrement convergente dans le cas d'une fonction \\(f\\) strictement monotone et ne pr\u00e9sentant pas de points d'inflexion dans l'intervalle \\([a,b]\\) (i.e. \\(f\"(x)\\) ne change pas de signe donc \\(f\\) a une concavit\u00e9 constante) si le point de d\u00e9part \\(x_0\\) est tel que : \\(f(x_0)f\"(x_0)>0\\) Pour choisir un point de d\u00e9part \\(x_0\\) qui ne risque pas de faire diverger la m\u00e9thode de Newton, il faut donc s'assurer de respecter cette condition.","title":"Convergence"},{"location":"Chap2_Recherche_de_racines/#exemple_3","text":"Voici les 4 premi\u00e8res it\u00e9rations de la m\u00e9thode de Newton appliqu\u00e9e \u00e0 notre probl\u00e8me exemple. Le point de d\u00e9part de la recherche choisi est de 2.5 pour des raisons de lisibilit\u00e9 : On note que le choix de point de d\u00e9part est correct, car \\(f\\) est \u00e0 concavit\u00e9 constante positive ( \\(f\"(x)=2>0\\) ) et \\(f(x_0)=4.25>0\\) , ce qui signifie que \\(f(x_0)f\"(x_0)>0\\) est bien respect\u00e9e. Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode de Newton, avec un point de d\u00e9part de votre choix, estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Comparez cette valeur \u00e0 celle obtenue pour les m\u00e9thodes pr\u00e9c\u00e9dentes. Quelle est la m\u00e9thode lin\u00e9aris\u00e9e la plus rapide ? NB: La m\u00e9thode de Newton appliqu\u00e9e au cas de l'estimation de \\(\\sqrt{2}\\) est un cas particulier de la c\u00e9l\u00e8bre m\u00e9thode d'H\u00e9ron d'Alexandrie.","title":"Exemple"},{"location":"Chap2_Recherche_de_racines/#methode-du-point-fixe","text":"","title":"M\u00e9thode du point fixe"},{"location":"Chap2_Recherche_de_racines/#definitions","text":"Id\u00e9e Transformer l'\u00e9quation \\(f(x)=0\\) en une \u00e9quation \u00e9quivalente \\(g(x)=x\\) o\u00f9 \\(g\\) est une fonction auxiliaire bien choisie. La racine \\(c\\) est alors un point fixe de \\(g\\) et approcher les z\u00e9ros de \\(f\\) revient \u00e0 approcher les points fixes de \\(g\\) . Cette transformation est toujours possible mais pas unique . Dans la pratique, la m\u00e9thode du point fixe consiste \u00e0 transformer le probl\u00e8me \\(f(x)=0\\) o\u00f9 \\(f:[a,b] \\rightarrow \\mathbb{R}\\) en un probl\u00e8me \u00e9quivalent \\(x=g(x)\\) , en passant par un sch\u00e9ma it\u00e9ratif \\(x_{n+1}=g(x_n)\\) o\u00f9 \\(g\\) est une fonction bien choisie. L'it\u00e9ration est dite de point fixe , et la fonction \\(g\\) est la fonction d'it\u00e9ration associ\u00e9e. Th\u00e9or\u00e8me du point fixe Soit \\(g\\) une fonction continue et \\((x_n)\\) une suite g\u00e9n\u00e9r\u00e9e par l'it\u00e9ration de point fixe \\(x_{n+1}=g(x_n)\\) . Si \\(\\lim\\limits_{n \\to \\infty} x_n = c\\) alors par continuit\u00e9 de \\(g\\) : \\(\\lim\\limits_{n \\to \\infty} g(x_n) = g(c) = c\\) Donc \\(c\\) est un point fixe de \\(g\\) . Les m\u00e9thodes lin\u00e9aris\u00e9es sont des m\u00e9thodes de point fixe : on obtient \\(x_{n+1}\\) \u00e0 partir de \\(x_n\\) en \u00e9valuant toujours la m\u00eame expression \\(x_{n+1}=g(x_n)\\) . Par exemple, dans notre probl\u00e8me de l'approximation de \\(\\sqrt{2}\\) , r\u00e9soudre \\(x^2-2=0\\) revient \u00e0 trouver le point fixe de \\(g(x)=x-\\frac{f(x)}{f'(x)}=\\frac{x+\\frac{2}{x}}{2}\\) .","title":"D\u00e9finitions"},{"location":"Chap2_Recherche_de_racines/#algorithme_4","text":"Soit \\(f\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . On suppose que \\(f\\) admet une unique racine dans \\(]a,b[\\) et que \\(f(a)f(b)<0\\) . On choisi d'initialiser la m\u00e9thode avec \\(x_0 \\in [a,b]\\) . Cette m\u00e9thode n\u00e9cessite la s\u00e9lection d'une fonction d'it\u00e9ration de point fixe \\(g\\) . Dans le cas de notre probl\u00e8me exemple, on pourra choisir : def g(x): return x/2+1/x Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : f la fonction dont on cherche les racines. g la fonction d'it\u00e9ration choisie. x_0 point de d\u00e9part de la recherche. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la racine \u00e0 l'it\u00e9ration n. x_n_old l'estimation de la racine \u00e0 l'it\u00e9ration n-1. r_n le r\u00e9sidu. def point_fixe(f,g,x_0,n_max,e): #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = x_0 #Estimation de la racine \u00e0 l'it\u00e9ration n-1 x_n = g(x_n_old) #Estimation de la racine \u00e0 l'it\u00e9ration n r_n = f(x_n) #R\u00e9sidu #It\u00e9rations de l'algorithme du point fixe #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(abs(x_n-x_n_old)>e)and(abs(r_n)>e): #Mettre \u00e0 jour l'estimation de la racine : x_n_old = x_n #It\u00e9ration n x_n = g(x_n) #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour le r\u00e9sidu : r_n = f(x_n) #Renvoyer l'estimation de la racine et le r\u00e9sidu : return x_n,r_n","title":"Algorithme"},{"location":"Chap2_Recherche_de_racines/#convergence_4","text":"Th\u00e9or\u00e8me de la convergence globale des it\u00e9rations de point fixe Soit \\(g\\) une fonction continue de \\([a,b]\\) dans \\(\\mathbb{R}\\) . 1. Hypoth\u00e8se d'inclusion (ou de stabilit\u00e9) : Si \\(\\forall x \\in [a,b]\\) , \\(g(x) \\in [a,b]\\) alors \\(g\\) admet un point fixe dans \\([a,b]\\) . 2. Hypoth\u00e8se de contraction stricte : Si de plus, \\(\\exists K \\in ]0,1[\\) tel que \\(\\mid g(x)-g(y) \\mid \\leq K \\mid x-y \\mid \\forall x,y \\in [a,b]\\) (on dit que \\(g\\) est strictement contractante ) alors \\(g\\) admet un point fixe unique not\u00e9 \\(c\\) dans \\([a,b]\\) et la suite \\(x_{n+1}=g(x_n)\\) converge vers \\(c\\) pour toute valeur de d\u00e9part \\(x_0\\) dans \\([a,b]\\) . On appelle alors \\(c\\) un point attracteur . 2 r\u00e8gles pratiques pour v\u00e9rifier l'hypoth\u00e8se de contraction : Th\u00e9or\u00e8me Soit \\(g\\) une fonction continue et d\u00e9rivable de \\([a,b]\\) dans \\(\\mathbb{R}\\) . Si \\(\\forall x\\in [a,b]\\) , \\(\\mid g'(x) \\mid <1\\) alors \\(g\\) est strictement contractante sur \\([a,b]\\) . Si \\(\\forall x\\in [a,b]\\) , \\(\\mid g'(x) \\mid >1\\) alors la suite diverge si \\(x_0 \\neq c\\) . On parle alors de point r\u00e9pulsif En pratique, il est souvent difficile de montrer la convergence globale. D'o\u00f9 l'utilit\u00e9 d'une \u00e9tude de convergence locale : Th\u00e9or\u00e8me de la convergence locale des it\u00e9rations de point fixe Soit \\(c\\) un point fixe d'une fonction \\(g\\) d\u00e9rivable au voisinage de \\(c\\) . Si \\(\\mid g'(c) \\mid < 1\\) alors il existe \\(\\delta >0\\) tel que la suite \\(x_{n+1}=g(x_n)\\) converge vers \\(c\\) pour tout \\(x_0\\) tel que \\(\\mid x_0-c \\mid < \\delta\\) . Si \\(\\mid g'(c) \\mid > 1\\) la convergence est impossible. Si \\(\\mid g'(c) \\mid = 1\\) on peut avoir convergence ou divergence.","title":"Convergence"},{"location":"Chap2_Recherche_de_racines/#exemple_4","text":"Consid\u00e9rons \u00e0 nouveau notre probl\u00e8me d'approximation de \\(\\sqrt{2}\\) par la recherche des racines de \\(f(x)=x^2-2\\) . Nous proposons d'essayer les fonctions d'it\u00e9ration suivantes : \\(g_1(x)=\\frac{2}{x}\\) \\(g_2(x)=2x-\\frac{2}{x}\\) \\(g_3(x)=\\frac{x}{2}+\\frac{1}{x}\\) \\(g_1'(x)=-\\frac{2}{x^2}\\) \\(g_2'(x)=2+\\frac{2}{x^2}\\) \\(g_3'(x)=\\frac{1}{2}-\\frac{1}{x^2}\\) \\(g_1'(c)=-1\\) \\(g_2'(c)=3\\) \\(g_3'(c)=0\\) On s'attend donc \u00e0 ce que \\(g_1\\) et \\(g_2\\) divergent, et \u00e0 ce que \\(g_3\\) converge. V\u00e9rifions avec un point de d\u00e9part de 2 pour les 3 fonctions. Voici les 4 premi\u00e8res it\u00e9rations pour \\(g_1(x)=\\frac{2}{x}\\) : La valeur de \\(x_n\\) oscille entre 1 et 2 sans jamais s'arr\u00eater. La suite ne converge donc pas. Voici les 4 premi\u00e8res it\u00e9rations pour \\(g_2(x)=2x-\\frac{2}{x}\\) : On observe que \\(x_n\\) diverge en escalier. Voici les 4 premi\u00e8res it\u00e9rations pour \\(g_3(x)=\\frac{x}{2}+\\frac{1}{x}\\) : Cette fois-ci, \\(x_n\\) converge rapidement vers \\(\\sqrt{2}\\) . On retrouve donc bien les r\u00e9sultats attendus. Exercice : En adaptant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour la m\u00e9thode du point fixe, avec le m\u00eame point de d\u00e9part que vous avez choisi pour la m\u00e9thode de Newton, estimez la valeur de \\(\\sqrt{2}\\) avec une pr\u00e9cision de \\(10^{-6}\\) . Combien d'it\u00e9rations sont n\u00e9cessaires pour obtenir cette pr\u00e9cision ? Comparez cette valeur \u00e0 celle obtenue pour la m\u00e9thode de Newton. Expliquez pourquoi ces r\u00e9sultats sont identiques.","title":"Exemple"},{"location":"Chap2_Recherche_de_racines/#vitesse-de-convergence-des-methodes","text":"","title":"Vitesse de convergence des m\u00e9thodes"},{"location":"Chap2_Recherche_de_racines/#methode-de-la-dichotomie_1","text":"On rappelle que la m\u00e9thode de la dichotomie converge de mani\u00e8re lin\u00e9aire .","title":"M\u00e9thode de la dichotomie"},{"location":"Chap2_Recherche_de_racines/#methodes-de-point-fixe","text":"Supposons que \\(g\\) est d\u00e9rivable \\(p \\geq 1\\) fois dans \\([a,b]\\) contenant le point fixe \\(c\\) . La formule de Taylor au voisinage de \\(c\\) , \u00e0 l'it\u00e9ration \\(n\\) est : \\(g(x_n) = g(c) + g'(c)(x_n-c) + g\"(c)\\frac{(x_n-c)^2}{2!} + ... + g^{(p)}(c)\\frac{(x_n-c)^p}{p!}\\) D'o\u00f9 l'erreur absolue \u00e0 l'it\u00e9ration \\(n+1\\) : \\(e_{n+1} = x_{n+1}-c = g(x_n)-g(c) = g'(c)e_n + g\"(c)\\frac{e_n^2}{2!} + ... + g^{(p)}(c)\\frac{e_n^p}{p!}\\) Th\u00e9or\u00e8me La m\u00e9thode du point fixe est d'ordre \\(p>1\\) si et seulement si : \\(g^{(i)}(c)=0\\) \\(\\forall 1 \\leq i \\leq p-1\\) et \\(g^{(p)}(c) \\neq 0\\) La m\u00e9thode est convergente du 1er ordre (i.e. lin\u00e9aire ) si \\(g'(c) \\neq 0\\) et \\(\\lim\\limits_{n \\to \\infty} \\frac{\\mid e_{n+1} \\mid}{\\mid e_n \\mid} = \\mid g'(c) \\mid < 1\\) . (C'est ce que l'on appelle le \"facteur de r\u00e9duction\" de l'erreur). La m\u00e9thode est convergente d'ordre 2 (i.e. quadratique ) si \\(g'(c)=0\\) et \\(g\"(c) \\neq 0\\) et \\(\\lim\\limits_{n \\to \\infty} \\frac{\\mid e_{n+1} \\mid}{\\mid e_n \\mid^2} = \\frac{1}{2} \\mid g\"(c) \\mid\\) .","title":"M\u00e9thodes de point fixe"},{"location":"Chap2_Recherche_de_racines/#methode-de-la-secante_1","text":"On rappelle que dans le cas de la m\u00e9thode de la s\u00e9cante : \\(g(x) = x_0-f(x_0)\\frac{x-x_0}{f(x)-f(x_0)}\\) Donc \\(g'(x) = -f(x_0) \\frac{f(x)-f(x_0)-(x-x_0)f'(x)}{(f(x)-f(x_0))^2}\\) D'o\u00f9 \\(g'(c) = \\frac{f(x_0)+f'(c)(c-x_0)}{f(x_0)}\\) D'apr\u00e8s la formule de Taylor, il existe au moins un \\(c \\in ]a,x_0[\\) tel que : \\(f(x_0)+f'(c)(c-x_0) = f\"(c)\\frac{(c-x_0)^2}{2!}\\) Donc s'il n'y a pas de point d'inflexion, \\(g'(c) \\neq 0\\) et la m\u00e9thode converge lin\u00e9airement .","title":"M\u00e9thode de la s\u00e9cante"},{"location":"Chap2_Recherche_de_racines/#methode-de-newton_1","text":"On rappelle que dans le cas de la m\u00e9thode de Newton : \\(g(x) = x-\\frac{f(x)}{f'(x)}\\) Donc \\(g'(x) = 1 - \\frac{f'(x)f'(x)-f(x)f\"(x)}{f'(x)^2} = \\frac{f(x)f\"(x)}{f'(x)^2}\\) D'o\u00f9 \\(g'(c) = \\frac{f(c)f\"(c)}{f'(c)^2} = 0\\) De plus, \\(g\"(x) = \\frac{(f'(x)f\"(x)+f(x)f\"'(x))f'(x)^2-f(x)f\"(x)(2f\"(x)f'(x))}{f'(x)^4}\\) D'o\u00f9 \\(g\"(c)=\\frac{f\"(c)}{f'(c)}\\) Par cons\u00e9quent, si \\(c\\) est un z\u00e9ro simple de la fonction \\(f\\) (i.e. \\(f\"(c) \\neq 0\\) ) alors la m\u00e9thode converge quadratiquement . Si \\(f\"(c) = 0\\) , la convergence est d'ordre sup\u00e9rieur \u00e0 2 .","title":"M\u00e9thode de Newton"},{"location":"Chap2_Recherche_de_racines/#conclusions","text":"La m\u00e9thode de la dichotomie est simple mais lente car elle ne prend pas en compte le comportement de \\(f\\) . Elle est n\u00e9anmoins utile pour initialiser des m\u00e9thodes plus rapides . La m\u00e9thode de Newton est convergente d'ordre au moins 2 mais n\u00e9cessite le calcul de d\u00e9riv\u00e9es et un bon choix d'initialisation . Les m\u00e9thodes d'it\u00e9rations de point fixe convergent sous des conditions portant sur la fonction d'it\u00e9ration \\(g\\) et sa d\u00e9riv\u00e9e \\(g'\\) . Leur convergence est g\u00e9n\u00e9ralement lin\u00e9aire , mais devient quadratique quand \\(g'(c)=0\\) .","title":"Conclusions"},{"location":"Chap3_Interpolation_polynomiale/","text":"Chapitre III : Interpolation polynomiale Ce chapitre porte sur les m\u00e9thodes num\u00e9riques pour l'approximation de valeurs inconnues d'une fonction \u00e0 partir des valeurs connues, par interpolation avec un polyn\u00f4me. Position du probl\u00e8me Motivation Soit une fonction \\(f(x)\\) connue en seulement \\(n+1\\) points, appel\u00e9s points ou noeuds d'interpolation \\((x_i,f(x_i))\\) avec \\(i=0,1,2,...,n\\) de l'intervalle \\([a,b]\\) . Peut-on approcher \\(f(x)\\) pour tout \\(x\\) de \\([a,b]\\) par une fonction ? Il existe une infinit\u00e9 de fonctions d'interpolation, mais le plus simple est d'approcher la fonction par un polyn\u00f4me de degr\u00e9 suffisamment \u00e9lev\u00e9 pour que sa courbe passe par les point d'interpolation. Un polyn\u00f4me \\(p\\) de degr\u00e9 inf\u00e9rieur ou \u00e9gal \u00e0 \\(n\\) s'exprime dans la base canonique \\({1,x,x^2,...,x^n}\\) de la mani\u00e8re suivante : \\(p(x) = \\displaystyle\\sum_{k=0}^{n} a_k x^k\\) o\u00f9 les \\(a_k\\) sont les coefficients du polyn\u00f4me. L'interpolation polynomiale consiste donc \u00e0 d\u00e9terminer les coefficients \\(a_k\\) tels que \\(p(x_i) = f(x_i)\\) pour \\(i=0,1,2,...,n\\) . NB : L'interpolation polynomiale pourra sont utiles pour les m\u00e9thodes num\u00e9riques de calcul d'int\u00e9grales et de d\u00e9riv\u00e9es. Attention ! L'interpolation polynomiale et l'approximation polynomiales sont des approches diff\u00e9rentes. L'approximation polynomiale de donn\u00e9es bruit\u00e9es cherche un polyn\u00f4me de degr\u00e9 inf\u00e9rieur au nombre de donn\u00e9es qui ne passe pas n\u00e9cessairement par tous les points connus. Existence et unicit\u00e9 d'un polyn\u00f4me d'interpolation Th\u00e9or\u00e8me d'Evariste Galois Un polyn\u00f4me de degr\u00e9 \\(n\\) a au plus \\(n\\) racines qui peuvent \u00eatre r\u00e9elles ou complexes conjugu\u00e9es. D'o\u00f9 le corollaire : Corollaire On ne peut faire passer par \\(n+1\\) points distincts qu'un seul polyn\u00f4me de degr\u00e9 n. Toutes les m\u00e9thodes pr\u00e9sent\u00e9es dans la suite de ce chapitre doivent donc aboutir au m\u00eame polyn\u00f4me . Exemple de probl\u00e8me Au cours de ce chapitre, nous appliquerons les diff\u00e9rentes m\u00e9thodes num\u00e9riques d'interpolation polynomiale \u00e0 un m\u00eame exemple : l'estimation de la dur\u00e9e du jour \u00e0 l'UFR des sciences de l'UVSQ . La dur\u00e9e du jour (temps entre le lever et le coucher du soleil) en heures peut \u00eatre approxim\u00e9e par la fonction \\(f\\) suivante : \\(f(x) = \\frac{48}{2 \\pi} \\arccos(\\tan(\\lambda) \\tan(\\arcsin(\\sin(\\frac{2 \\pi x}{365}) \\sin(\\delta))))\\) avec \\(x\\) le jour depuis l'\u00e9quinoxe d'automne, \\(\\lambda\\) la latitude du lieu, et \\(\\delta\\) la latitude des tropiques. On sait que \\(\\delta \\approx 23.438403\u00b0\\) . On prendra ici l'exemple de l'UFR des sciences de l'UVSQ, dont la latitude est \\(\\lambda \\approx 48.81094\u00b0\\) . On peut voir que cette formule implique 6 appels \u00e0 des fonctions trigonom\u00e9triques, ce qui peut rendre non n\u00e9gligeable le temps n\u00e9cessaire pour \u00e9valuer \\(f\\) en un grand nombre de points. D'o\u00f9 l'int\u00e9r\u00eat de n'\u00e9valuer la fonction qu'en un nombre limit\u00e9 de points, et d'utiliser l'interpolation polynomiale pour d\u00e9terminer d'autres valeurs de \\(f(x)\\) . Nous choisirons ici d'\u00e9valuer la fonction pour les 10 valeurs de \\(x\\) suivantes : x = jours depuis l'\u00e9quinoxe d'automne 30 60 90 120 150 180 240 270 300 330 f(x) = dur\u00e9e du jour en heures 10.24 8.73 8.04 8.63 10.09 11.84 15.16 15.95 15.47 14.06 Et nous essayerons d'estimer la valeur de \\(f\\) pour \\(x = 210\\) (i.e. la dur\u00e9e du jour en heures pour le 210\u00e8me jour depuis l'\u00e9quinoxe d'automne) par interpolation polynomiale. Sous Python on utilisera la biblioth\u00e8que Numpy : import numpy as np Puis, on d\u00e9finira les variables globales suivantes : l = 48.81094*np.pi/180 #Latitude de l'UFR des sciences a = 23.438403*np.pi/180 #Latitude des tropiques La fonction \\(f\\) sera d\u00e9finie comme : def f(d): return 48/(2*np.pi)*np.arccos(np.tan(l)*np.tan(np.arcsin(np.sin(a)*np.sin(d*2*np.pi/365.25)))) On calculera alors les 10 valeurs \"connues\" de la fonction de la mani\u00e8re suivante : x = np.array([30,60,90,120,150,180,240,270,300,330],dtype='float') y = f(x) (On a ici d\u00e9finit avec Numpy un vecteur de 10 valeurs \\(x\\) , auquel on a appliqu\u00e9 \\(f\\) . Le vecteur r\u00e9sultant est stock\u00e9 dans \\(y\\) ). Matrices de Vandermonde Une 1\u00e8re approche pour obtenir un polyn\u00f4me d'interpolation est la suivante : d\u00e9terminer les coefficients \\(a_i\\) du polyn\u00f4me en r\u00e9solvant les \\((n+1)\\) \u00e9quations de collocation \\(p(x_i) = f(x_i)\\) pour \\(i=0,1,2,...,n\\) . Ceci revient \u00e0 r\u00e9soudre le syst\u00e8me lin\u00e9aire de \\((n+1)\\) \u00e9quations \u00e0 \\((n+1)\\) inconnues : \\(\\begin{cases} a_0 + a_1 x_0 + a_2 x_0^2 + ... + a_n x_0^n = f(x_0)\\\\ a_0 + a_1 x_1 + a_2 x_1^2 + ... + a_n x_1^n = f(x_1)\\\\ ...\\\\ a_0 + a_1 x_n + a_2 x_n^2 + ... + a_n x_n^n = f(x_n) \\end{cases}\\) Ce syst\u00e8me admet une unique solution si les \\(x_i\\) sont distincts 2 \u00e0 2. Il peut s'\u00e9crire sous la forme matricielle suivante : \\(\\begin{pmatrix} 1 & x_0 & x_0^2 & \\cdots & x_0^n \\\\ 1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 &\\cdots & x_n^n \\end{pmatrix} \\begin{pmatrix} a_0\\\\ a_1\\\\ \\vdots\\\\ a_n \\end{pmatrix} = \\begin{pmatrix} f(x_0)\\\\ f(x_1)\\\\ \\vdots\\\\ f(x_n) \\end{pmatrix}\\) On reconnait ici une matrice de Vandermonde . La matrice de Vandermonde est inversible si et seulement si les \\(x_i\\) sont distincts 2 \u00e0 2. Dans la pratique, l'inversion de la matrice de Vandermonde ne conduit pas \u00e0 une solution satisfaisante : Le nombre d'\u00e9quations / d'inconnues croit avec le nombre de points d'interpolations, augmentant le nombre d'op\u00e9rations de l'ordre de \\(2 n^3 / 3\\) . Ce type de syst\u00e8me est souvent mal conditionn\u00e9, ce qui rend la solution num\u00e9rique tr\u00e8s sensible aux erreurs d'arrondi. C'est pourquoi dans la suite, on va pr\u00e9f\u00e9rer des techniques exprimant le polyn\u00f4me dans une autre base que la base canonique . Polyn\u00f4mes de Lagrange L'algorithme Base de Lagrange Soient des \\(x_i\\) (avec \\(i=0,1,2,...,n\\) ) 2 \u00e0 2 distincts. On appelle base de Lagrange relative aux points x_i les polyn\u00f4mes : \\(L_i(x) = \\displaystyle\\prod_{j=0 , j \\neq i}^{n} \\frac{(x-x_j)}{x_i-x_j}\\) soit \\(L_i(x) = \\frac{(x-x_0)(x-x_1)...(x-x_n)}{(x_i-x_0)(x_i-x_1)...(x_i-x_n)}\\) \\(L_i\\) v\u00e9rifie \\(L_i(x_i)=1\\) et \\(L_i(x_j)=0\\) si \\(j \\neq i\\) . La famille des \\((L_i(x))\\) forme une base de l'ensemble des polyn\u00f4mes, et le polyn\u00f4me qui interpoles les valeurs de \\(f(x_i)\\) aux points \\(x_i\\) s'\u00e9crit : \\(p(x) = \\displaystyle\\sum_{i=0}^{n} f(x_i) L_i(x) = f(x_0) L_0(x) + f(x_1) L_1(x) + ... + f(x_n) L_n(x)\\) \\(p\\) v\u00e9rifie bien que \\(\\forall i = 0,1,2,...,n\\) , \\(p(x_i) = f(x_i)\\) et est unique si les points \\(x_i\\) sont distincts. Les coefficients du polyn\u00f4me sont directements les valeurs \\(f(x_i)\\) , qui sont connues. Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : x le vecteur des abscisses des point connus. y le vecteur des ordonn\u00e9es des points connus. xp l'abscisse du point que l'on veut interpoler. On notera les variables suivantes : Li le polyn\u00f4me de la base de Lagrange associ\u00e9 au point \\(x_i\\) . yp l'ordonn\u00e9e du point que l'on veut interpoler. def lagrange(x,y,xp): #R\u00e9cup\u00e9ration du nombre de points connus : n = len(x) #Initialisation de l'ordonn\u00e9e du point interpol\u00e9 yp : yp = 0 #1\u00e8re boucle sur les points connus (x[i],y[i]) : for i in range(n): #Initialisation du polyn\u00f4me de la base de Lagrange associ\u00e9 au i-\u00e8me #point connu Li : Li = 1 #2\u00e8me boucle sur les abscisses connues x[j]: for j in range(n): #Dans le cas o\u00f9 les 2 abscisses connues x[i] et x[j] sont distinctes, #multiplication de Li par un coefficient obtenu avec xp, x[i] et #x[j], de telle fa\u00e7on que Li = 1 si xp = x[i] et Li = 0 si xp = x[j] #(voir formule du cours) : if j!=i: Li = Li*(xp-x[j])/(x[i]-x[j]) #Addition \u00e0 yp de la valeur du polyn\u00f4me y[i] Li, qui est \u00e9gal \u00e0 y[i] en #x[i] et nul pour tout les x[j] (voir formule du cours): yp = yp + y[i]*Li #Renvoyer l'ordonn\u00e9e du point interpol\u00e9 : return yp La base des polyn\u00f4mes de Lagrange permet de ne pas avoir \u00e0 r\u00e9soudre un syst\u00e8me lin\u00e9aire de \\(n+1\\) \u00e9quations \u00e0 \\(n+1\\) inconnues. Lorsque \\(n\\) est petit, il reste cependant plus simple de r\u00e9soudre le syst\u00e8me d'\u00e9quations. Un d\u00e9saventage pratique de l'interpolation de Lagrange est le fait qu'il soit n\u00e9cessaire tout recalculer si on ajoute un point d'interpolation. L'interpolation de Newton, pr\u00e9sent\u00e9e dans la suite, n'a pas ce d\u00e9saventage. Exemple Voici la construction du polyn\u00f4me de Lagrange pour notre probl\u00e8me exemple : On observe bien que pour \\(i=0,1,...,9\\) , chaque \\(f(x_i)L_i(x)\\) passe par \\(f(x_i)\\) en \\(x = x_i\\) et par 0 en \\(x = x_j\\) pour \\(j \\neq i\\) . Le polyn\u00f4me construit passe bien par \\(f(x_i)\\) pour tous les \\(x_i\\) . On trouve une valeur interpol\u00e9e en \\(x = 210\\) d'environ 13.61. Exercice : En modifiant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour l'interpolation de Lagrange, ainsi que la fonction \\(f\\) , d\u00e9terminez l'erreur d'interpolation en \\(x = 210\\) avec 4 chiffres significatifs. Polyn\u00f4mes de Newton L'algorithme Base de Newton Soient des \\(x_i\\) (avec \\(i=0,1,2,...,n\\) ) 2 \u00e0 2 distincts. On appelle base de Newton relative aux points \\(x_i\\) les polyn\u00f4mes : \\(v_0(x) = 1\\) \\(v_i(x) = \\displaystyle\\prod_{j=0}^{i-1} (x-x_j) = (x-x_0)(x-x_1)...(x-x_{i-1})\\) \\(v_i\\) v\u00e9rifie \\(v_i(x_j) = 0\\) pour \\(j<i\\) . La famille \\((v_i(x))\\) forme une base de l'ensemble des polyn\u00f4mes, et le polyn\u00f4me qui interpole les valeurs \\(f(x_i)\\) aux points \\(x_i\\) s'\u00e9crit : \\(p(x) = \\displaystyle\\sum_{i=0}^{n} c_i v_i(x) = c_0 v_0(x) + c_1 v_1(x) + ... + c_n v_n(x)\\) avec des coefficients c_i \u00e0 d\u00e9terminer, tels que \\(p(x_i) = f(x_i) \\forall i = 0,1,2,...,n\\) . \\(p\\) est unique si les \\(x_i\\) sont 2 \u00e0 2 distincts. L'un des int\u00e9r\u00eats de la base de Newton est que si l'on ajoute un nouveau point d'interpolation \\((x_{n+1},f(x_{n+1}))\\) , les coefficients \\(c_0,c_1,c_2,...,c_n\\) restent inchang\u00e9s et il suffit de calculer \\(c_{n+1}\\) et d'ajouter un terme \\(c_{n+1} v_{n+1}\\) \u00e0 l'expression de \\(p\\) . Les \\(c_i\\) sont les solutions du syst\u00e8me lin\u00e9aire (triangulaire inf\u00e9rieur) suivant : \\(\\begin{cases} f(x_0) = c_0 v_0(x_0) = c_0\\\\ f(x_1) = c_0 + c_1 v_1(x_1) = c_0 + c_1 (x_1-x_0)\\\\ f(x_2) = c_0 + c_1 v_1(x_2) + c_2 v_2(x_2) = c_0 + c_1 (x_2-x_0) + c_2 (x_2-x_0)(x_2-x_1)\\\\ ...\\\\ f(x_n) = c_0 + \\displaystyle\\sum_{i=1}^{n} c_i v_i(x_n) \\end{cases}\\) Ce syst\u00e8me est en apparence simple, et peut \u00eatre r\u00e9solu de proche en proche. Mais dans la pratique, les expressions des \\(c_i\\) deviennent de plus en plus complexes. C'est pourquoi on fait appel aux diff\u00e9rences divis\u00e9es pour exprimer les coefficients de fa\u00e7on compacte. Les diff\u00e9rences divis\u00e9es Soit \\(f\\) une fonction d\u00e9finie aux points \\(x_i\\) , 2 \u00e0 2 distincts. On d\u00e9finit les diff\u00e9rences divis\u00e9es par r\u00e9currence comme suit : \\(\\begin{cases} f[x_i] = f(x_i), i=0,...,n\\\\ f[x_i...x_{i+k}] = \\frac{f[x_i...x_{i+k-1}]-f[x_{i+1}...x_{i+k}]}{x_i-x_{i+k}}, i=0,...,n \\end{cases}\\) avec \\(k\\) l'ordre de la r\u00e9currence. Les coefficients \\(c_i\\) peuvent \u00eatre calcul\u00e9s par r\u00e9currence \u00e0 partir des diff\u00e9rences divis\u00e9es de la mani\u00e8re suivante : \\(c_i = f[x_0 x_1 ... x_i] = \\frac{f[x_0 x_1 ... x_{i-1}]-f[x_1 x_2 ... x_i]}{x_0-x_i}\\) C'est ce que l'on appelle la diff\u00e9rence divis\u00e9e d'ordre i . Le polyn\u00f4me d'interpolation de Newton de degr\u00e9 \\(n\\) qui interpole les valeurs \\(f(x_i)\\) aux points \\(x_i\\) \\((i=0,1,2,...,n)\\) , distincts 2 \u00e0 2, s'\u00e9crit donc : \\(p(x) = \\displaystyle\\sum_{i=0}^{n} f[x_0 x_1 ... x_i] v_i(x)\\) Soit \\(p(x) = f[x_0] + f[x_0 x_1] (x-x_0) + f[x_0 x_1 x_2] (x-x_0)(x-x_1) + ... + f[x_0 x_1 ... x_n] \\displaystyle\\prod_{j=0}^{n-1} (x-x_j)\\) Le calcul effectif du polyn\u00f4me d'interpolation se fait donc de la mani\u00e8re suivante : \\(i=0\\) \\(i=1\\) \\(i=2\\) ... \\(i=n\\) \\(x_0\\) \\(f[x_0] = c_0\\) ... \\(x_1\\) \\(f[x_1]\\) \\(f[x_0 x_1] = c_1\\) ... \\(x_2\\) \\(f[x_2]\\) \\(f[x_1 x_2]\\) \\(f[x_0 x_1 x_2] = c_2\\) ... ... ... ... ... ... \\(x_n\\) \\(f[x_n]\\) \\(f[x_{n-1} x_n]\\) \\(f[x_{n-2} x_{n-1} x_n]\\) ... \\(f[x_0 x_1 ... x_n] = c_n\\) Seules les valeurs sur la diagonale interviennent dans l'expression du polyn\u00f4me d'interpolation de Newton. Si on ajoute un nouveau point d'interpolation \\((x_{n+1})\\) , il suffit d'ajouter une ligne au tableau. Pour d\u00e9terminer les valeurs de ce tableau, on applique la m\u00e9thode ici illustr\u00e9e pour 3 points d'interpolation, en enregistrant les coefficients dans un vecteur \\(C\\) : Une fois les coefficients calcul\u00e9s, on utilise la strat\u00e9gie de l' algorithme de Horner pour le calcul effectif du polyn\u00f4me interpolateur de Newton. Cette strat\u00e9gie se base sur le sch\u00e9ma suivant, ici illustr\u00e9 pour 4 points : \\(p(x) = c_0 + c_1 (x-x_0) + c_2 (x-x_0) (x-x_1) + c_3 (x-x_0) (x-x_1) (x-x_2)\\) \\(= c_0 + (x-x_0) (c_1 + (c_2 (x-x_1) + c_3 (x-x_1) (x-x_2)))\\) \\(= c_0 + (x-x_0) (c_1 + (x-x_1)(c_2 + c_3 (x-x_2)))\\) Cette m\u00e9thode permet de r\u00e9duire consid\u00e9rablement le nombre d'op\u00e9rations n\u00e9cessaires au calcul du polyn\u00f4me. Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : x le vecteur des abscisses des point connus. y le vecteur des ordonn\u00e9es des points connus. xp l'abscisse du point que l'on veut interpoler. On notera les variables suivantes : c le vecteur des coefficients du polyn\u00f4me de Newton. yp l'ordonn\u00e9e du point que l'on veut interpoler. def newton(x,y,xp): #R\u00e9cup\u00e9ration du nombre de points connus : n = len(x) #Initialisation d'un vecteur nul qui contiendra les coefficients du #polyn\u00f4me de Newton : c = np.zeros(n) #Boucle sur les ordonn\u00e9es connues pour d\u00e9terminer la 1\u00e8re colonne du #tableau des diff\u00e9rences divis\u00e9es (ordre 0) : for i in range(n): c[i] = y[i] #Boucle pour calculer les colonnes du tableau des diff\u00e9rences divis\u00e9es #(de gauche \u00e0 droite): for i in range(1,n): #Boucle pour calculer les lignes du tableau des diff\u00e9rences divis\u00e9es #(du bas jusqu'\u00e0 la diagonale de chaque colonnes): for k in range(n-1,i-1,-1): c[k] = (c[k]-c[k-1])/(x[k]-x[k-i]) #(Avec cette formule r\u00e9cursive sur les \u00e9l\u00e9ments du vecteur des #coefficients, on ne gardera en m\u00e9moire que les \u00e9l\u00e9ments de la #diagonale du tableau des diff\u00e9rences divis\u00e9es). #Calcul de l'ordonn\u00e9e du point interpol\u00e9 avec l'algorithme de Horner : yp = c[n-1] for i in range(n-2,-1,-1): yp = c[i] + (xp-x[i])*yp #Renvoyer l'ordonn\u00e9e du point interpol\u00e9 : return yp Exemple Voici la construction du polyn\u00f4me de Newton pour notre probl\u00e8me exemple : On observe bien qu'au fur et \u00e0 mesure que pour \\(i=0,1,...,9\\) , on ajoute les \\(c_i v_i(x)\\) au polyn\u00f4me, il passe par un point d'interpolation \\(x_i\\) de plus. Le polyn\u00f4me construit passe bien par \\(f(x_i)\\) pour tous les \\(x_i\\) . On trouve encore une fois une valeur interpol\u00e9e en \\(x = 210\\) d'environ 13.61 (r\u00e9sultat attendu puisqu'il s'agit en th\u00e9orie du m\u00eame polyn\u00f4me d'interpolation que celui obtenu avec la m\u00e9thode de Lagrange). Exercice : En modifiant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour l'interpolation de Newton, ainsi que la fonction \\(f\\) , d\u00e9terminez l'erreur d'interpolation en \\(x = 210\\) avec 4 chiffres significatifs. Comparez cette valeur \u00e0 l'erreur obtenue pour l'interpolation de Lagrange. Interpr\u00e9tez ce r\u00e9sultat. Erreur d'interpolation Formule de Taylor-Young Th\u00e9or\u00e8me de Taylor-Young Soit \\(f\\) une fonction continument d\u00e9rivable jusqu'\u00e0 l'ordre \\(n+1\\) sur un intervalle \\(I=[a,b]\\) contenant \\(n+1\\) points d'interpolation \\(x_i\\) ( \\(i=0,1,2,...,n\\) ). Alors \\(\\forall x \\in I\\) , \\(\\exists c \\in I\\) tel que : \\(e(x) = f(x) - p(x) = \\frac{f^{(n+1)}(c)}{(n+1)!} \\displaystyle\\prod_{i=0}^{n} (x-x_i)\\) Aux points d'interpolation, on a bien entendu \\(e(x_i) = 0\\) ( \\(i=0,1,2,...,n\\) ). La formule de Taylor-Young permet de borner l'erreur, et si possible de guider le choix des points d'interpolation. Dans le cas d'une distribution uniforme de points d'interpolation, on note : \\(x_i = x_{i-1} + h = x_0 + i h\\) avec \\(i=1,2,...,n\\) et \\(h>0\\) et \\(x_0\\) donn\u00e9s. Par exemple, \\(x_0=a\\) et \\(h = \\frac{b-a}{n}\\) sur \\([a,b]\\) . On montre alors que l'erreur d'interpolation est de l'ordre de \\(O(h^n)\\) . Plus exactement : \\(max_{x \\in [a,b]} e(x) \\leq \\frac{max_{x \\in [a,b]} f^{(n+1)}(x)}{4(n+1)} h^{n+1}\\) Ph\u00e9nom\u00e8ne de Runge Cependant, m\u00eame dans le cas d'une distribution uniforme des points d'interpolation, l'erreur ne tend pas n\u00e9cessairement vers 0 quand \\(n\\) tend vers l'infini. Quand l'erreur tend vers l'infini quand \\(n\\) tend vers l'infini, on parle de ph\u00e9nom\u00e8ne de Runge . Voici par exemple une fonction connue pour \u00eatre sensible au ph\u00e9nom\u00e8ne de Runge : \\(f(x) = \\frac{1}{1+32x^2}\\) Voici le polyn\u00f4me de Lagrange obtenu pour diff\u00e9rents nombres de points d'interpolation \u00e9quidistants : On voit des oscillations apparaitre, particuli\u00e8rement au voisinage des extr\u00e9mit\u00e9s de l'intervalle, lorsque le nombre de points d'interpolation augmente. On peut \u00e9viter le ph\u00e9nom\u00e8ne de Runge en choisissant correctement la distribution des points d'interpolation. En particulier, ce ph\u00e9nom\u00e8ne ne survient pas lorsque les points d'interpolation sont les racines du polyn\u00f4me de Chebychev . Interpolation aux noeuds de Chebychev D\u00e9finition On d\u00e9finit le polyn\u00f4me de Chebychev de degr\u00e9 \\(n\\) par : \\(T_n(x) = cos(n cos(x)) \\in [-1,1]\\) \\(T_n\\) poss\u00e8de \\(n\\) racines dans \\([-1,1]\\) . Les racines de ce polyn\u00f4me sont : \\(x_i = cos(\\frac{(2i+1) \\pi}{2n})\\) avec \\(i=0,1,2,...,n-1\\) Plus g\u00e9n\u00e9ralement, sur un intervalle \\([a,b]\\) : \\(x_i = \\frac{a+b}{2} + \\frac{b-a}{2} cos(\\frac{(2i+1) \\pi}{2n})\\) On peut montrer que l'erreur \\(e(x) = f(x)-p(x)\\) est minimale lorsque les points d'interpolation correspondent aux racines du polyn\u00f4me de Chebychev. Toutes les fonctions continuement d\u00e9rivables voient leur polyn\u00f4me d'interpolation converger pour ce choix de points d'interpolation. Voici \u00e0 nouveau le polyn\u00f4me de Lagrange obtenu avec \\(f(x) = \\frac{1}{1+32x^2}\\) pour diff\u00e9rents nombres de points d'interpolation, mais cette fois-ci en utilisant le polyn\u00f4me de Chebychev : Comme pr\u00e9vu, le polyn\u00f4me d'interpolation converge avec la fonction quand le nombre de points augmente. On a bien \u00e9vit\u00e9 le ph\u00e9nom\u00e8ne de Runge. Interpolation par morceaux L'interpolation polynomiale ne permet pas de correctement interpoler des fonctions qui varient rapidement. Il faudrait des polyn\u00f4mes de degr\u00e9 \u00e9lev\u00e9, ce qui entrainerait des oscillations. Id\u00e9e Pour \u00e9viter les oscillations, on peut utiliser l'interpolation par morceaux avec des polyn\u00f4mes de degr\u00e9 faible : - Des fonctions affines : interpolation affine ou lin\u00e9aire composite . - Des fonctions splines cubiques : interpolation par splines cubiques . Principe Etant donn\u00e9e une distribution de points \\(a=x_0<x_1<...<x_n=b\\) , on construit une approximation polynomiale de la fonction \\(f\\) sur chaque sous-intervalle \\([x_i,x_{i+1}]\\) \\((i=0,1,...,n-1)\\) . On obtient ainsi une fonction \\(g\\) telle que \\(g(x_i) = f(x_i)\\) pour \\(i=0,1,2,...,n\\) : - Si \\(g\\) est de degr\u00e9 0 ou 1 : ligne bris\u00e9e (interpolation affine). - Si \\(g\\) est de degr\u00e9 >1 : le choix n'est plus unique, et on ajoute des conditions de r\u00e9gularit\u00e9 aux noeuds pour que \\(g\\) soit continument d\u00e9rivable en ces points. Interpolation affine Sur chaque sous-intervalle \\([x_i,x_{i+1}]\\) avec \\(i=0,1,...,n-1\\) , on interpole \\(f\\) par un polyn\u00f4me de degr\u00e9 inf\u00e9rieur ou \u00e9gal \u00e0 1 : \\(g(x) = f(x_i) + \\frac{f(x_{i+1})-f(x_i)}{x_{i+1}-x_i} (x-x_i)\\) ou \\(g(x) = f(x_i) + f[x_{i+1} x_i] (x-x_i)\\) Si \\(f\\) est 2 fois d\u00e9rivables sur \\([a,b] = [x_0,x_n]\\) on montre alors que : \\(max \\mid f(x)-g(x) \\mid \\leq \\frac{H^2}{8} max_{x \\in [a,b]} |f\"(x)|\\) o\u00f9 \\(H\\) d\u00e9signe la longueur du plus grand sous-intervalle. Par cons\u00e9quent, pour tout \\(x \\in [a,b]\\) , l'erreur d'interpolation tend vers 0 quand \\(H\\) tend vers 0 (\u00e0 condition que \\(f\\) soit assez r\u00e9guli\u00e8re). Voici l'interpolation affine appliqu\u00e9e \u00e0 notre exemple : On trouve une valeur interpol\u00e9e en \\(x = 210\\) d'environ 13.50. Interpolation par fonctions splines L'inconv\u00e9nient de l'interpolation affine est que l'approximation de la fonction \\(f\\) manque de r\u00e9gularit\u00e9 : la fonction \\(g\\) n'est pas d\u00e9rivable. Dans le cadre de l'interpolation par splines cubiques, on va choisir une fonction v\u00e9rifiant les crit\u00e8res suivants : Sur chaque sous-intervalle \\([x_i,x_{i+1}]\\) avec \\(i=0,1,...,n\\) , la fonction est un polyn\u00f4me de degr\u00e9 \\(\\leq 3\\) qui interpole les points \\((x_i,f(x_i))\\) et \\((x_{i+1},f(x_{i+1}))\\) : \\(g_i(x) = a_i x^3 + b_i x^2 + c_i x + d_i\\) \\(g\\) est 2 fois continument d\u00e9rivable aux points int\u00e9rieurs \\(x_i\\) avec \\(i=0,1,...,n\\) . La fonction \\(g\\) obtenue en reliant les diff\u00e9rents \\(g_i\\) est ainsi 2 fois d\u00e9rivable. On peut montrer que si \\(f\\) est 3 fois d\u00e9rivable sur \\([a,b] = [x_0,x_n]\\) alors l'erreur d'interpolation tend vers 0 en \\(H^2\\) (avec \\(H\\) la longueur du plus grand sous-intervalle). La fonction \\(g\\) est appel\u00e9e spline d'interpolation cubique . Supposons \\(n+1\\) points d'interpolation et donc \\(n\\) sous-intervalles \\([x_i,x_{i+1}]\\) . Pour d\u00e9terminer \\(g\\) , il faut d\u00e9terminer \\(4n\\) coefficients \\((a_i,b_i,c_i,d_i)\\) pour \\(i=0,1,...,n-1\\) . Les contraintes sont les suivantes : Pour les points aux extr\u00e9mit\u00e9s : \\(g_0(x_0) = f(x_0)\\) et \\(g_{n-1}(x_n) = f(x_n)\\) soit 2 \u00e9quations . Pour les points int\u00e9rieurs : \\(g_{i-1}(x_i) = f(x_i)\\) et \\(g_i(x_i) = f(x_i)\\) pour \\(i=1,...,n-1\\) soit 2(n-1) \u00e9quations . Pour assurer la r\u00e9gularit\u00e9 de la courbe : \\(g'_{i-1}(x_i) = g'_i(x_i)\\) et \\(g\"_{i-1}(x_i) = g\"_i(x_i)\\) pour \\(i=1,...,n-1\\) soit 2(n-1) \u00e9quations . Au total, on a donc 4n-2 \u00e9quations pour 4n inconnues . On peut ajouter des contraintes pour avoir 2 \u00e9quations suppl\u00e9mentaires . Par exemple : \\(g\"_0(x_0)\\) et \\(g\"_n(x_n)=0\\) . Voici l'interpolation par splines cubiques appliqu\u00e9e \u00e0 notre exemple : On trouve une valeur interpol\u00e9e en \\(x = 210\\) d'environ 13.64. Conclusions Il existe un unique polyn\u00f4me de degr\u00e9 \u00e9gal \u00e0 \\(n\\) qui interpole les valeurs \\(f(x_i)\\) aux points \\(x_i\\) . Pour trouver ce polyn\u00f4me, on peut utiliser (du moins pratique au plus pratique) : la base canonique , la base de Lagrange , ou la base de Newton . L'erreur d'interpolation ne tend pas n\u00e9cessairement vers 0 quand \\(n\\) tend vers l'infini : c'est le ph\u00e9nom\u00e8ne de Runge . Ce probl\u00e8me peut \u00eatre r\u00e9solu par l'interpolation aux noeuds de Chebychev . Lorsque la fonction n'est connue qu'en certains points et varie vite, on peut aussi \u00e9vite le ph\u00e9nom\u00e8ne de Runge en utilisant l'interpolation par morceaux (affine ou splines cubiques).","title":"III. Interpolation polynomiale"},{"location":"Chap3_Interpolation_polynomiale/#chapitre-iii-interpolation-polynomiale","text":"Ce chapitre porte sur les m\u00e9thodes num\u00e9riques pour l'approximation de valeurs inconnues d'une fonction \u00e0 partir des valeurs connues, par interpolation avec un polyn\u00f4me.","title":"Chapitre III : Interpolation polynomiale"},{"location":"Chap3_Interpolation_polynomiale/#position-du-probleme","text":"","title":"Position du probl\u00e8me"},{"location":"Chap3_Interpolation_polynomiale/#motivation","text":"Soit une fonction \\(f(x)\\) connue en seulement \\(n+1\\) points, appel\u00e9s points ou noeuds d'interpolation \\((x_i,f(x_i))\\) avec \\(i=0,1,2,...,n\\) de l'intervalle \\([a,b]\\) . Peut-on approcher \\(f(x)\\) pour tout \\(x\\) de \\([a,b]\\) par une fonction ? Il existe une infinit\u00e9 de fonctions d'interpolation, mais le plus simple est d'approcher la fonction par un polyn\u00f4me de degr\u00e9 suffisamment \u00e9lev\u00e9 pour que sa courbe passe par les point d'interpolation. Un polyn\u00f4me \\(p\\) de degr\u00e9 inf\u00e9rieur ou \u00e9gal \u00e0 \\(n\\) s'exprime dans la base canonique \\({1,x,x^2,...,x^n}\\) de la mani\u00e8re suivante : \\(p(x) = \\displaystyle\\sum_{k=0}^{n} a_k x^k\\) o\u00f9 les \\(a_k\\) sont les coefficients du polyn\u00f4me. L'interpolation polynomiale consiste donc \u00e0 d\u00e9terminer les coefficients \\(a_k\\) tels que \\(p(x_i) = f(x_i)\\) pour \\(i=0,1,2,...,n\\) . NB : L'interpolation polynomiale pourra sont utiles pour les m\u00e9thodes num\u00e9riques de calcul d'int\u00e9grales et de d\u00e9riv\u00e9es. Attention ! L'interpolation polynomiale et l'approximation polynomiales sont des approches diff\u00e9rentes. L'approximation polynomiale de donn\u00e9es bruit\u00e9es cherche un polyn\u00f4me de degr\u00e9 inf\u00e9rieur au nombre de donn\u00e9es qui ne passe pas n\u00e9cessairement par tous les points connus.","title":"Motivation"},{"location":"Chap3_Interpolation_polynomiale/#existence-et-unicite-dun-polynome-dinterpolation","text":"Th\u00e9or\u00e8me d'Evariste Galois Un polyn\u00f4me de degr\u00e9 \\(n\\) a au plus \\(n\\) racines qui peuvent \u00eatre r\u00e9elles ou complexes conjugu\u00e9es. D'o\u00f9 le corollaire : Corollaire On ne peut faire passer par \\(n+1\\) points distincts qu'un seul polyn\u00f4me de degr\u00e9 n. Toutes les m\u00e9thodes pr\u00e9sent\u00e9es dans la suite de ce chapitre doivent donc aboutir au m\u00eame polyn\u00f4me .","title":"Existence et unicit\u00e9 d'un polyn\u00f4me d'interpolation"},{"location":"Chap3_Interpolation_polynomiale/#exemple-de-probleme","text":"Au cours de ce chapitre, nous appliquerons les diff\u00e9rentes m\u00e9thodes num\u00e9riques d'interpolation polynomiale \u00e0 un m\u00eame exemple : l'estimation de la dur\u00e9e du jour \u00e0 l'UFR des sciences de l'UVSQ . La dur\u00e9e du jour (temps entre le lever et le coucher du soleil) en heures peut \u00eatre approxim\u00e9e par la fonction \\(f\\) suivante : \\(f(x) = \\frac{48}{2 \\pi} \\arccos(\\tan(\\lambda) \\tan(\\arcsin(\\sin(\\frac{2 \\pi x}{365}) \\sin(\\delta))))\\) avec \\(x\\) le jour depuis l'\u00e9quinoxe d'automne, \\(\\lambda\\) la latitude du lieu, et \\(\\delta\\) la latitude des tropiques. On sait que \\(\\delta \\approx 23.438403\u00b0\\) . On prendra ici l'exemple de l'UFR des sciences de l'UVSQ, dont la latitude est \\(\\lambda \\approx 48.81094\u00b0\\) . On peut voir que cette formule implique 6 appels \u00e0 des fonctions trigonom\u00e9triques, ce qui peut rendre non n\u00e9gligeable le temps n\u00e9cessaire pour \u00e9valuer \\(f\\) en un grand nombre de points. D'o\u00f9 l'int\u00e9r\u00eat de n'\u00e9valuer la fonction qu'en un nombre limit\u00e9 de points, et d'utiliser l'interpolation polynomiale pour d\u00e9terminer d'autres valeurs de \\(f(x)\\) . Nous choisirons ici d'\u00e9valuer la fonction pour les 10 valeurs de \\(x\\) suivantes : x = jours depuis l'\u00e9quinoxe d'automne 30 60 90 120 150 180 240 270 300 330 f(x) = dur\u00e9e du jour en heures 10.24 8.73 8.04 8.63 10.09 11.84 15.16 15.95 15.47 14.06 Et nous essayerons d'estimer la valeur de \\(f\\) pour \\(x = 210\\) (i.e. la dur\u00e9e du jour en heures pour le 210\u00e8me jour depuis l'\u00e9quinoxe d'automne) par interpolation polynomiale. Sous Python on utilisera la biblioth\u00e8que Numpy : import numpy as np Puis, on d\u00e9finira les variables globales suivantes : l = 48.81094*np.pi/180 #Latitude de l'UFR des sciences a = 23.438403*np.pi/180 #Latitude des tropiques La fonction \\(f\\) sera d\u00e9finie comme : def f(d): return 48/(2*np.pi)*np.arccos(np.tan(l)*np.tan(np.arcsin(np.sin(a)*np.sin(d*2*np.pi/365.25)))) On calculera alors les 10 valeurs \"connues\" de la fonction de la mani\u00e8re suivante : x = np.array([30,60,90,120,150,180,240,270,300,330],dtype='float') y = f(x) (On a ici d\u00e9finit avec Numpy un vecteur de 10 valeurs \\(x\\) , auquel on a appliqu\u00e9 \\(f\\) . Le vecteur r\u00e9sultant est stock\u00e9 dans \\(y\\) ).","title":"Exemple de probl\u00e8me"},{"location":"Chap3_Interpolation_polynomiale/#matrices-de-vandermonde","text":"Une 1\u00e8re approche pour obtenir un polyn\u00f4me d'interpolation est la suivante : d\u00e9terminer les coefficients \\(a_i\\) du polyn\u00f4me en r\u00e9solvant les \\((n+1)\\) \u00e9quations de collocation \\(p(x_i) = f(x_i)\\) pour \\(i=0,1,2,...,n\\) . Ceci revient \u00e0 r\u00e9soudre le syst\u00e8me lin\u00e9aire de \\((n+1)\\) \u00e9quations \u00e0 \\((n+1)\\) inconnues : \\(\\begin{cases} a_0 + a_1 x_0 + a_2 x_0^2 + ... + a_n x_0^n = f(x_0)\\\\ a_0 + a_1 x_1 + a_2 x_1^2 + ... + a_n x_1^n = f(x_1)\\\\ ...\\\\ a_0 + a_1 x_n + a_2 x_n^2 + ... + a_n x_n^n = f(x_n) \\end{cases}\\) Ce syst\u00e8me admet une unique solution si les \\(x_i\\) sont distincts 2 \u00e0 2. Il peut s'\u00e9crire sous la forme matricielle suivante : \\(\\begin{pmatrix} 1 & x_0 & x_0^2 & \\cdots & x_0^n \\\\ 1 & x_1 & x_1^2 & \\cdots & x_1^n \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 &\\cdots & x_n^n \\end{pmatrix} \\begin{pmatrix} a_0\\\\ a_1\\\\ \\vdots\\\\ a_n \\end{pmatrix} = \\begin{pmatrix} f(x_0)\\\\ f(x_1)\\\\ \\vdots\\\\ f(x_n) \\end{pmatrix}\\) On reconnait ici une matrice de Vandermonde . La matrice de Vandermonde est inversible si et seulement si les \\(x_i\\) sont distincts 2 \u00e0 2. Dans la pratique, l'inversion de la matrice de Vandermonde ne conduit pas \u00e0 une solution satisfaisante : Le nombre d'\u00e9quations / d'inconnues croit avec le nombre de points d'interpolations, augmentant le nombre d'op\u00e9rations de l'ordre de \\(2 n^3 / 3\\) . Ce type de syst\u00e8me est souvent mal conditionn\u00e9, ce qui rend la solution num\u00e9rique tr\u00e8s sensible aux erreurs d'arrondi. C'est pourquoi dans la suite, on va pr\u00e9f\u00e9rer des techniques exprimant le polyn\u00f4me dans une autre base que la base canonique .","title":"Matrices de Vandermonde"},{"location":"Chap3_Interpolation_polynomiale/#polynomes-de-lagrange","text":"","title":"Polyn\u00f4mes de Lagrange"},{"location":"Chap3_Interpolation_polynomiale/#lalgorithme","text":"Base de Lagrange Soient des \\(x_i\\) (avec \\(i=0,1,2,...,n\\) ) 2 \u00e0 2 distincts. On appelle base de Lagrange relative aux points x_i les polyn\u00f4mes : \\(L_i(x) = \\displaystyle\\prod_{j=0 , j \\neq i}^{n} \\frac{(x-x_j)}{x_i-x_j}\\) soit \\(L_i(x) = \\frac{(x-x_0)(x-x_1)...(x-x_n)}{(x_i-x_0)(x_i-x_1)...(x_i-x_n)}\\) \\(L_i\\) v\u00e9rifie \\(L_i(x_i)=1\\) et \\(L_i(x_j)=0\\) si \\(j \\neq i\\) . La famille des \\((L_i(x))\\) forme une base de l'ensemble des polyn\u00f4mes, et le polyn\u00f4me qui interpoles les valeurs de \\(f(x_i)\\) aux points \\(x_i\\) s'\u00e9crit : \\(p(x) = \\displaystyle\\sum_{i=0}^{n} f(x_i) L_i(x) = f(x_0) L_0(x) + f(x_1) L_1(x) + ... + f(x_n) L_n(x)\\) \\(p\\) v\u00e9rifie bien que \\(\\forall i = 0,1,2,...,n\\) , \\(p(x_i) = f(x_i)\\) et est unique si les points \\(x_i\\) sont distincts. Les coefficients du polyn\u00f4me sont directements les valeurs \\(f(x_i)\\) , qui sont connues. Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : x le vecteur des abscisses des point connus. y le vecteur des ordonn\u00e9es des points connus. xp l'abscisse du point que l'on veut interpoler. On notera les variables suivantes : Li le polyn\u00f4me de la base de Lagrange associ\u00e9 au point \\(x_i\\) . yp l'ordonn\u00e9e du point que l'on veut interpoler. def lagrange(x,y,xp): #R\u00e9cup\u00e9ration du nombre de points connus : n = len(x) #Initialisation de l'ordonn\u00e9e du point interpol\u00e9 yp : yp = 0 #1\u00e8re boucle sur les points connus (x[i],y[i]) : for i in range(n): #Initialisation du polyn\u00f4me de la base de Lagrange associ\u00e9 au i-\u00e8me #point connu Li : Li = 1 #2\u00e8me boucle sur les abscisses connues x[j]: for j in range(n): #Dans le cas o\u00f9 les 2 abscisses connues x[i] et x[j] sont distinctes, #multiplication de Li par un coefficient obtenu avec xp, x[i] et #x[j], de telle fa\u00e7on que Li = 1 si xp = x[i] et Li = 0 si xp = x[j] #(voir formule du cours) : if j!=i: Li = Li*(xp-x[j])/(x[i]-x[j]) #Addition \u00e0 yp de la valeur du polyn\u00f4me y[i] Li, qui est \u00e9gal \u00e0 y[i] en #x[i] et nul pour tout les x[j] (voir formule du cours): yp = yp + y[i]*Li #Renvoyer l'ordonn\u00e9e du point interpol\u00e9 : return yp La base des polyn\u00f4mes de Lagrange permet de ne pas avoir \u00e0 r\u00e9soudre un syst\u00e8me lin\u00e9aire de \\(n+1\\) \u00e9quations \u00e0 \\(n+1\\) inconnues. Lorsque \\(n\\) est petit, il reste cependant plus simple de r\u00e9soudre le syst\u00e8me d'\u00e9quations. Un d\u00e9saventage pratique de l'interpolation de Lagrange est le fait qu'il soit n\u00e9cessaire tout recalculer si on ajoute un point d'interpolation. L'interpolation de Newton, pr\u00e9sent\u00e9e dans la suite, n'a pas ce d\u00e9saventage.","title":"L'algorithme"},{"location":"Chap3_Interpolation_polynomiale/#exemple","text":"Voici la construction du polyn\u00f4me de Lagrange pour notre probl\u00e8me exemple : On observe bien que pour \\(i=0,1,...,9\\) , chaque \\(f(x_i)L_i(x)\\) passe par \\(f(x_i)\\) en \\(x = x_i\\) et par 0 en \\(x = x_j\\) pour \\(j \\neq i\\) . Le polyn\u00f4me construit passe bien par \\(f(x_i)\\) pour tous les \\(x_i\\) . On trouve une valeur interpol\u00e9e en \\(x = 210\\) d'environ 13.61. Exercice : En modifiant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour l'interpolation de Lagrange, ainsi que la fonction \\(f\\) , d\u00e9terminez l'erreur d'interpolation en \\(x = 210\\) avec 4 chiffres significatifs.","title":"Exemple"},{"location":"Chap3_Interpolation_polynomiale/#polynomes-de-newton","text":"","title":"Polyn\u00f4mes de Newton"},{"location":"Chap3_Interpolation_polynomiale/#lalgorithme_1","text":"Base de Newton Soient des \\(x_i\\) (avec \\(i=0,1,2,...,n\\) ) 2 \u00e0 2 distincts. On appelle base de Newton relative aux points \\(x_i\\) les polyn\u00f4mes : \\(v_0(x) = 1\\) \\(v_i(x) = \\displaystyle\\prod_{j=0}^{i-1} (x-x_j) = (x-x_0)(x-x_1)...(x-x_{i-1})\\) \\(v_i\\) v\u00e9rifie \\(v_i(x_j) = 0\\) pour \\(j<i\\) . La famille \\((v_i(x))\\) forme une base de l'ensemble des polyn\u00f4mes, et le polyn\u00f4me qui interpole les valeurs \\(f(x_i)\\) aux points \\(x_i\\) s'\u00e9crit : \\(p(x) = \\displaystyle\\sum_{i=0}^{n} c_i v_i(x) = c_0 v_0(x) + c_1 v_1(x) + ... + c_n v_n(x)\\) avec des coefficients c_i \u00e0 d\u00e9terminer, tels que \\(p(x_i) = f(x_i) \\forall i = 0,1,2,...,n\\) . \\(p\\) est unique si les \\(x_i\\) sont 2 \u00e0 2 distincts. L'un des int\u00e9r\u00eats de la base de Newton est que si l'on ajoute un nouveau point d'interpolation \\((x_{n+1},f(x_{n+1}))\\) , les coefficients \\(c_0,c_1,c_2,...,c_n\\) restent inchang\u00e9s et il suffit de calculer \\(c_{n+1}\\) et d'ajouter un terme \\(c_{n+1} v_{n+1}\\) \u00e0 l'expression de \\(p\\) . Les \\(c_i\\) sont les solutions du syst\u00e8me lin\u00e9aire (triangulaire inf\u00e9rieur) suivant : \\(\\begin{cases} f(x_0) = c_0 v_0(x_0) = c_0\\\\ f(x_1) = c_0 + c_1 v_1(x_1) = c_0 + c_1 (x_1-x_0)\\\\ f(x_2) = c_0 + c_1 v_1(x_2) + c_2 v_2(x_2) = c_0 + c_1 (x_2-x_0) + c_2 (x_2-x_0)(x_2-x_1)\\\\ ...\\\\ f(x_n) = c_0 + \\displaystyle\\sum_{i=1}^{n} c_i v_i(x_n) \\end{cases}\\) Ce syst\u00e8me est en apparence simple, et peut \u00eatre r\u00e9solu de proche en proche. Mais dans la pratique, les expressions des \\(c_i\\) deviennent de plus en plus complexes. C'est pourquoi on fait appel aux diff\u00e9rences divis\u00e9es pour exprimer les coefficients de fa\u00e7on compacte. Les diff\u00e9rences divis\u00e9es Soit \\(f\\) une fonction d\u00e9finie aux points \\(x_i\\) , 2 \u00e0 2 distincts. On d\u00e9finit les diff\u00e9rences divis\u00e9es par r\u00e9currence comme suit : \\(\\begin{cases} f[x_i] = f(x_i), i=0,...,n\\\\ f[x_i...x_{i+k}] = \\frac{f[x_i...x_{i+k-1}]-f[x_{i+1}...x_{i+k}]}{x_i-x_{i+k}}, i=0,...,n \\end{cases}\\) avec \\(k\\) l'ordre de la r\u00e9currence. Les coefficients \\(c_i\\) peuvent \u00eatre calcul\u00e9s par r\u00e9currence \u00e0 partir des diff\u00e9rences divis\u00e9es de la mani\u00e8re suivante : \\(c_i = f[x_0 x_1 ... x_i] = \\frac{f[x_0 x_1 ... x_{i-1}]-f[x_1 x_2 ... x_i]}{x_0-x_i}\\) C'est ce que l'on appelle la diff\u00e9rence divis\u00e9e d'ordre i . Le polyn\u00f4me d'interpolation de Newton de degr\u00e9 \\(n\\) qui interpole les valeurs \\(f(x_i)\\) aux points \\(x_i\\) \\((i=0,1,2,...,n)\\) , distincts 2 \u00e0 2, s'\u00e9crit donc : \\(p(x) = \\displaystyle\\sum_{i=0}^{n} f[x_0 x_1 ... x_i] v_i(x)\\) Soit \\(p(x) = f[x_0] + f[x_0 x_1] (x-x_0) + f[x_0 x_1 x_2] (x-x_0)(x-x_1) + ... + f[x_0 x_1 ... x_n] \\displaystyle\\prod_{j=0}^{n-1} (x-x_j)\\) Le calcul effectif du polyn\u00f4me d'interpolation se fait donc de la mani\u00e8re suivante : \\(i=0\\) \\(i=1\\) \\(i=2\\) ... \\(i=n\\) \\(x_0\\) \\(f[x_0] = c_0\\) ... \\(x_1\\) \\(f[x_1]\\) \\(f[x_0 x_1] = c_1\\) ... \\(x_2\\) \\(f[x_2]\\) \\(f[x_1 x_2]\\) \\(f[x_0 x_1 x_2] = c_2\\) ... ... ... ... ... ... \\(x_n\\) \\(f[x_n]\\) \\(f[x_{n-1} x_n]\\) \\(f[x_{n-2} x_{n-1} x_n]\\) ... \\(f[x_0 x_1 ... x_n] = c_n\\) Seules les valeurs sur la diagonale interviennent dans l'expression du polyn\u00f4me d'interpolation de Newton. Si on ajoute un nouveau point d'interpolation \\((x_{n+1})\\) , il suffit d'ajouter une ligne au tableau. Pour d\u00e9terminer les valeurs de ce tableau, on applique la m\u00e9thode ici illustr\u00e9e pour 3 points d'interpolation, en enregistrant les coefficients dans un vecteur \\(C\\) : Une fois les coefficients calcul\u00e9s, on utilise la strat\u00e9gie de l' algorithme de Horner pour le calcul effectif du polyn\u00f4me interpolateur de Newton. Cette strat\u00e9gie se base sur le sch\u00e9ma suivant, ici illustr\u00e9 pour 4 points : \\(p(x) = c_0 + c_1 (x-x_0) + c_2 (x-x_0) (x-x_1) + c_3 (x-x_0) (x-x_1) (x-x_2)\\) \\(= c_0 + (x-x_0) (c_1 + (c_2 (x-x_1) + c_3 (x-x_1) (x-x_2)))\\) \\(= c_0 + (x-x_0) (c_1 + (x-x_1)(c_2 + c_3 (x-x_2)))\\) Cette m\u00e9thode permet de r\u00e9duire consid\u00e9rablement le nombre d'op\u00e9rations n\u00e9cessaires au calcul du polyn\u00f4me. Voici l'algorithme sous la forme d'une fonction Python. Elle prend en entr\u00e9e : x le vecteur des abscisses des point connus. y le vecteur des ordonn\u00e9es des points connus. xp l'abscisse du point que l'on veut interpoler. On notera les variables suivantes : c le vecteur des coefficients du polyn\u00f4me de Newton. yp l'ordonn\u00e9e du point que l'on veut interpoler. def newton(x,y,xp): #R\u00e9cup\u00e9ration du nombre de points connus : n = len(x) #Initialisation d'un vecteur nul qui contiendra les coefficients du #polyn\u00f4me de Newton : c = np.zeros(n) #Boucle sur les ordonn\u00e9es connues pour d\u00e9terminer la 1\u00e8re colonne du #tableau des diff\u00e9rences divis\u00e9es (ordre 0) : for i in range(n): c[i] = y[i] #Boucle pour calculer les colonnes du tableau des diff\u00e9rences divis\u00e9es #(de gauche \u00e0 droite): for i in range(1,n): #Boucle pour calculer les lignes du tableau des diff\u00e9rences divis\u00e9es #(du bas jusqu'\u00e0 la diagonale de chaque colonnes): for k in range(n-1,i-1,-1): c[k] = (c[k]-c[k-1])/(x[k]-x[k-i]) #(Avec cette formule r\u00e9cursive sur les \u00e9l\u00e9ments du vecteur des #coefficients, on ne gardera en m\u00e9moire que les \u00e9l\u00e9ments de la #diagonale du tableau des diff\u00e9rences divis\u00e9es). #Calcul de l'ordonn\u00e9e du point interpol\u00e9 avec l'algorithme de Horner : yp = c[n-1] for i in range(n-2,-1,-1): yp = c[i] + (xp-x[i])*yp #Renvoyer l'ordonn\u00e9e du point interpol\u00e9 : return yp","title":"L'algorithme"},{"location":"Chap3_Interpolation_polynomiale/#exemple_1","text":"Voici la construction du polyn\u00f4me de Newton pour notre probl\u00e8me exemple : On observe bien qu'au fur et \u00e0 mesure que pour \\(i=0,1,...,9\\) , on ajoute les \\(c_i v_i(x)\\) au polyn\u00f4me, il passe par un point d'interpolation \\(x_i\\) de plus. Le polyn\u00f4me construit passe bien par \\(f(x_i)\\) pour tous les \\(x_i\\) . On trouve encore une fois une valeur interpol\u00e9e en \\(x = 210\\) d'environ 13.61 (r\u00e9sultat attendu puisqu'il s'agit en th\u00e9orie du m\u00eame polyn\u00f4me d'interpolation que celui obtenu avec la m\u00e9thode de Lagrange). Exercice : En modifiant la fonction Python donn\u00e9e pr\u00e9c\u00e9demment pour l'interpolation de Newton, ainsi que la fonction \\(f\\) , d\u00e9terminez l'erreur d'interpolation en \\(x = 210\\) avec 4 chiffres significatifs. Comparez cette valeur \u00e0 l'erreur obtenue pour l'interpolation de Lagrange. Interpr\u00e9tez ce r\u00e9sultat.","title":"Exemple"},{"location":"Chap3_Interpolation_polynomiale/#erreur-dinterpolation","text":"","title":"Erreur d'interpolation"},{"location":"Chap3_Interpolation_polynomiale/#formule-de-taylor-young","text":"Th\u00e9or\u00e8me de Taylor-Young Soit \\(f\\) une fonction continument d\u00e9rivable jusqu'\u00e0 l'ordre \\(n+1\\) sur un intervalle \\(I=[a,b]\\) contenant \\(n+1\\) points d'interpolation \\(x_i\\) ( \\(i=0,1,2,...,n\\) ). Alors \\(\\forall x \\in I\\) , \\(\\exists c \\in I\\) tel que : \\(e(x) = f(x) - p(x) = \\frac{f^{(n+1)}(c)}{(n+1)!} \\displaystyle\\prod_{i=0}^{n} (x-x_i)\\) Aux points d'interpolation, on a bien entendu \\(e(x_i) = 0\\) ( \\(i=0,1,2,...,n\\) ). La formule de Taylor-Young permet de borner l'erreur, et si possible de guider le choix des points d'interpolation. Dans le cas d'une distribution uniforme de points d'interpolation, on note : \\(x_i = x_{i-1} + h = x_0 + i h\\) avec \\(i=1,2,...,n\\) et \\(h>0\\) et \\(x_0\\) donn\u00e9s. Par exemple, \\(x_0=a\\) et \\(h = \\frac{b-a}{n}\\) sur \\([a,b]\\) . On montre alors que l'erreur d'interpolation est de l'ordre de \\(O(h^n)\\) . Plus exactement : \\(max_{x \\in [a,b]} e(x) \\leq \\frac{max_{x \\in [a,b]} f^{(n+1)}(x)}{4(n+1)} h^{n+1}\\)","title":"Formule de Taylor-Young"},{"location":"Chap3_Interpolation_polynomiale/#phenomene-de-runge","text":"Cependant, m\u00eame dans le cas d'une distribution uniforme des points d'interpolation, l'erreur ne tend pas n\u00e9cessairement vers 0 quand \\(n\\) tend vers l'infini. Quand l'erreur tend vers l'infini quand \\(n\\) tend vers l'infini, on parle de ph\u00e9nom\u00e8ne de Runge . Voici par exemple une fonction connue pour \u00eatre sensible au ph\u00e9nom\u00e8ne de Runge : \\(f(x) = \\frac{1}{1+32x^2}\\) Voici le polyn\u00f4me de Lagrange obtenu pour diff\u00e9rents nombres de points d'interpolation \u00e9quidistants : On voit des oscillations apparaitre, particuli\u00e8rement au voisinage des extr\u00e9mit\u00e9s de l'intervalle, lorsque le nombre de points d'interpolation augmente. On peut \u00e9viter le ph\u00e9nom\u00e8ne de Runge en choisissant correctement la distribution des points d'interpolation. En particulier, ce ph\u00e9nom\u00e8ne ne survient pas lorsque les points d'interpolation sont les racines du polyn\u00f4me de Chebychev .","title":"Ph\u00e9nom\u00e8ne de Runge"},{"location":"Chap3_Interpolation_polynomiale/#interpolation-aux-noeuds-de-chebychev","text":"D\u00e9finition On d\u00e9finit le polyn\u00f4me de Chebychev de degr\u00e9 \\(n\\) par : \\(T_n(x) = cos(n cos(x)) \\in [-1,1]\\) \\(T_n\\) poss\u00e8de \\(n\\) racines dans \\([-1,1]\\) . Les racines de ce polyn\u00f4me sont : \\(x_i = cos(\\frac{(2i+1) \\pi}{2n})\\) avec \\(i=0,1,2,...,n-1\\) Plus g\u00e9n\u00e9ralement, sur un intervalle \\([a,b]\\) : \\(x_i = \\frac{a+b}{2} + \\frac{b-a}{2} cos(\\frac{(2i+1) \\pi}{2n})\\) On peut montrer que l'erreur \\(e(x) = f(x)-p(x)\\) est minimale lorsque les points d'interpolation correspondent aux racines du polyn\u00f4me de Chebychev. Toutes les fonctions continuement d\u00e9rivables voient leur polyn\u00f4me d'interpolation converger pour ce choix de points d'interpolation. Voici \u00e0 nouveau le polyn\u00f4me de Lagrange obtenu avec \\(f(x) = \\frac{1}{1+32x^2}\\) pour diff\u00e9rents nombres de points d'interpolation, mais cette fois-ci en utilisant le polyn\u00f4me de Chebychev : Comme pr\u00e9vu, le polyn\u00f4me d'interpolation converge avec la fonction quand le nombre de points augmente. On a bien \u00e9vit\u00e9 le ph\u00e9nom\u00e8ne de Runge.","title":"Interpolation aux noeuds de Chebychev"},{"location":"Chap3_Interpolation_polynomiale/#interpolation-par-morceaux","text":"L'interpolation polynomiale ne permet pas de correctement interpoler des fonctions qui varient rapidement. Il faudrait des polyn\u00f4mes de degr\u00e9 \u00e9lev\u00e9, ce qui entrainerait des oscillations. Id\u00e9e Pour \u00e9viter les oscillations, on peut utiliser l'interpolation par morceaux avec des polyn\u00f4mes de degr\u00e9 faible : - Des fonctions affines : interpolation affine ou lin\u00e9aire composite . - Des fonctions splines cubiques : interpolation par splines cubiques . Principe Etant donn\u00e9e une distribution de points \\(a=x_0<x_1<...<x_n=b\\) , on construit une approximation polynomiale de la fonction \\(f\\) sur chaque sous-intervalle \\([x_i,x_{i+1}]\\) \\((i=0,1,...,n-1)\\) . On obtient ainsi une fonction \\(g\\) telle que \\(g(x_i) = f(x_i)\\) pour \\(i=0,1,2,...,n\\) : - Si \\(g\\) est de degr\u00e9 0 ou 1 : ligne bris\u00e9e (interpolation affine). - Si \\(g\\) est de degr\u00e9 >1 : le choix n'est plus unique, et on ajoute des conditions de r\u00e9gularit\u00e9 aux noeuds pour que \\(g\\) soit continument d\u00e9rivable en ces points.","title":"Interpolation par morceaux"},{"location":"Chap3_Interpolation_polynomiale/#interpolation-affine","text":"Sur chaque sous-intervalle \\([x_i,x_{i+1}]\\) avec \\(i=0,1,...,n-1\\) , on interpole \\(f\\) par un polyn\u00f4me de degr\u00e9 inf\u00e9rieur ou \u00e9gal \u00e0 1 : \\(g(x) = f(x_i) + \\frac{f(x_{i+1})-f(x_i)}{x_{i+1}-x_i} (x-x_i)\\) ou \\(g(x) = f(x_i) + f[x_{i+1} x_i] (x-x_i)\\) Si \\(f\\) est 2 fois d\u00e9rivables sur \\([a,b] = [x_0,x_n]\\) on montre alors que : \\(max \\mid f(x)-g(x) \\mid \\leq \\frac{H^2}{8} max_{x \\in [a,b]} |f\"(x)|\\) o\u00f9 \\(H\\) d\u00e9signe la longueur du plus grand sous-intervalle. Par cons\u00e9quent, pour tout \\(x \\in [a,b]\\) , l'erreur d'interpolation tend vers 0 quand \\(H\\) tend vers 0 (\u00e0 condition que \\(f\\) soit assez r\u00e9guli\u00e8re). Voici l'interpolation affine appliqu\u00e9e \u00e0 notre exemple : On trouve une valeur interpol\u00e9e en \\(x = 210\\) d'environ 13.50.","title":"Interpolation affine"},{"location":"Chap3_Interpolation_polynomiale/#interpolation-par-fonctions-splines","text":"L'inconv\u00e9nient de l'interpolation affine est que l'approximation de la fonction \\(f\\) manque de r\u00e9gularit\u00e9 : la fonction \\(g\\) n'est pas d\u00e9rivable. Dans le cadre de l'interpolation par splines cubiques, on va choisir une fonction v\u00e9rifiant les crit\u00e8res suivants : Sur chaque sous-intervalle \\([x_i,x_{i+1}]\\) avec \\(i=0,1,...,n\\) , la fonction est un polyn\u00f4me de degr\u00e9 \\(\\leq 3\\) qui interpole les points \\((x_i,f(x_i))\\) et \\((x_{i+1},f(x_{i+1}))\\) : \\(g_i(x) = a_i x^3 + b_i x^2 + c_i x + d_i\\) \\(g\\) est 2 fois continument d\u00e9rivable aux points int\u00e9rieurs \\(x_i\\) avec \\(i=0,1,...,n\\) . La fonction \\(g\\) obtenue en reliant les diff\u00e9rents \\(g_i\\) est ainsi 2 fois d\u00e9rivable. On peut montrer que si \\(f\\) est 3 fois d\u00e9rivable sur \\([a,b] = [x_0,x_n]\\) alors l'erreur d'interpolation tend vers 0 en \\(H^2\\) (avec \\(H\\) la longueur du plus grand sous-intervalle). La fonction \\(g\\) est appel\u00e9e spline d'interpolation cubique . Supposons \\(n+1\\) points d'interpolation et donc \\(n\\) sous-intervalles \\([x_i,x_{i+1}]\\) . Pour d\u00e9terminer \\(g\\) , il faut d\u00e9terminer \\(4n\\) coefficients \\((a_i,b_i,c_i,d_i)\\) pour \\(i=0,1,...,n-1\\) . Les contraintes sont les suivantes : Pour les points aux extr\u00e9mit\u00e9s : \\(g_0(x_0) = f(x_0)\\) et \\(g_{n-1}(x_n) = f(x_n)\\) soit 2 \u00e9quations . Pour les points int\u00e9rieurs : \\(g_{i-1}(x_i) = f(x_i)\\) et \\(g_i(x_i) = f(x_i)\\) pour \\(i=1,...,n-1\\) soit 2(n-1) \u00e9quations . Pour assurer la r\u00e9gularit\u00e9 de la courbe : \\(g'_{i-1}(x_i) = g'_i(x_i)\\) et \\(g\"_{i-1}(x_i) = g\"_i(x_i)\\) pour \\(i=1,...,n-1\\) soit 2(n-1) \u00e9quations . Au total, on a donc 4n-2 \u00e9quations pour 4n inconnues . On peut ajouter des contraintes pour avoir 2 \u00e9quations suppl\u00e9mentaires . Par exemple : \\(g\"_0(x_0)\\) et \\(g\"_n(x_n)=0\\) . Voici l'interpolation par splines cubiques appliqu\u00e9e \u00e0 notre exemple : On trouve une valeur interpol\u00e9e en \\(x = 210\\) d'environ 13.64.","title":"Interpolation par fonctions splines"},{"location":"Chap3_Interpolation_polynomiale/#conclusions","text":"Il existe un unique polyn\u00f4me de degr\u00e9 \u00e9gal \u00e0 \\(n\\) qui interpole les valeurs \\(f(x_i)\\) aux points \\(x_i\\) . Pour trouver ce polyn\u00f4me, on peut utiliser (du moins pratique au plus pratique) : la base canonique , la base de Lagrange , ou la base de Newton . L'erreur d'interpolation ne tend pas n\u00e9cessairement vers 0 quand \\(n\\) tend vers l'infini : c'est le ph\u00e9nom\u00e8ne de Runge . Ce probl\u00e8me peut \u00eatre r\u00e9solu par l'interpolation aux noeuds de Chebychev . Lorsque la fonction n'est connue qu'en certains points et varie vite, on peut aussi \u00e9vite le ph\u00e9nom\u00e8ne de Runge en utilisant l'interpolation par morceaux (affine ou splines cubiques).","title":"Conclusions"},{"location":"Chap4_Integration_numerique/","text":"Chapitre IV : Int\u00e9gration num\u00e9rique Ce chapitre porte sur les m\u00e9thodes num\u00e9riques pour l'approximation de l'int\u00e9grale d'une fonction. Position du probl\u00e8me Motivation Le but de l'int\u00e9gration num\u00e9rique est d'\u00e9valuer la valeur de l' int\u00e9grale \\(I\\) d'une fonction \\(f\\) continue sur un intervalle \\([a,b]\\) avec \\(a<b\\) r\u00e9els : \\(I = \\int_{a}^{b} f(x) dx\\) On rappelle que cette int\u00e9grale repr\u00e9sente l' aire comprise entre la courbe de la fonction, l'axe des abscisses, et les droites \\(x=a\\) et \\(x=b\\) . \\(I\\) est une int\u00e9grale d\u00e9finie dont le r\u00e9sultat est un scalaire. La fonction \\(f\\) peut \u00eatre connue qu'en certains points, mais ne dispose pas de singularit\u00e9 sur \\([a,b]\\) , qui est suppos\u00e9 fini et ferm\u00e9 . Dans certains cas limit\u00e9s, l'int\u00e9grale peut \u00eatre calcul\u00e9e analytiquement, \u00e0 partir de la primitive de \\(f\\) , not\u00e9e \\(F\\) : \\(I = \\int_{a}^{b} f(x) dx = F(b)-F(a)\\) Cependant, ce calcul peut \u00eatre long et compliqu\u00e9, et beaucoup de fonctions ne disposent pas d'expression analytique pour leurs primitives. On pr\u00e9f\u00e8rera faire appel \u00e0 des m\u00e9thodes num\u00e9rique pour calculer une valeur approch\u00e9e de \\(I\\) . L'approximation de \\(I\\) s'effectue le plus souvent \u00e0 l'aide de combinaisons lin\u00e9aires des valeurs de \\(f\\) : des formules de quadrature de type interpolation . L'int\u00e9grale \\(I\\) est remplac\u00e9e par une somme finie : \\(I \\approx \\sum_{i=0}^{n} w_i f(x_i)\\) . Exemple de probl\u00e8me Au cours de ce chapitre, nous appliquerons les diff\u00e9rentes m\u00e9thodes d'int\u00e9gration \u00e0 un m\u00eame exemple : La mod\u00e9lisation de la r\u00e9fl\u00e9ctivit\u00e9 radar des gouttes de pluies . En 1948, Marshall et Palmer ont propos\u00e9 un mod\u00e8le du facteur de r\u00e9flectivit\u00e9 des gouttes de pluies \\(Z\\) (en \\(mm^6 m^{-3}\\) ) pour les radars m\u00e9t\u00e9orologiques : \\(Z = \\int_{D_{min}}^{D_{max}} N_0 e^{- \\Lambda D} D^6 dD\\) avec \\(D\\) le diam\u00e8tre des gouttes suppos\u00e9es sph\u00e9riques. Les param\u00e8tres de ce mod\u00e8le sont : \\(D_{min}\\) la plus petite taille de goutte, que nous fixerons \u00e0 \\(1 mm\\) . \\(D_{max}\\) la plus grande taille de goutte, que nous fixerons \u00e0 \\(3 mm\\) . \\(\\Lambda\\) une constante empirique en \\(mm^{-1}\\) , pour laquelle Marshall et Palmer proposent \\(4.1 R^{-0.21}\\) , avec \\(R\\) le taux de pluie que nous fixerons \u00e0 \\(5 mm.h^{-1}\\) . \\(N_0\\) une constante empirique en \\(m^{-3} mm^{-1}\\) nomm\u00e9e \"param\u00e8tre de forme\", pour lequel Marshall et Palmer proposent \\(N_0 = 8000\\) . Ce mod\u00e8le est encore aujourd'hui utilis\u00e9 pour l'interpr\u00e9tation des mesures des radars m\u00e9t\u00e9orologiques, dans le but d'estimer les pr\u00e9cipitations aux sol \u00e0 partir des r\u00e9flectivit\u00e9s mesur\u00e9es. Afin d'estimer la r\u00e9flectivit\u00e9 \\(Z\\) li\u00e9e aux gouttes de pluie entre 1 et 3 cm, nous essayerons ici de calculer l'int\u00e9grale entre \\(x=1\\) et \\(x=3\\) de la fonction \\(f(x) = N_0 e^{- \\Lambda x} x^6\\) , dont la valeur est d'environ \\(2337.49 mm^6/m^3\\) . Sous Python on utilisera la biblioth\u00e8que Numpy : import numpy as np Puis, on d\u00e9finira les variables globales suivantes : N0 = 8000 #Param\u00e8tre de forme (m3/mm) R = 5 #Taux de pluie (mm/h) La fonction \\(f\\) sera d\u00e9finie comme : def f(D): return N0*np.exp(-(4.1*R**-0.21)*D)*D**6 Formules de quadrature Principe Id\u00e9e Approcher la fonction \\(f\\) par un polyn\u00f4me \\(p\\) . Si cette approximation est suffisamment bonne : \\(I = \\int_{a}^{b} f(x) dx \\approx \\int_{a}^{b} p(x) dx\\) Cette approche a 2 avantages : Les polyn\u00f4mes sont faciles \u00e0 int\u00e9grer. Cette m\u00e9thode est utilisable m\u00eame si on ne connait \\(f\\) qu'en un nombre fini de points \\((x_i,f(x_i))\\) . Les m\u00e9thodes de Newton-Cotes et de Gauss s'appuient sur cette id\u00e9e en utilisant des formules de quadrature de type interpolation , qui s'expriment comme une combinaison lin\u00e9aire de valeurs de la fonction en des points \u00e0 d\u00e9finir . On cherche donc une valeur approch\u00e9e de \\(I\\) au moyen d'une somme finie : \\(I = \\int_{a}^{b} f(x) dx \\approx I_n = \\sum_{i=0}^{n} w_i f(x_i)\\) On dit que \\(I_n\\) est une formule de quadrature de type interpolation \u00e0 \\(n+1\\) points Les valeurs \\(x_i\\) sont les \" pivots \" ou \"points / noeuds de quadrature\". Les coefficients \\(w_i\\) sont les \" poids \" de la formule de quadrature. Formules de type interpolation Soient \\((x_i,f(x_i))\\) avec \\(i=0,1,...,n+1\\) points d'interpolation. Un choix naturel consiste \u00e0 approximer la fonction \\(f\\) par le polyn\u00f4me de Lagrange de degr\u00e9 \\(\\leq n\\) qui passe par ces \\(n+1\\) points : \\(f(x) \\approx p(x) = \\sum_{i=0}^{n} f(x_i) L_i(x)\\) Il en r\u00e9sulte la formule de quadrature de type interpolation \u00e0 \\(n+1\\) points : \\(I = \\int_{a}^{b} f(x) dx \\approx I_n = \\int_{a}^{b} p(x) dx = \\int_{a}^{b} \\sum_{i=0}^{n} f(x_i) L_i(x) dx\\) Par lin\u00e9arit\u00e9 de l'int\u00e9grale, on obtient les coefficients de la formule de quadrature : \\(I_n = \\sum_{i=0}^{n} f(x_i) \\int_{a}^{b} L_i(x) dx\\) D'o\u00f9 : \\(w_i = \\int_{a}^{b} L_i(x) dx\\) Les poids \\(w_i\\) s'obtiennent par int\u00e9gration des polyn\u00f4mes de Lagrange . Ils sont donc ind\u00e9pendants de \\(f\\) et ne d\u00e9pendent que des points \\(x_i\\) . Degr\u00e9 de pr\u00e9cision On d\u00e9finit l' erreur de troncation pour l'int\u00e9gration comme : \\(E(f) = I-I_n\\) Une formule de quadrature est dite exacte pour \\(f\\) si \\(E(f)=0\\) . Degr\u00e9 de pr\u00e9cision Le degr\u00e9 de pr\u00e9cision d'une formule de quadrature est l'entier positif \\(n\\) tel que \\(E(p_i)=0\\) pour tout polyn\u00f4me de degr\u00e9 \\(i \\leq n\\) et \\(E(p_{n+1}) \\neq 0\\) pour un polyn\u00f4me \\(p_{n+1}\\) de degr\u00e9 \\(n+1\\) . Donc une formule de quadrature exacte sur l'ensemble des polyn\u00f4mes de degr\u00e9 \\(\\leq n\\) est au moins de degr\u00e9 de pr\u00e9cision \\(n\\) . Autrement dit, une formule de quadrature de degr\u00e9 de pr\u00e9cision \\(n\\) v\u00e9rifie : \\(I = \\int_{a}^{b} p_k(x) dx = \\int_{a}^{b} x^k dx = I_n = \\sum_{i=0}^{n} w_i x_i^k\\) pour tout \\(0 < k \\leq n\\) \\(I = \\int_{a}^{b} p_{n+1}(x) dx = \\int_{a}^{b} x^{n+1} dx \\neq I_n = \\sum_{i=0}^{n} w_i x_i^{n+1}\\) Th\u00e9or\u00e8me Une formule de quadrature \u00e0 \\(n+1\\) points est exacte sur l'ensemble des polyn\u00f4mes de degr\u00e9 \\(\\leq n\\) si est seulement si c'est une formule de quadrature de type interpolation \u00e0 \\(n+1\\) points. Donc une formule de quadrature de type interpolation \u00e0 \\(n+1\\) points est au moins de degr\u00e9 de pr\u00e9cision \\(n\\) . M\u00e9thodes de Newton-cotes simples Les m\u00e9thodes de Newton-Cotes s'appuient sur la formule de quadrature de type interpolation de Lagrange : \\(I = \\int_{a}^{b} f(x) dx = I_n + E(f) = \\sum_{i=0}^{n} f(x_i) \\int_{a}^{b} L_i(x) dx + E(f)\\) o\u00f9 \\(E(f)\\) est l'erreur de troncature. Les pivots de quadrature sont r\u00e9guli\u00e8rement espac\u00e9s : \\(x_i = x_0 + ih\\) avec \\(i=0,1,...,n\\) et \\(h = \\frac{b-a}{n}\\) Les pivots sont donc fixes (\u00e9quidistants) alors que les poids sont ajust\u00e9s . La r\u00e9gularit\u00e9 de la subdivision permet d'obtenir des formules qui sont tr\u00e8s g\u00e9n\u00e9rales. Il y a \\(n+1\\) pivots donc cette m\u00e9thode est exacte pour les polyn\u00f4mes de degr\u00e9 \\(\\leq n\\) au moins . M\u00e9thode des rectangles (n=0) Lorsque l'on ne dispose que d'un seul point \\((x_0,f(x_0))\\) , on peut utiliser la formule des rectangles : \\(I = \\int_{a}^{b} f(x) dx \\approx I_0 = \\int_{a}^{b} f(x_0) dx = (b-a) f(x_0)\\) On donnera diff\u00e9rents noms \u00e0 la m\u00e9thode suivant le choix de \\(x_0\\) : \"A gauche\" : si on choisit \\(x_0 = a\\) . \"A droite\" : si on choisit \\(x_0 = b\\) . \"Au point milieu\" : si on choisit \\(x_0 = \\frac{a+b}{2}\\) On remarque que \\(I_0\\) est l' aire du rectangle de largeur \\(b-a\\) et de longueur \\(f(x_0)\\) . Voici pour illustration l'aire de ce rectangle dans le cas de notre exemple : Les formules des rectangles \u00e0 droite et \u00e0 gauche sont exactes pour des polyn\u00f4mes de degr\u00e9 0 uniquement : leur degr\u00e9 de pr\u00e9cision est de 0 . La formule au point milieu est aussi exacte pour les fonctions affines car elle exploite les sym\u00e9tries du probl\u00e8me : son degr\u00e9 de pr\u00e9cision est de 1 . Si \\(f\\) est continue et 2 fois d\u00e9rivable sur \\([a,b]\\) , alors il existe \\(\\xi \\in ]a,b[\\) tel que \\(I = I_0 + E(f)\\) avec : \\(\\begin{cases} E(f) = \\frac{(b-a)^2}{2} f'(\\xi) = \\frac{h^2}{2} f'(\\xi) \\; si \\; x_0 = a \\; ou \\; x_0 = b \\\\ E(f) = \\frac{(b-a)^3}{24} f\"(\\xi) = \\frac{h^3}{24} f\"(\\xi) \\; si \\; x_0 = \\frac{a+b}{2} \\end{cases}\\) On en d\u00e9duit que : Donc plus \\([a,b]\\) est petit, plus l'erreur est faible. Pour les formules \u00e0 droite et \u00e0 gauche, l'erreur d\u00e9croit en \\(h^2\\) . Pour la formule au point milieu, l'erreur d\u00e9croit en \\(h^3\\) . Plus les variations de \\(f\\) sont limit\u00e9es ( \\(f'\\) faible), plus l'erreur est faible pour les m\u00e9thodes \u00e0 droite et \u00e0 gauche. M\u00e9thode des trap\u00e8zes (n=1) Lorsque l'on dispose que de 2 points \\((x_0,f(x_0))\\) et \\((x_1,f(x_1))\\) , on peut utiliser la formule des trap\u00e8zes : \\(I = \\int_{a}^{b} f(x) dx \\approx I_1 = \\frac{(b-a)}{2} (f(a)+f(b))\\) \\(I_1\\) est une formule de quadrature de type interpolation \u00e0 2 points. On remarque qu'elle correspond \u00e0 l' aire d'un trap\u00e8ze . Voici pour illustration l'aire de ce trap\u00e8ze dans le cas de notre exemple : Si \\(f\\) est continue et 2 fois d\u00e9rivable sur \\([a,b]\\) , alors il existe \\(\\xi \\in ]a,b[\\) tel que \\(I = I_1 + E(f)\\) avec : \\(E(f) = - \\frac{h^3}{12} f\"(\\xi)\\) La m\u00e9thode des trap\u00e8zes est exacte sur l'espace des polyn\u00f4mes de degr\u00e9 \\(\\leq 1\\) donc de degr\u00e9 de pr\u00e9cision 1 . Par contre, elle est 2X plus lente que la m\u00e9thode des rectangles au point milieu pour le m\u00eame degr\u00e9 de pr\u00e9cision. M\u00e9thode de Simpson (n=2) Lorsque l'on dispose que de 3 points \\((x_0,f(x_0))\\) , \\((x_1,f(x_1))\\) et \\((x_2,f(x_2))\\) , on peut utiliser la formule de Simpson . Si on choisit \\(x_0 = a\\) , \\(x_1 = \\frac{a+b}{2}\\) et \\(x_2 = b\\) : \\(I = \\int_{a}^{b} f(x) dx \\approx I_2 = \\frac{(b-a)}{6} (f(a)+4 f(\\frac{a+b}{2})+f(b))\\) \\(I_2\\) est une formule de quadrature de type interpolation \u00e0 3 points. On remarque qu'elle correspond \u00e0 l' aire sous un morceau de parabole . Voici pour illustration l'aire sous la parabole dans le cas de notre exemple : Si \\(f\\) est continue et 4 fois d\u00e9rivable sur \\([a,b]\\) , alors il existe \\(\\xi \\in ]a,b[\\) tel que \\(I = I_2 + E(f)\\) avec : \\(E(f) = - \\frac{h^5}{90} f^{(4)}(\\xi)\\) La m\u00e9thode de Simpson est exacte sur l'espace des polyn\u00f4mes de degr\u00e9 \\(\\leq 2\\) , et on peut m\u00eame montrer qu'elle est de degr\u00e9 de pr\u00e9cision 3 . Elle n\u00e9cessite l'\u00e9valuation de \\(f\\) en 3 points, mais l'erreur diminue rapidement, en \\(h^5\\) . M\u00e9thodes de Newton-Cotes (n>0) On peut g\u00e9n\u00e9raliser les formule de quadrature de type interpolation de Lagrange \u00e0 tout degr\u00e9 \\(n>0\\) . On parle alors de m\u00e9thodes de Newton-Cotes de mani\u00e8re g\u00e9n\u00e9rale. On distingue 2 formules suivant si \\(n\\) est pair ou impair : Cas de \\(n\\) impair Si \\(f\\) est d\u00e9rivable \\(n+1\\) fois sur \\([a,b]\\) , alors il existe un r\u00e9el \\(K\\) et \\(\\xi \\in ]a,b[\\) tel que \\(E(f) = \\frac{K}{(n+1)!} (b-a)^{n+2} f^{(n+1)}(\\xi)\\) Le degr\u00e9 de pr\u00e9cision est \\(n\\) . Cas de \\(n\\) pair Si \\(f\\) est d\u00e9rivable \\(n+2\\) fois sur \\([a,b]\\) , alors il existe un r\u00e9el \\(K\\) et \\(\\xi \\in ]a,b[\\) tel que \\(E(f) = \\frac{K}{(n+2)!} (b-a)^{n+3} f^{(n+2)}(\\xi)\\) Le degr\u00e9 de pr\u00e9cision est \\(n+1\\) . On peut en conclure qu'une formule centr\u00e9e ( \\(n\\) pair, donc nombre de points impair) est pr\u00e9f\u00e9rable. Voici les formules des m\u00e9thodes de Newton-Cotes jusqu'\u00e0 \\(n=6\\) : Degr\u00e9 \\(n\\) Nom Formule 1 M\u00e9thode des trap\u00e8zes \\(\\frac{b-a}{2}(f(x_0)+f(x_1))\\) 2 M\u00e9thode de Simpson 1/3 \\(\\frac{b-a}{6}(f(x_0)+4f(x_1)+f(x_2))\\) 3 M\u00e9thode de Simpson 3/8 \\(\\frac{b-a}{8}(f(x_0)+3f(x_1)+3f(x_2)+f(x_3))\\) 4 M\u00e9thode de Boole-Villarceau \\(\\frac{b-a}{90}(7f(x_0)+32f(x_1)+12f(x_2)+32f(x_3)+7f(x_4))\\) 6 M\u00e9thode de Weddle-Hardy \\(\\frac{b-a}{840}(41f(x_0)+216f(x_1)+27f(x_2)+272f(x_3)+27f(x_4)+216f(x_5)+41f(x_6))\\) En pratique, les m\u00e9thodes de Newton-Cotes ne sont presque jamais utilis\u00e9es pour \\(n>6\\) . Algorithmes Voici les diff\u00e9rentes m\u00e9thodes de Newton-Cotes pr\u00e9sent\u00e9es pr\u00e9c\u00e9demment sous la forme de fonctions Python. Elles prennent toutes en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. La fonction pour la m\u00e9thode des rectangles \u00e0 gauche : def rectangles_gauche(f,a,b): return (b-a)*f(a) La fonction pour la m\u00e9thode des rectangles \u00e0 droite : def rectangles_droite(f,a,b): return (b-a)*f(b) La fonction pour la m\u00e9thode des rectangles au point milieu : def rectangles_milieu(f,a,b): return (b-a)*f((a+b)/2) La fonction pour la m\u00e9thode des trap\u00e8zes : def trapezes(f,a,b): return (b-a)*(f(a)+f(b))/2 La fonction pour la m\u00e9thode de Simpson : def simpson(f,a,b): return (b-a)*(f(a)+4*f((a+b)/2)+f(b))/6 Exemple En appliquant les algorithmes pr\u00e9c\u00e9dents \u00e0 notre probl\u00e8me exemple, on trouve les valeurs suivantes : M\u00e9thode Estimation de Z ( \\(mm^6 m^{-3}\\) ) Rectangles \u00e0 gauche 859.36 Rectangles \u00e0 droite 1807.24 Rectangles au point milieu 2954.01 Trap\u00e8zes 1333.30 Simpson 2413.78 Ces estimations sont \u00e0 comparer \u00e0 la valeur th\u00e9orique \\(Z = 2337.49 mm^6/m^3\\) . Notre fonction \u00e0 int\u00e9grer n'\u00e9tant pas un polyn\u00f4me de degr\u00e9 \\(\\leq n\\) , on ne s'attendait pas \u00e0 avoir un r\u00e9sultat exact. On note toutefois que plus le degr\u00e9 de pr\u00e9cision augmente, meilleure est l'estimation. Exercice : D\u00e9terminez la formule de Newton-Cotes pour \\(n=4\\) (aussi appell\u00e9e \"m\u00e9thode de Boole-Villarceau\"), et impl\u00e9mentez-l\u00e0 sous la forme d'une fonction Python. Quelle estimation de \\(Z\\) obtenez-vous ? Cette valeur est-elle plus proche du r\u00e9sultat attendu ? Ce r\u00e9sultat \u00e9tait-il attendu ? M\u00e9thode de Newton-Cotes composites Pour am\u00e9liorer la pr\u00e9cision des m\u00e9thode de Newton-Cotes, on pourrai augmenter le nombre de point d'int\u00e9gration, et donc le degr\u00e9 du polyn\u00f4me d'interpolation. Mais cela conduit \u00e0 des formules de plus en plus complexes, et m\u00eame potentiellement \u00e0 un ph\u00e9nom\u00e8ne de Runge (voir le Chapitre 3). C'est pourquoi on utilise en g\u00e9n\u00e9ral des m\u00e9thodes composites avec des formules de Newton-Cotes de degr\u00e9 \\(n<6\\) . L'id\u00e9e des formules composites est la suivante : D\u00e9couper l'intervalle \\([a,b]\\) en \\(M\\) sous-intervalles. Appliquer une formule de Newton-Cotes sur chaque sous-intervalle pour estimer l'aire sous la courbe. Additionner les aires estim\u00e9es pour chaque sous-intervalle. Ceci revient \u00e0 l'expression suivante de l'int\u00e9grale : \\(I = \\int_{a}^{b} f(x) dx = \\displaystyle\\sum_{j=0}^{M} \\int_{x_{j-1}=a+(j-1)h}^{x_j=a+jh} f(x) dx\\) Dans ce cas, les points de subdivision sont r\u00e9guli\u00e8rement espac\u00e9s d'un pas \\(h=\\frac{b-a}{M}\\) . Une formule de quadrature de type interpolation composite s'\u00e9crit donc : \\(I_{n,M} = \\displaystyle\\sum_{j=1}^{M} \\displaystyle\\sum_{i=0}^{n} w_i^{(j)} f(x_i^{(j)})\\) Formule composite des rectangles Soit \\(M+1\\) points de discr\u00e9tisation \\((x_j,f(x_j))\\) avec \\(x_j=a+jh\\) et \\(h=\\frac{b-a}{M}\\) . Rectangles \u00e0 gauche : Sur chaque sous-intervalle \\([x_j,x_{j+1}]\\) : \\(\\int_{x_{j+1}}^{x_j} f(x) dx \\approx hf(x_{j})\\) D'o\u00f9 la formule composite des rectangles \u00e0 gauche : \\(\\int_{a}^{b} f(x) dx \\approx h(\\displaystyle\\sum_{j=0}^{M-1} f(x_{j}))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Rectangles \u00e0 droite : Sur chaque sous-intervalle \\([x_j,x_{j+1}]\\) : \\(\\int_{x_{j+1}}^{x_j} f(x) dx \\approx hf(x_{j+1})\\) D'o\u00f9 la formule composite des rectangles \u00e0 droite : \\(\\int_{a}^{b} f(x) dx \\approx h(\\displaystyle\\sum_{j=0}^{M-1} f(x_{j+1}))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Rectangles au point milieu : Sur chaque sous-intervalle \\([x_j,x_{j+1}]\\) : \\(\\int_{x_{j+1}}^{x_j} f(x) dx \\approx hf(\\frac{x_{j}+x_{j+1}}{2})\\) D'o\u00f9 la formule composite des rectangles au point milieu : \\(\\int_{a}^{b} f(x) dx \\approx h(\\displaystyle\\sum_{j=0}^{M-1} f(\\frac{x_{j}+x_{j+1}}{2}))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Ces formules n\u00e9cessitent \\(M\\) \u00e9valuations de \\(f\\) . On peut alors montrer que dans le cas du point milieu, l'erreur est major\u00e9e ainsi : \\(\\mid E(f) \\mid \\leq \\frac{b-a}{24} h^2 max_{x \\in [a,b]} \\mid f\"(x) \\mid\\) On observe que \\(\\lim\\limits_{M \\rightarrow \\infty} E(f) = \\lim\\limits_{h \\rightarrow 0} E(f) = 0\\) . On en d\u00e9duit que la m\u00e9thode converge bien vers la valeur exacte de l'int\u00e9grale. L'ordre de convergence est de 2 : l'erreur est divis\u00e9e par 4 lorsque h est divis\u00e9 par 2 . Formule composite des trap\u00e8zes Sur chaque sous-intervalle \\([x_j,x_{j+1}]\\) : \\(\\int_{x_{j+1}}^{x_j} f(x) dx \\approx \\frac{h}{2} (f(x_{j})+f(x_{j+1}))\\) D'o\u00f9 la formule composite des trap\u00e8zes : \\(\\int_{a}^{b} f(x) dx \\approx h (\\frac{1}{2} f(a) + \\displaystyle\\sum_{j=1}^{M-1} f(x_j) + \\frac{1}{2} f(b))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Cette formule n\u00e9cessite \\(M+1\\) \u00e9valuations de \\(f\\) . On peut montrer que l'erreur est major\u00e9e ainsi : \\(\\mid E(f) \\mid \\leq \\frac{b-a}{12} h^2 max_{x \\in [a,b]} \\mid f\"(x) \\mid\\) On observe que \\(\\lim\\limits_{M \\rightarrow \\infty} E(f) = \\lim\\limits_{h \\rightarrow 0} E(f) = 0\\) . On en d\u00e9duit que la m\u00e9thode converge bien vers la valeur exacte de l'int\u00e9grale. L'ordre de convergence est de 2 : l'erreur est divis\u00e9e par 4 lorsque h est divis\u00e9 par 2 . Formule composite de Simpson Pour faciliter l'\u00e9criture, on d\u00e9finira pour la m\u00e9thode de Simpson composite \\(h = \\frac{b-a}{2M}\\) pour un nombre de sous-intervalles \\(M\\) . Sur chaque sous-intervalle \\([x_j,x_{j+2}]\\) : \\(\\int_{x_j}^{x_{j+2}} f(x) dx \\approx \\frac{h}{3} (f(x_{j})+4f(x_{j+1})+f(x_{j+2}))\\) D'o\u00f9 la formule composite de Simpson : \\(\\int_{a}^{b} f(x) dx \\approx \\frac{h}{3} (f(a) + 2 \\displaystyle\\sum_{j=1}^{M-1} f(x_{2j}) + 4 \\displaystyle\\sum_{j=0}^{M-1} f(x_{2j+1}) + f(b))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Cette formule n\u00e9cessite \\(2M+1\\) \u00e9valuations de \\(f\\) . On peut montrer que l'erreur est major\u00e9e ainsi : \\(\\mid E(f) \\mid \\leq \\frac{b-a}{90} h^4 max_{x \\in [a,b]} \\mid f^{(4)}(x) \\mid\\) On observe que \\(\\lim\\limits_{M \\rightarrow \\infty} E(f) = \\lim\\limits_{h \\rightarrow 0} E(f) = 0\\) . On en d\u00e9duit que la m\u00e9thode converge bien vers la valeur exacte de l'int\u00e9grale. L'ordre de convergence est de 4 : l'erreur est divis\u00e9e par 16 lorsque h est divis\u00e9 par 2 . Algorithmes Voici une fonction Python pour calculer une int\u00e9grale \u00e0 partir des m\u00e9thodes simples programm\u00e9es pr\u00e9c\u00e9demment. Elle prend en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. methode la m\u00e9thode d'int\u00e9gration (rectangles, trap\u00e8zes ou Simpson) sous la forme d'une fonction Python. M le nombre de sous-intervalles d'int\u00e9gration. def methode_composite(f,a,b,methode,M): #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de #(b-a)/M : x_i = [a+i*(b-a)/M for i in range(M+1)] #Initialisation de la somme des aires sous la courbe des diff\u00e9rents #sous-intervalles : aire = 0 #Boucle sur les sous-intervalles : for i in range(M): #Addition au compteur de l'aire sous la courbe pour ce sous-intervalle : aire += methode(f,x_i[i],x_i[i+1]) #Renvoyer l'estimation de l'aire sous la courbe pour l'intervalle [a,b] : return aire On peut appliquer la m\u00e9thode de Simpson composite \\(M=5\\) \u00e0 notre exemple avec la commande Python : I_simpson_composite = methode_composite(f,1,3,simpson,5) Si cette fonction est \u00e9l\u00e9gante, car elle est utilisable pour toutes les m\u00e9thodes programm\u00e9es pr\u00e9c\u00e9demment, il est \u00e0 noter qu'elle n'est pas optimis\u00e9e pour la m\u00e9thode des trap\u00e8zes et la m\u00e9thode de Simpson. En effet, avec cette impl\u00e9mentation, on \u00e9value plusieurs fois la fonction aux m\u00eames points. Pour une impl\u00e9mentation plus optimis\u00e9e, il vaut mieux programmer une fonction pour chaque m\u00e9thode composite, en se basant sur les formules donn\u00e9es pr\u00e9c\u00e9demment. Toutes les fonctions suivantes prennent toutes en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. M le nombre de sous-intervalles d'int\u00e9gration. Voici la fonction pour la m\u00e9thode composite des rectangles \u00e0 gauche : def rectangles_gauche_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/M #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(M+1)] #Initialiser la somme des \u00e9valuations de f pour chaque sous-intervalle: somme = 0 #Boucle sur les sous-intervalles : for i in range(M): #Sommer la valeur de f \"\u00e0 gauche\" du sous-intervalle : somme += f(x_i[i]) #Calcul de la somme des aires des sous-intervalles : aire = somme*h return aire Voici la fonction pour la m\u00e9thode composite des rectangles \u00e0 droite : def rectangles_droite_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/M #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(M+1)] #Initialiser la somme des \u00e9valuations de f pour chaque sous-intervalle: somme = 0 #Boucle sur les sous-intervalles : for i in range(M): #Sommer la valeur de f \"\u00e0 droite\" du sous-intervalle : somme += f(x_i[i+1]) #Calcul de la somme des aires des sous-intervalles : aire = somme*h return aire Voici la fonction pour la m\u00e9thode composite des rectangles au point milieu : def rectangles_milieu_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/M #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(M+1)] #Initialiser la somme des \u00e9valuations de f pour chaque sous-intervalle: somme = 0 #Boucle sur les sous-intervalles : for i in range(M): #Sommer la valeur de f \"au milieu\" du sous-intervalle : somme += f((x_i[i]+x_i[i+1])/2) #Calcul de la somme des aires des sous-intervalles : aire = somme*h return aire Voici la fonction pour la m\u00e9thode composite des trap\u00e8zes : def trapezes_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/M #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(M+1)] #Initialiser la somme des \u00e9valuations de f pour chaque sous-intervalle avec #f(a) et f(b): somme = (f(x_i[0])+f(x_i[-1]))/2 #Boucle sur les sous-intervalles : for i in range(1,M): #Sommer la valeur de f \u00e0 gauche du sous-intervalle : somme += f(x_i[i]) #Calcul de la somme des aires des sous-intervalles : aire = somme*h return aire Voici la fonction pour la m\u00e9thode composite de Simpson : def simpson_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/(2*M) #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(2*M+1)] #Initialiser la somme des \u00e9valuations de f pour les \u00e9l\u00e9ments pairs et #impairs de l'intervalle : somme_pair = 0 somme_impair = 0 #1\u00e8re boucle sur les \u00e9l\u00e9ments pairs des sous-intervalles : for i in range(1,M): #Sommer la valeur de f du sous-intervalle : somme_pair += f(x_i[2*i]) #2nde boucle sur les \u00e9l\u00e9ments impairs des sous-intervalles : for i in range(M): somme_impair += f(x_i[2*i+1]) #Additionner les valeurs de f : somme = f(x_i[0])+f(x_i[-1])+2*somme_pair+4*somme_impair #Calcul de la somme des aires des sous-intervalles : aire = somme*h/3 return aire Exemples En appliquant les algorithmes pr\u00e9c\u00e9dents \u00e0 notre probl\u00e8me exemple, avec un nombre de sous-intervalles \\(M = 5\\) on trouve les valeurs suivantes : M\u00e9thode composite (M=5) Estimation de Z ( \\(mm^6 m^{-3}\\) ) Rectangles \u00e0 gauche 2213.67 Rectangles \u00e0 droite 2403.25 Rectangles au point milieu 2352.11 Trap\u00e8zes 2308.46 Simpson 2337.56 Ces estimations sont \u00e0 comparer \u00e0 la valeur th\u00e9orique \\(Z = 2337.49 mm^6/m^3\\) . Exercice : Utilisez les fonctions Python pr\u00e9c\u00e9dentes pour calculer l'erreur absolue commise par les m\u00e9thodes composite des trap\u00e8zes et de Simpson. Calculez l'erreur pour M = 1, 2, 4, 8 et 16. Qu'observez-vous ? Acc\u00e9l\u00e9ration de Romberg Concernant les m\u00e9thodes composites, il reste la question suivante : Comment choisir un pas d'int\u00e9gration \\(h\\) pour obtenir la pr\u00e9cision souhait\u00e9e \\(\\epsilon\\) ? L' acc\u00e9l\u00e9ration de Romberg est une m\u00e9thode it\u00e9rative qui permet d'am\u00e9liorer l'ordre de convergence de la m\u00e9thode des trap\u00e8zes ou de Simpson. Elle tire partie du fait que : Pour la m\u00e9thode des trap\u00e8zes : \\(I = I_h + E_h(f) = I_{2h} + E_{2h}(f)\\) avec \\(E_{2h}(f) = 4 E_h(f)\\) d'o\u00f9 \\(\\mid I_{2h} - I_h \\mid = 3 E_h(f)\\) et donc : \\(E_h(f) = \\frac{\\mid I_{2h} - I_h \\mid}{3}\\) Pour la m\u00e9thode de Simpson : \\(I = I_h + E_h(f) = I_{2h} + E_{2h}(f)\\) avec \\(E_{2h}(f) = 16 E_h(f)\\) d'o\u00f9 \\(\\mid I_{2h} - I_h \\mid = 15 E_h(f)\\) et donc : \\(E_h(f) = \\frac{\\mid I_{2h} - I_h \\mid}{15}\\) On d\u00e9termine alors le pas d'int\u00e9gration \\(h\\) permettant d'obtenir \\(\\epsilon\\) avec l'algorithme suivant : Algorithme d'acc\u00e9l\u00e9ration de Romberg On se donne un d\u00e9coupage initial \\(2h\\) On calcule \\(I_{2h}\\) On double le nombre d'intervalles : on prend un pas de \\(h\\) On calcule \\(I_h\\) Pour la m\u00e9thode des trap\u00e8zes : tant que \\(\\mid I_{2h} - I_h \\mid > 3 \\epsilon\\) on r\u00e9p\u00e8te les op\u00e9rations pr\u00e9c\u00e9dentes Pour la m\u00e9thode de Simpson : tant que \\(\\mid I_{2h} - I_h \\mid > 15 \\epsilon\\) on r\u00e9p\u00e8te les op\u00e9rations pr\u00e9c\u00e9dentes Voici une fonction Python impl\u00e9mentant l'acc\u00e9l\u00e9ration de Romberg pour la m\u00e9thode des trap\u00e8zes composite. Elle prend en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. epsilon la pr\u00e9cision souhait\u00e9e sur la valeur de l'int\u00e9grale. def romberg_trapezes(f,a,b,epsilon): #Initialiser le nombre de sous-intervalles d'int\u00e9gration M : M = 1 #Initialiser les aires estim\u00e9es pour M et 2M sous-intervalles d'int\u00e9gration : aire_M = trapezes_composite(f,a,b,M) aire_2M = trapezes_composite(f,a,b,2*M) #Boucler tant que la condition sur l'erreur d'int\u00e9gration n'est pas atteinte : while abs(aire_M-aire_2M)>(3*epsilon): #Multiplier le nombre de sous-intervalles d'int\u00e9gration par 2 : M *= 2 #Estimer l'aire pour M et 2M sous-intervalles d'int\u00e9gration : aire_M = trapezes_composite(f,a,b,M) aire_2M = trapezes_composite(f,a,b,2*M) return aire_2M Voici une application de l'acc\u00e9l\u00e9ration de Romberg \u00e0 notre probl\u00e8me, avec la m\u00e9thode des trap\u00e8zes composite, en demandant une pr\u00e9cision de \\(1 mm^6/m^3\\) : Pour obtenir une pr\u00e9cision de \\(10^{-3}\\) sur la valeur de \\(Z = 2337.49 mm^6/m^3\\) avec la m\u00e9thode des trap\u00e8zes composite, on trouve par la m\u00e9thode de Romberg qu'il faut \\(M=1024\\) . M\u00e9thodes de Gauss Principe Les m\u00e9thodes de Newton-Cotes fixent les pivots de quadrature et utilisent des poids assurant un degr\u00e9 de pr\u00e9cision de \\(n\\) ou \\(n+1\\) . L'id\u00e9e des m\u00e9thodes de Gauss est de choisir les pivots pour que le degr\u00e9 de pr\u00e9cision de la formule soit le plus \u00e9lev\u00e9 possible. Soit la formule de quadrature \u00e0 \\((n+1)\\) points : \\(I = \\int_{a}^{b} f(x) dx \\approx I_n = \\displaystyle\\sum_{i=0}^{n} w_i f(x_i)\\) Les \\((n+1)\\) pivots et les \\((n+1)\\) poids, donc les \\(2(n+1)\\) param\u00e8tres, sont ajust\u00e9s pour optimiser la pr\u00e9cision de la m\u00e9thode. Pour que la m\u00e9thode soit exacte pour les polyn\u00f4mes de degr\u00e9 \\(\\leq 2n+1\\) on cherche donc les \\(2(n+1)\\) param\u00e8tres tels que : \\(\\int_{a}^{b} x^k dx = \\displaystyle\\sum_{i=0}^{n} w_i x_i^k\\) avec \\(k=0,1,...,2n+1\\) M\u00e9thode \u00e0 1 point (n=0) A l'ordre 1 ( \\(n=0\\) ), on a 2 inconnues \u00e0 ajuster : le point \\(x_0\\) et le coefficient \\(w_0\\) . Soit \\(I_0 = w_0 f(x_0)\\) On va ajuster ces param\u00e8tres pour que la m\u00e9thode soit exacte dans l'ensemble des polyn\u00f4mes de degr\u00e9 \\(\\leq 1\\) . On r\u00e9sout donc le syst\u00e8me de 2 \u00e9quations \u00e0 2 inconnues : \\(\\begin{cases} \\int_{a}^{b} 1 dx = w_0\\\\ \\int_{a}^{b} x dx = w_0 x_0\\\\ \\end{cases}\\) On obtient la formule optimale de Gauss : \\(I_0 = (b-a) f(\\frac{a+b}{2})\\) On retrouve alors la m\u00e9thode du point milieu de Newton-Cotes. M\u00e9thode \u00e0 2 points (n=1) A l'ordre 2 ( \\(n=1\\) ), on a 4 inconnues \u00e0 ajuster : les 2 points \\(x_0\\) et \\(x_1\\) , et les 2 coefficients \\(w_0\\) et \\(w_1\\) . Soit \\(I_1 = w_0 f(x_0) + w_1 f(x_1)\\) On va ajuster ces param\u00e8tres pour que la m\u00e9thode soit exacte dans l'ensemble des polyn\u00f4mes de degr\u00e9 \\(\\leq 3\\) (\u00e0 comparer au degr\u00e9 \\(\\leq 1\\) de la m\u00e9thode des trap\u00e8zes). On r\u00e9sout donc le syst\u00e8me de 4 \u00e9quations \u00e0 4 inconnues : \\(\\begin{cases} \\int_{a}^{b} 1 dx = w_0 + w_1\\\\ \\int_{a}^{b} x dx = w_0 x_0 + w_1 x_1\\\\ \\int_{a}^{b} x^2 dx = w_0 x_0^2 + w_1 x_1^2\\\\ \\int_{a}^{b} x^3 dx = w_0 x_0^3 + w_1 x_1^3 \\end{cases}\\) On obtient la formule optimale de Gauss : \\(I_1 = \\frac{b-a}{2} (f(\\frac{-1}{\\sqrt{3}} \\frac{b-a}{2} + \\frac{a+b}{2})+f(\\frac{1}{\\sqrt{3}} \\frac{b-a}{2} + \\frac{a+b}{2}))\\) G\u00e9n\u00e9ralisation (n quelconque) On peut g\u00e9n\u00e9raliser les formules trouv\u00e9es pr\u00e9c\u00e9demment pour un nombre de points quelconque. Les points et les poids sont d\u00e9termin\u00e9s pour l'int\u00e9grale d'un polyn\u00f4me entre -1 et 1, puis par un changement de variable \\(x_i = \\frac{b-a}{2} y_i + \\frac{a+b}{2}\\) et \\(w_i = \\frac{b-a}{2} v_i\\) on obtient la formule g\u00e9n\u00e9rale suivante : \\(I_n = \\frac{b-a}{2} \\displaystyle\\sum_{i=1}^{n} v_i f(\\frac{b-a}{2} y_i + \\frac{a+b}{2})\\) Voici les points et les poids \\(y_i\\) d\u00e9termin\u00e9s pour des ordres de 0 \u00e0 3 : Ordre \\(n\\) Nombre de points Points \\(y_i\\) Poids \\(v_i\\) 0 1 0 2 1 2 \\(-\\sqrt{\\frac{1}{3}}\\) et \\(\\sqrt{\\frac{1}{3}}\\) 1 et 1 2 3 0, \\(-\\sqrt{\\frac{3}{5}}\\) et \\(\\sqrt{\\frac{3}{5}}\\) \\(\\frac{8}{9}\\) , \\(\\frac{5}{9}\\) et \\(\\frac{5}{9}\\) 3 4 \\(\\sqrt{\\frac{3}{7}-\\frac{2}{7}\\sqrt{\\frac{6}{5}}}\\) , \\(-\\sqrt{\\frac{3}{7}-\\frac{2}{7}\\sqrt{\\frac{6}{5}}}\\) , \\(\\sqrt{\\frac{3}{7}+\\frac{2}{7}\\sqrt{\\frac{6}{5}}}\\) et \\(-\\sqrt{\\frac{3}{7}+\\frac{2}{7}\\sqrt{\\frac{6}{5}}}\\) \\(\\frac{18+\\sqrt{30}}{36}\\) , \\(\\frac{18+\\sqrt{30}}{36}\\) , \\(\\frac{18-\\sqrt{30}}{36}\\) et \\(\\frac{18-\\sqrt{30}}{36}\\) NB : Pour les curieux, les points \\(y_i\\) et les poids \\(v_i\\) sont d\u00e9termin\u00e9s gr\u00e2ce \u00e0 ce qu'on appelle les \"polyn\u00f4mes de Legendre\". Les m\u00e9thodes de Gauss sont plus performantes que les m\u00e9thodes de Newton-Cotes en termes de pr\u00e9cision pour un m\u00eame temps de calcul : l'erreur d\u00e9croit exponentiellement avec \\(n\\) . Toutefois, la th\u00e9orie derri\u00e8re ces m\u00e9thodes est plus difficile , et leur programmation est plus lourde si l'on doit d\u00e9terminer les points / poids soi-m\u00eame. Algorithmes Voici une fonction Python impl\u00e9mentant la m\u00e9thode de Gauss pour des points / poids donn\u00e9s. Elle prend en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. y_i la liste des points d\u00e9termin\u00e9s pour l'ordre de la m\u00e9thode choisie. v_i la liste des poids d\u00e9termin\u00e9s pour l'ordre de la m\u00e9thode choisie. def gauss(f,a,b,y_i,v_i): #Initialiser la somme de la formule de quadrature : somme = 0 #Boucles sur les points / poids : for i in range(len(y_i)): #Sommer l'\u00e9valuation de f au point choisi, pond\u00e9r\u00e9e par le poids choisi : somme += v_i[i]*f(y_i[i]*(b-a)/2+((a+b)/2)) #Calculer l'estimation de l'aire sous la courbe : aire = somme*(b-a)/2 return aire On peut appliquer la m\u00e9thode de Gauss d'ordre 2 \u00e0 notre exemple avec la commande Python : I_gauss_ord2 = gauss(f,1,3,[0,-(3/5)**0.5,(3/5)**0.5],[8/9,5/9,5/9]) Exemples En appliquant l'algorithme pr\u00e9c\u00e9dent pour diff\u00e9rents ordres \u00e0 notre probl\u00e8me exemple, on trouve les valeurs suivantes : M\u00e9thode de Gauss d'ordre Estimation de Z ( \\(mm^6 m^{-3}\\) ) \\(n=0\\) 2954.01 \\(n=1\\) 2285.50 \\(n=2\\) 2338.36 \\(n=3\\) 2337.64 Ces estimations sont \u00e0 comparer \u00e0 la valeur th\u00e9orique \\(Z = 2337.49 mm^6/m^3\\) . Notre fonction \u00e0 int\u00e9grer n'\u00e9tant pas un polyn\u00f4me de degr\u00e9 \\(\\leq 2n+1\\) , on ne s'attendait pas \u00e0 avoir un r\u00e9sultat exact. N\u00e9anmoins, on peut observer comme attendu que le r\u00e9sultat converge beaucoup plus rapidement avec \\(n\\) que les m\u00e9thodes de Newton-Cotes. Exercice : Avec la fonction Python pr\u00e9c\u00e9dente, calculez l'erreur absolue commise par les m\u00e9thodes de Gauss pour \\(n\\) allant de 0 \u00e0 3. Tracez ces point sur un graphique. Comment l'erreur semble diminuer avec \\(n\\) ? Ce r\u00e9sultat est-il attendu ? Conclusions On appelle formule de quadrature une formule permettant d'approcher l'int\u00e9grale d'une fonction \\(f\\) sur un intervalle. De type interpolation , elle s'exprime comme une combinaison lin\u00e9aire de valeurs de \\(f\\) (pivots) et de poids. Le degr\u00e9 de pr\u00e9cision d'une formule de quadrature est le degr\u00e9 maximal des polyn\u00f4mes pouvant \u00eatre int\u00e9gr\u00e9s exactement. Les m\u00e9thodes de Newton-Cotes s'appuient sur des formules de quadrature de type interpolation de Lagrange . Les plus connues sont les m\u00e9thodes du point milieu , des trap\u00e8zes et de Simpson . Les pivots sont \u00e9quidistants et les poids ajust\u00e9s. Elles sont exactes pour les polyn\u00f4mes de degr\u00e9 \\(\\leq n+1\\) . Les formules de Newton-Cotes ne sont pas utilis\u00e9es au-del\u00e0 de l'ordre 6. On utilise leur version composite . La pr\u00e9cision d\u00e9pend alors de la taille des sous-intervalles. Les m\u00e9thodes de Gauss ajustent les pivots et les poids. Elles sont exactes pour les polyn\u00f4mes de degr\u00e9 \\(\\leq 2n+1\\) , et on les dit donc optimales . Leur th\u00e9orie / programmation est cependant plus lourde. NB : Pour les curieux, il existe aussi des m\u00e9thodes d'int\u00e9gration num\u00e9rique qui ne s'appuient pas sur des formules de quadrature. Par exemple, la m\u00e9thode de Monte-Carlo.","title":"IV. Int\u00e9gration num\u00e9rique"},{"location":"Chap4_Integration_numerique/#chapitre-iv-integration-numerique","text":"Ce chapitre porte sur les m\u00e9thodes num\u00e9riques pour l'approximation de l'int\u00e9grale d'une fonction.","title":"Chapitre IV : Int\u00e9gration num\u00e9rique"},{"location":"Chap4_Integration_numerique/#position-du-probleme","text":"","title":"Position du probl\u00e8me"},{"location":"Chap4_Integration_numerique/#motivation","text":"Le but de l'int\u00e9gration num\u00e9rique est d'\u00e9valuer la valeur de l' int\u00e9grale \\(I\\) d'une fonction \\(f\\) continue sur un intervalle \\([a,b]\\) avec \\(a<b\\) r\u00e9els : \\(I = \\int_{a}^{b} f(x) dx\\) On rappelle que cette int\u00e9grale repr\u00e9sente l' aire comprise entre la courbe de la fonction, l'axe des abscisses, et les droites \\(x=a\\) et \\(x=b\\) . \\(I\\) est une int\u00e9grale d\u00e9finie dont le r\u00e9sultat est un scalaire. La fonction \\(f\\) peut \u00eatre connue qu'en certains points, mais ne dispose pas de singularit\u00e9 sur \\([a,b]\\) , qui est suppos\u00e9 fini et ferm\u00e9 . Dans certains cas limit\u00e9s, l'int\u00e9grale peut \u00eatre calcul\u00e9e analytiquement, \u00e0 partir de la primitive de \\(f\\) , not\u00e9e \\(F\\) : \\(I = \\int_{a}^{b} f(x) dx = F(b)-F(a)\\) Cependant, ce calcul peut \u00eatre long et compliqu\u00e9, et beaucoup de fonctions ne disposent pas d'expression analytique pour leurs primitives. On pr\u00e9f\u00e8rera faire appel \u00e0 des m\u00e9thodes num\u00e9rique pour calculer une valeur approch\u00e9e de \\(I\\) . L'approximation de \\(I\\) s'effectue le plus souvent \u00e0 l'aide de combinaisons lin\u00e9aires des valeurs de \\(f\\) : des formules de quadrature de type interpolation . L'int\u00e9grale \\(I\\) est remplac\u00e9e par une somme finie : \\(I \\approx \\sum_{i=0}^{n} w_i f(x_i)\\) .","title":"Motivation"},{"location":"Chap4_Integration_numerique/#exemple-de-probleme","text":"Au cours de ce chapitre, nous appliquerons les diff\u00e9rentes m\u00e9thodes d'int\u00e9gration \u00e0 un m\u00eame exemple : La mod\u00e9lisation de la r\u00e9fl\u00e9ctivit\u00e9 radar des gouttes de pluies . En 1948, Marshall et Palmer ont propos\u00e9 un mod\u00e8le du facteur de r\u00e9flectivit\u00e9 des gouttes de pluies \\(Z\\) (en \\(mm^6 m^{-3}\\) ) pour les radars m\u00e9t\u00e9orologiques : \\(Z = \\int_{D_{min}}^{D_{max}} N_0 e^{- \\Lambda D} D^6 dD\\) avec \\(D\\) le diam\u00e8tre des gouttes suppos\u00e9es sph\u00e9riques. Les param\u00e8tres de ce mod\u00e8le sont : \\(D_{min}\\) la plus petite taille de goutte, que nous fixerons \u00e0 \\(1 mm\\) . \\(D_{max}\\) la plus grande taille de goutte, que nous fixerons \u00e0 \\(3 mm\\) . \\(\\Lambda\\) une constante empirique en \\(mm^{-1}\\) , pour laquelle Marshall et Palmer proposent \\(4.1 R^{-0.21}\\) , avec \\(R\\) le taux de pluie que nous fixerons \u00e0 \\(5 mm.h^{-1}\\) . \\(N_0\\) une constante empirique en \\(m^{-3} mm^{-1}\\) nomm\u00e9e \"param\u00e8tre de forme\", pour lequel Marshall et Palmer proposent \\(N_0 = 8000\\) . Ce mod\u00e8le est encore aujourd'hui utilis\u00e9 pour l'interpr\u00e9tation des mesures des radars m\u00e9t\u00e9orologiques, dans le but d'estimer les pr\u00e9cipitations aux sol \u00e0 partir des r\u00e9flectivit\u00e9s mesur\u00e9es. Afin d'estimer la r\u00e9flectivit\u00e9 \\(Z\\) li\u00e9e aux gouttes de pluie entre 1 et 3 cm, nous essayerons ici de calculer l'int\u00e9grale entre \\(x=1\\) et \\(x=3\\) de la fonction \\(f(x) = N_0 e^{- \\Lambda x} x^6\\) , dont la valeur est d'environ \\(2337.49 mm^6/m^3\\) . Sous Python on utilisera la biblioth\u00e8que Numpy : import numpy as np Puis, on d\u00e9finira les variables globales suivantes : N0 = 8000 #Param\u00e8tre de forme (m3/mm) R = 5 #Taux de pluie (mm/h) La fonction \\(f\\) sera d\u00e9finie comme : def f(D): return N0*np.exp(-(4.1*R**-0.21)*D)*D**6","title":"Exemple de probl\u00e8me"},{"location":"Chap4_Integration_numerique/#formules-de-quadrature","text":"","title":"Formules de quadrature"},{"location":"Chap4_Integration_numerique/#principe","text":"Id\u00e9e Approcher la fonction \\(f\\) par un polyn\u00f4me \\(p\\) . Si cette approximation est suffisamment bonne : \\(I = \\int_{a}^{b} f(x) dx \\approx \\int_{a}^{b} p(x) dx\\) Cette approche a 2 avantages : Les polyn\u00f4mes sont faciles \u00e0 int\u00e9grer. Cette m\u00e9thode est utilisable m\u00eame si on ne connait \\(f\\) qu'en un nombre fini de points \\((x_i,f(x_i))\\) . Les m\u00e9thodes de Newton-Cotes et de Gauss s'appuient sur cette id\u00e9e en utilisant des formules de quadrature de type interpolation , qui s'expriment comme une combinaison lin\u00e9aire de valeurs de la fonction en des points \u00e0 d\u00e9finir . On cherche donc une valeur approch\u00e9e de \\(I\\) au moyen d'une somme finie : \\(I = \\int_{a}^{b} f(x) dx \\approx I_n = \\sum_{i=0}^{n} w_i f(x_i)\\) On dit que \\(I_n\\) est une formule de quadrature de type interpolation \u00e0 \\(n+1\\) points Les valeurs \\(x_i\\) sont les \" pivots \" ou \"points / noeuds de quadrature\". Les coefficients \\(w_i\\) sont les \" poids \" de la formule de quadrature.","title":"Principe"},{"location":"Chap4_Integration_numerique/#formules-de-type-interpolation","text":"Soient \\((x_i,f(x_i))\\) avec \\(i=0,1,...,n+1\\) points d'interpolation. Un choix naturel consiste \u00e0 approximer la fonction \\(f\\) par le polyn\u00f4me de Lagrange de degr\u00e9 \\(\\leq n\\) qui passe par ces \\(n+1\\) points : \\(f(x) \\approx p(x) = \\sum_{i=0}^{n} f(x_i) L_i(x)\\) Il en r\u00e9sulte la formule de quadrature de type interpolation \u00e0 \\(n+1\\) points : \\(I = \\int_{a}^{b} f(x) dx \\approx I_n = \\int_{a}^{b} p(x) dx = \\int_{a}^{b} \\sum_{i=0}^{n} f(x_i) L_i(x) dx\\) Par lin\u00e9arit\u00e9 de l'int\u00e9grale, on obtient les coefficients de la formule de quadrature : \\(I_n = \\sum_{i=0}^{n} f(x_i) \\int_{a}^{b} L_i(x) dx\\) D'o\u00f9 : \\(w_i = \\int_{a}^{b} L_i(x) dx\\) Les poids \\(w_i\\) s'obtiennent par int\u00e9gration des polyn\u00f4mes de Lagrange . Ils sont donc ind\u00e9pendants de \\(f\\) et ne d\u00e9pendent que des points \\(x_i\\) .","title":"Formules de type interpolation"},{"location":"Chap4_Integration_numerique/#degre-de-precision","text":"On d\u00e9finit l' erreur de troncation pour l'int\u00e9gration comme : \\(E(f) = I-I_n\\) Une formule de quadrature est dite exacte pour \\(f\\) si \\(E(f)=0\\) . Degr\u00e9 de pr\u00e9cision Le degr\u00e9 de pr\u00e9cision d'une formule de quadrature est l'entier positif \\(n\\) tel que \\(E(p_i)=0\\) pour tout polyn\u00f4me de degr\u00e9 \\(i \\leq n\\) et \\(E(p_{n+1}) \\neq 0\\) pour un polyn\u00f4me \\(p_{n+1}\\) de degr\u00e9 \\(n+1\\) . Donc une formule de quadrature exacte sur l'ensemble des polyn\u00f4mes de degr\u00e9 \\(\\leq n\\) est au moins de degr\u00e9 de pr\u00e9cision \\(n\\) . Autrement dit, une formule de quadrature de degr\u00e9 de pr\u00e9cision \\(n\\) v\u00e9rifie : \\(I = \\int_{a}^{b} p_k(x) dx = \\int_{a}^{b} x^k dx = I_n = \\sum_{i=0}^{n} w_i x_i^k\\) pour tout \\(0 < k \\leq n\\) \\(I = \\int_{a}^{b} p_{n+1}(x) dx = \\int_{a}^{b} x^{n+1} dx \\neq I_n = \\sum_{i=0}^{n} w_i x_i^{n+1}\\) Th\u00e9or\u00e8me Une formule de quadrature \u00e0 \\(n+1\\) points est exacte sur l'ensemble des polyn\u00f4mes de degr\u00e9 \\(\\leq n\\) si est seulement si c'est une formule de quadrature de type interpolation \u00e0 \\(n+1\\) points. Donc une formule de quadrature de type interpolation \u00e0 \\(n+1\\) points est au moins de degr\u00e9 de pr\u00e9cision \\(n\\) .","title":"Degr\u00e9 de pr\u00e9cision"},{"location":"Chap4_Integration_numerique/#methodes-de-newton-cotes-simples","text":"Les m\u00e9thodes de Newton-Cotes s'appuient sur la formule de quadrature de type interpolation de Lagrange : \\(I = \\int_{a}^{b} f(x) dx = I_n + E(f) = \\sum_{i=0}^{n} f(x_i) \\int_{a}^{b} L_i(x) dx + E(f)\\) o\u00f9 \\(E(f)\\) est l'erreur de troncature. Les pivots de quadrature sont r\u00e9guli\u00e8rement espac\u00e9s : \\(x_i = x_0 + ih\\) avec \\(i=0,1,...,n\\) et \\(h = \\frac{b-a}{n}\\) Les pivots sont donc fixes (\u00e9quidistants) alors que les poids sont ajust\u00e9s . La r\u00e9gularit\u00e9 de la subdivision permet d'obtenir des formules qui sont tr\u00e8s g\u00e9n\u00e9rales. Il y a \\(n+1\\) pivots donc cette m\u00e9thode est exacte pour les polyn\u00f4mes de degr\u00e9 \\(\\leq n\\) au moins .","title":"M\u00e9thodes de Newton-cotes simples"},{"location":"Chap4_Integration_numerique/#methode-des-rectangles-n0","text":"Lorsque l'on ne dispose que d'un seul point \\((x_0,f(x_0))\\) , on peut utiliser la formule des rectangles : \\(I = \\int_{a}^{b} f(x) dx \\approx I_0 = \\int_{a}^{b} f(x_0) dx = (b-a) f(x_0)\\) On donnera diff\u00e9rents noms \u00e0 la m\u00e9thode suivant le choix de \\(x_0\\) : \"A gauche\" : si on choisit \\(x_0 = a\\) . \"A droite\" : si on choisit \\(x_0 = b\\) . \"Au point milieu\" : si on choisit \\(x_0 = \\frac{a+b}{2}\\) On remarque que \\(I_0\\) est l' aire du rectangle de largeur \\(b-a\\) et de longueur \\(f(x_0)\\) . Voici pour illustration l'aire de ce rectangle dans le cas de notre exemple : Les formules des rectangles \u00e0 droite et \u00e0 gauche sont exactes pour des polyn\u00f4mes de degr\u00e9 0 uniquement : leur degr\u00e9 de pr\u00e9cision est de 0 . La formule au point milieu est aussi exacte pour les fonctions affines car elle exploite les sym\u00e9tries du probl\u00e8me : son degr\u00e9 de pr\u00e9cision est de 1 . Si \\(f\\) est continue et 2 fois d\u00e9rivable sur \\([a,b]\\) , alors il existe \\(\\xi \\in ]a,b[\\) tel que \\(I = I_0 + E(f)\\) avec : \\(\\begin{cases} E(f) = \\frac{(b-a)^2}{2} f'(\\xi) = \\frac{h^2}{2} f'(\\xi) \\; si \\; x_0 = a \\; ou \\; x_0 = b \\\\ E(f) = \\frac{(b-a)^3}{24} f\"(\\xi) = \\frac{h^3}{24} f\"(\\xi) \\; si \\; x_0 = \\frac{a+b}{2} \\end{cases}\\) On en d\u00e9duit que : Donc plus \\([a,b]\\) est petit, plus l'erreur est faible. Pour les formules \u00e0 droite et \u00e0 gauche, l'erreur d\u00e9croit en \\(h^2\\) . Pour la formule au point milieu, l'erreur d\u00e9croit en \\(h^3\\) . Plus les variations de \\(f\\) sont limit\u00e9es ( \\(f'\\) faible), plus l'erreur est faible pour les m\u00e9thodes \u00e0 droite et \u00e0 gauche.","title":"M\u00e9thode des rectangles (n=0)"},{"location":"Chap4_Integration_numerique/#methode-des-trapezes-n1","text":"Lorsque l'on dispose que de 2 points \\((x_0,f(x_0))\\) et \\((x_1,f(x_1))\\) , on peut utiliser la formule des trap\u00e8zes : \\(I = \\int_{a}^{b} f(x) dx \\approx I_1 = \\frac{(b-a)}{2} (f(a)+f(b))\\) \\(I_1\\) est une formule de quadrature de type interpolation \u00e0 2 points. On remarque qu'elle correspond \u00e0 l' aire d'un trap\u00e8ze . Voici pour illustration l'aire de ce trap\u00e8ze dans le cas de notre exemple : Si \\(f\\) est continue et 2 fois d\u00e9rivable sur \\([a,b]\\) , alors il existe \\(\\xi \\in ]a,b[\\) tel que \\(I = I_1 + E(f)\\) avec : \\(E(f) = - \\frac{h^3}{12} f\"(\\xi)\\) La m\u00e9thode des trap\u00e8zes est exacte sur l'espace des polyn\u00f4mes de degr\u00e9 \\(\\leq 1\\) donc de degr\u00e9 de pr\u00e9cision 1 . Par contre, elle est 2X plus lente que la m\u00e9thode des rectangles au point milieu pour le m\u00eame degr\u00e9 de pr\u00e9cision.","title":"M\u00e9thode des trap\u00e8zes (n=1)"},{"location":"Chap4_Integration_numerique/#methode-de-simpson-n2","text":"Lorsque l'on dispose que de 3 points \\((x_0,f(x_0))\\) , \\((x_1,f(x_1))\\) et \\((x_2,f(x_2))\\) , on peut utiliser la formule de Simpson . Si on choisit \\(x_0 = a\\) , \\(x_1 = \\frac{a+b}{2}\\) et \\(x_2 = b\\) : \\(I = \\int_{a}^{b} f(x) dx \\approx I_2 = \\frac{(b-a)}{6} (f(a)+4 f(\\frac{a+b}{2})+f(b))\\) \\(I_2\\) est une formule de quadrature de type interpolation \u00e0 3 points. On remarque qu'elle correspond \u00e0 l' aire sous un morceau de parabole . Voici pour illustration l'aire sous la parabole dans le cas de notre exemple : Si \\(f\\) est continue et 4 fois d\u00e9rivable sur \\([a,b]\\) , alors il existe \\(\\xi \\in ]a,b[\\) tel que \\(I = I_2 + E(f)\\) avec : \\(E(f) = - \\frac{h^5}{90} f^{(4)}(\\xi)\\) La m\u00e9thode de Simpson est exacte sur l'espace des polyn\u00f4mes de degr\u00e9 \\(\\leq 2\\) , et on peut m\u00eame montrer qu'elle est de degr\u00e9 de pr\u00e9cision 3 . Elle n\u00e9cessite l'\u00e9valuation de \\(f\\) en 3 points, mais l'erreur diminue rapidement, en \\(h^5\\) .","title":"M\u00e9thode de Simpson (n=2)"},{"location":"Chap4_Integration_numerique/#methodes-de-newton-cotes-n0","text":"On peut g\u00e9n\u00e9raliser les formule de quadrature de type interpolation de Lagrange \u00e0 tout degr\u00e9 \\(n>0\\) . On parle alors de m\u00e9thodes de Newton-Cotes de mani\u00e8re g\u00e9n\u00e9rale. On distingue 2 formules suivant si \\(n\\) est pair ou impair : Cas de \\(n\\) impair Si \\(f\\) est d\u00e9rivable \\(n+1\\) fois sur \\([a,b]\\) , alors il existe un r\u00e9el \\(K\\) et \\(\\xi \\in ]a,b[\\) tel que \\(E(f) = \\frac{K}{(n+1)!} (b-a)^{n+2} f^{(n+1)}(\\xi)\\) Le degr\u00e9 de pr\u00e9cision est \\(n\\) . Cas de \\(n\\) pair Si \\(f\\) est d\u00e9rivable \\(n+2\\) fois sur \\([a,b]\\) , alors il existe un r\u00e9el \\(K\\) et \\(\\xi \\in ]a,b[\\) tel que \\(E(f) = \\frac{K}{(n+2)!} (b-a)^{n+3} f^{(n+2)}(\\xi)\\) Le degr\u00e9 de pr\u00e9cision est \\(n+1\\) . On peut en conclure qu'une formule centr\u00e9e ( \\(n\\) pair, donc nombre de points impair) est pr\u00e9f\u00e9rable. Voici les formules des m\u00e9thodes de Newton-Cotes jusqu'\u00e0 \\(n=6\\) : Degr\u00e9 \\(n\\) Nom Formule 1 M\u00e9thode des trap\u00e8zes \\(\\frac{b-a}{2}(f(x_0)+f(x_1))\\) 2 M\u00e9thode de Simpson 1/3 \\(\\frac{b-a}{6}(f(x_0)+4f(x_1)+f(x_2))\\) 3 M\u00e9thode de Simpson 3/8 \\(\\frac{b-a}{8}(f(x_0)+3f(x_1)+3f(x_2)+f(x_3))\\) 4 M\u00e9thode de Boole-Villarceau \\(\\frac{b-a}{90}(7f(x_0)+32f(x_1)+12f(x_2)+32f(x_3)+7f(x_4))\\) 6 M\u00e9thode de Weddle-Hardy \\(\\frac{b-a}{840}(41f(x_0)+216f(x_1)+27f(x_2)+272f(x_3)+27f(x_4)+216f(x_5)+41f(x_6))\\) En pratique, les m\u00e9thodes de Newton-Cotes ne sont presque jamais utilis\u00e9es pour \\(n>6\\) .","title":"M\u00e9thodes de Newton-Cotes (n&gt;0)"},{"location":"Chap4_Integration_numerique/#algorithmes","text":"Voici les diff\u00e9rentes m\u00e9thodes de Newton-Cotes pr\u00e9sent\u00e9es pr\u00e9c\u00e9demment sous la forme de fonctions Python. Elles prennent toutes en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. La fonction pour la m\u00e9thode des rectangles \u00e0 gauche : def rectangles_gauche(f,a,b): return (b-a)*f(a) La fonction pour la m\u00e9thode des rectangles \u00e0 droite : def rectangles_droite(f,a,b): return (b-a)*f(b) La fonction pour la m\u00e9thode des rectangles au point milieu : def rectangles_milieu(f,a,b): return (b-a)*f((a+b)/2) La fonction pour la m\u00e9thode des trap\u00e8zes : def trapezes(f,a,b): return (b-a)*(f(a)+f(b))/2 La fonction pour la m\u00e9thode de Simpson : def simpson(f,a,b): return (b-a)*(f(a)+4*f((a+b)/2)+f(b))/6","title":"Algorithmes"},{"location":"Chap4_Integration_numerique/#exemple","text":"En appliquant les algorithmes pr\u00e9c\u00e9dents \u00e0 notre probl\u00e8me exemple, on trouve les valeurs suivantes : M\u00e9thode Estimation de Z ( \\(mm^6 m^{-3}\\) ) Rectangles \u00e0 gauche 859.36 Rectangles \u00e0 droite 1807.24 Rectangles au point milieu 2954.01 Trap\u00e8zes 1333.30 Simpson 2413.78 Ces estimations sont \u00e0 comparer \u00e0 la valeur th\u00e9orique \\(Z = 2337.49 mm^6/m^3\\) . Notre fonction \u00e0 int\u00e9grer n'\u00e9tant pas un polyn\u00f4me de degr\u00e9 \\(\\leq n\\) , on ne s'attendait pas \u00e0 avoir un r\u00e9sultat exact. On note toutefois que plus le degr\u00e9 de pr\u00e9cision augmente, meilleure est l'estimation. Exercice : D\u00e9terminez la formule de Newton-Cotes pour \\(n=4\\) (aussi appell\u00e9e \"m\u00e9thode de Boole-Villarceau\"), et impl\u00e9mentez-l\u00e0 sous la forme d'une fonction Python. Quelle estimation de \\(Z\\) obtenez-vous ? Cette valeur est-elle plus proche du r\u00e9sultat attendu ? Ce r\u00e9sultat \u00e9tait-il attendu ?","title":"Exemple"},{"location":"Chap4_Integration_numerique/#methode-de-newton-cotes-composites","text":"Pour am\u00e9liorer la pr\u00e9cision des m\u00e9thode de Newton-Cotes, on pourrai augmenter le nombre de point d'int\u00e9gration, et donc le degr\u00e9 du polyn\u00f4me d'interpolation. Mais cela conduit \u00e0 des formules de plus en plus complexes, et m\u00eame potentiellement \u00e0 un ph\u00e9nom\u00e8ne de Runge (voir le Chapitre 3). C'est pourquoi on utilise en g\u00e9n\u00e9ral des m\u00e9thodes composites avec des formules de Newton-Cotes de degr\u00e9 \\(n<6\\) . L'id\u00e9e des formules composites est la suivante : D\u00e9couper l'intervalle \\([a,b]\\) en \\(M\\) sous-intervalles. Appliquer une formule de Newton-Cotes sur chaque sous-intervalle pour estimer l'aire sous la courbe. Additionner les aires estim\u00e9es pour chaque sous-intervalle. Ceci revient \u00e0 l'expression suivante de l'int\u00e9grale : \\(I = \\int_{a}^{b} f(x) dx = \\displaystyle\\sum_{j=0}^{M} \\int_{x_{j-1}=a+(j-1)h}^{x_j=a+jh} f(x) dx\\) Dans ce cas, les points de subdivision sont r\u00e9guli\u00e8rement espac\u00e9s d'un pas \\(h=\\frac{b-a}{M}\\) . Une formule de quadrature de type interpolation composite s'\u00e9crit donc : \\(I_{n,M} = \\displaystyle\\sum_{j=1}^{M} \\displaystyle\\sum_{i=0}^{n} w_i^{(j)} f(x_i^{(j)})\\)","title":"M\u00e9thode de Newton-Cotes composites"},{"location":"Chap4_Integration_numerique/#formule-composite-des-rectangles","text":"Soit \\(M+1\\) points de discr\u00e9tisation \\((x_j,f(x_j))\\) avec \\(x_j=a+jh\\) et \\(h=\\frac{b-a}{M}\\) . Rectangles \u00e0 gauche : Sur chaque sous-intervalle \\([x_j,x_{j+1}]\\) : \\(\\int_{x_{j+1}}^{x_j} f(x) dx \\approx hf(x_{j})\\) D'o\u00f9 la formule composite des rectangles \u00e0 gauche : \\(\\int_{a}^{b} f(x) dx \\approx h(\\displaystyle\\sum_{j=0}^{M-1} f(x_{j}))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Rectangles \u00e0 droite : Sur chaque sous-intervalle \\([x_j,x_{j+1}]\\) : \\(\\int_{x_{j+1}}^{x_j} f(x) dx \\approx hf(x_{j+1})\\) D'o\u00f9 la formule composite des rectangles \u00e0 droite : \\(\\int_{a}^{b} f(x) dx \\approx h(\\displaystyle\\sum_{j=0}^{M-1} f(x_{j+1}))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Rectangles au point milieu : Sur chaque sous-intervalle \\([x_j,x_{j+1}]\\) : \\(\\int_{x_{j+1}}^{x_j} f(x) dx \\approx hf(\\frac{x_{j}+x_{j+1}}{2})\\) D'o\u00f9 la formule composite des rectangles au point milieu : \\(\\int_{a}^{b} f(x) dx \\approx h(\\displaystyle\\sum_{j=0}^{M-1} f(\\frac{x_{j}+x_{j+1}}{2}))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Ces formules n\u00e9cessitent \\(M\\) \u00e9valuations de \\(f\\) . On peut alors montrer que dans le cas du point milieu, l'erreur est major\u00e9e ainsi : \\(\\mid E(f) \\mid \\leq \\frac{b-a}{24} h^2 max_{x \\in [a,b]} \\mid f\"(x) \\mid\\) On observe que \\(\\lim\\limits_{M \\rightarrow \\infty} E(f) = \\lim\\limits_{h \\rightarrow 0} E(f) = 0\\) . On en d\u00e9duit que la m\u00e9thode converge bien vers la valeur exacte de l'int\u00e9grale. L'ordre de convergence est de 2 : l'erreur est divis\u00e9e par 4 lorsque h est divis\u00e9 par 2 .","title":"Formule composite des rectangles"},{"location":"Chap4_Integration_numerique/#formule-composite-des-trapezes","text":"Sur chaque sous-intervalle \\([x_j,x_{j+1}]\\) : \\(\\int_{x_{j+1}}^{x_j} f(x) dx \\approx \\frac{h}{2} (f(x_{j})+f(x_{j+1}))\\) D'o\u00f9 la formule composite des trap\u00e8zes : \\(\\int_{a}^{b} f(x) dx \\approx h (\\frac{1}{2} f(a) + \\displaystyle\\sum_{j=1}^{M-1} f(x_j) + \\frac{1}{2} f(b))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Cette formule n\u00e9cessite \\(M+1\\) \u00e9valuations de \\(f\\) . On peut montrer que l'erreur est major\u00e9e ainsi : \\(\\mid E(f) \\mid \\leq \\frac{b-a}{12} h^2 max_{x \\in [a,b]} \\mid f\"(x) \\mid\\) On observe que \\(\\lim\\limits_{M \\rightarrow \\infty} E(f) = \\lim\\limits_{h \\rightarrow 0} E(f) = 0\\) . On en d\u00e9duit que la m\u00e9thode converge bien vers la valeur exacte de l'int\u00e9grale. L'ordre de convergence est de 2 : l'erreur est divis\u00e9e par 4 lorsque h est divis\u00e9 par 2 .","title":"Formule composite des trap\u00e8zes"},{"location":"Chap4_Integration_numerique/#formule-composite-de-simpson","text":"Pour faciliter l'\u00e9criture, on d\u00e9finira pour la m\u00e9thode de Simpson composite \\(h = \\frac{b-a}{2M}\\) pour un nombre de sous-intervalles \\(M\\) . Sur chaque sous-intervalle \\([x_j,x_{j+2}]\\) : \\(\\int_{x_j}^{x_{j+2}} f(x) dx \\approx \\frac{h}{3} (f(x_{j})+4f(x_{j+1})+f(x_{j+2}))\\) D'o\u00f9 la formule composite de Simpson : \\(\\int_{a}^{b} f(x) dx \\approx \\frac{h}{3} (f(a) + 2 \\displaystyle\\sum_{j=1}^{M-1} f(x_{2j}) + 4 \\displaystyle\\sum_{j=0}^{M-1} f(x_{2j+1}) + f(b))\\) Voici une illustration pour notre exemple, avec \\(M=5\\) : Cette formule n\u00e9cessite \\(2M+1\\) \u00e9valuations de \\(f\\) . On peut montrer que l'erreur est major\u00e9e ainsi : \\(\\mid E(f) \\mid \\leq \\frac{b-a}{90} h^4 max_{x \\in [a,b]} \\mid f^{(4)}(x) \\mid\\) On observe que \\(\\lim\\limits_{M \\rightarrow \\infty} E(f) = \\lim\\limits_{h \\rightarrow 0} E(f) = 0\\) . On en d\u00e9duit que la m\u00e9thode converge bien vers la valeur exacte de l'int\u00e9grale. L'ordre de convergence est de 4 : l'erreur est divis\u00e9e par 16 lorsque h est divis\u00e9 par 2 .","title":"Formule composite de Simpson"},{"location":"Chap4_Integration_numerique/#algorithmes_1","text":"Voici une fonction Python pour calculer une int\u00e9grale \u00e0 partir des m\u00e9thodes simples programm\u00e9es pr\u00e9c\u00e9demment. Elle prend en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. methode la m\u00e9thode d'int\u00e9gration (rectangles, trap\u00e8zes ou Simpson) sous la forme d'une fonction Python. M le nombre de sous-intervalles d'int\u00e9gration. def methode_composite(f,a,b,methode,M): #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de #(b-a)/M : x_i = [a+i*(b-a)/M for i in range(M+1)] #Initialisation de la somme des aires sous la courbe des diff\u00e9rents #sous-intervalles : aire = 0 #Boucle sur les sous-intervalles : for i in range(M): #Addition au compteur de l'aire sous la courbe pour ce sous-intervalle : aire += methode(f,x_i[i],x_i[i+1]) #Renvoyer l'estimation de l'aire sous la courbe pour l'intervalle [a,b] : return aire On peut appliquer la m\u00e9thode de Simpson composite \\(M=5\\) \u00e0 notre exemple avec la commande Python : I_simpson_composite = methode_composite(f,1,3,simpson,5) Si cette fonction est \u00e9l\u00e9gante, car elle est utilisable pour toutes les m\u00e9thodes programm\u00e9es pr\u00e9c\u00e9demment, il est \u00e0 noter qu'elle n'est pas optimis\u00e9e pour la m\u00e9thode des trap\u00e8zes et la m\u00e9thode de Simpson. En effet, avec cette impl\u00e9mentation, on \u00e9value plusieurs fois la fonction aux m\u00eames points. Pour une impl\u00e9mentation plus optimis\u00e9e, il vaut mieux programmer une fonction pour chaque m\u00e9thode composite, en se basant sur les formules donn\u00e9es pr\u00e9c\u00e9demment. Toutes les fonctions suivantes prennent toutes en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. M le nombre de sous-intervalles d'int\u00e9gration. Voici la fonction pour la m\u00e9thode composite des rectangles \u00e0 gauche : def rectangles_gauche_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/M #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(M+1)] #Initialiser la somme des \u00e9valuations de f pour chaque sous-intervalle: somme = 0 #Boucle sur les sous-intervalles : for i in range(M): #Sommer la valeur de f \"\u00e0 gauche\" du sous-intervalle : somme += f(x_i[i]) #Calcul de la somme des aires des sous-intervalles : aire = somme*h return aire Voici la fonction pour la m\u00e9thode composite des rectangles \u00e0 droite : def rectangles_droite_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/M #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(M+1)] #Initialiser la somme des \u00e9valuations de f pour chaque sous-intervalle: somme = 0 #Boucle sur les sous-intervalles : for i in range(M): #Sommer la valeur de f \"\u00e0 droite\" du sous-intervalle : somme += f(x_i[i+1]) #Calcul de la somme des aires des sous-intervalles : aire = somme*h return aire Voici la fonction pour la m\u00e9thode composite des rectangles au point milieu : def rectangles_milieu_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/M #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(M+1)] #Initialiser la somme des \u00e9valuations de f pour chaque sous-intervalle: somme = 0 #Boucle sur les sous-intervalles : for i in range(M): #Sommer la valeur de f \"au milieu\" du sous-intervalle : somme += f((x_i[i]+x_i[i+1])/2) #Calcul de la somme des aires des sous-intervalles : aire = somme*h return aire Voici la fonction pour la m\u00e9thode composite des trap\u00e8zes : def trapezes_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/M #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(M+1)] #Initialiser la somme des \u00e9valuations de f pour chaque sous-intervalle avec #f(a) et f(b): somme = (f(x_i[0])+f(x_i[-1]))/2 #Boucle sur les sous-intervalles : for i in range(1,M): #Sommer la valeur de f \u00e0 gauche du sous-intervalle : somme += f(x_i[i]) #Calcul de la somme des aires des sous-intervalles : aire = somme*h return aire Voici la fonction pour la m\u00e9thode composite de Simpson : def simpson_composite(f,a,b,M): #D\u00e9terminer la largeur h de chaque sous-intervalle : h = (b-a)/(2*M) #D\u00e9coupage de l'intervalle [a,b] en M sous-intervalles avec un pas de h : x_i = [a+i*h for i in range(2*M+1)] #Initialiser la somme des \u00e9valuations de f pour les \u00e9l\u00e9ments pairs et #impairs de l'intervalle : somme_pair = 0 somme_impair = 0 #1\u00e8re boucle sur les \u00e9l\u00e9ments pairs des sous-intervalles : for i in range(1,M): #Sommer la valeur de f du sous-intervalle : somme_pair += f(x_i[2*i]) #2nde boucle sur les \u00e9l\u00e9ments impairs des sous-intervalles : for i in range(M): somme_impair += f(x_i[2*i+1]) #Additionner les valeurs de f : somme = f(x_i[0])+f(x_i[-1])+2*somme_pair+4*somme_impair #Calcul de la somme des aires des sous-intervalles : aire = somme*h/3 return aire","title":"Algorithmes"},{"location":"Chap4_Integration_numerique/#exemples","text":"En appliquant les algorithmes pr\u00e9c\u00e9dents \u00e0 notre probl\u00e8me exemple, avec un nombre de sous-intervalles \\(M = 5\\) on trouve les valeurs suivantes : M\u00e9thode composite (M=5) Estimation de Z ( \\(mm^6 m^{-3}\\) ) Rectangles \u00e0 gauche 2213.67 Rectangles \u00e0 droite 2403.25 Rectangles au point milieu 2352.11 Trap\u00e8zes 2308.46 Simpson 2337.56 Ces estimations sont \u00e0 comparer \u00e0 la valeur th\u00e9orique \\(Z = 2337.49 mm^6/m^3\\) . Exercice : Utilisez les fonctions Python pr\u00e9c\u00e9dentes pour calculer l'erreur absolue commise par les m\u00e9thodes composite des trap\u00e8zes et de Simpson. Calculez l'erreur pour M = 1, 2, 4, 8 et 16. Qu'observez-vous ?","title":"Exemples"},{"location":"Chap4_Integration_numerique/#acceleration-de-romberg","text":"Concernant les m\u00e9thodes composites, il reste la question suivante : Comment choisir un pas d'int\u00e9gration \\(h\\) pour obtenir la pr\u00e9cision souhait\u00e9e \\(\\epsilon\\) ? L' acc\u00e9l\u00e9ration de Romberg est une m\u00e9thode it\u00e9rative qui permet d'am\u00e9liorer l'ordre de convergence de la m\u00e9thode des trap\u00e8zes ou de Simpson. Elle tire partie du fait que : Pour la m\u00e9thode des trap\u00e8zes : \\(I = I_h + E_h(f) = I_{2h} + E_{2h}(f)\\) avec \\(E_{2h}(f) = 4 E_h(f)\\) d'o\u00f9 \\(\\mid I_{2h} - I_h \\mid = 3 E_h(f)\\) et donc : \\(E_h(f) = \\frac{\\mid I_{2h} - I_h \\mid}{3}\\) Pour la m\u00e9thode de Simpson : \\(I = I_h + E_h(f) = I_{2h} + E_{2h}(f)\\) avec \\(E_{2h}(f) = 16 E_h(f)\\) d'o\u00f9 \\(\\mid I_{2h} - I_h \\mid = 15 E_h(f)\\) et donc : \\(E_h(f) = \\frac{\\mid I_{2h} - I_h \\mid}{15}\\) On d\u00e9termine alors le pas d'int\u00e9gration \\(h\\) permettant d'obtenir \\(\\epsilon\\) avec l'algorithme suivant : Algorithme d'acc\u00e9l\u00e9ration de Romberg On se donne un d\u00e9coupage initial \\(2h\\) On calcule \\(I_{2h}\\) On double le nombre d'intervalles : on prend un pas de \\(h\\) On calcule \\(I_h\\) Pour la m\u00e9thode des trap\u00e8zes : tant que \\(\\mid I_{2h} - I_h \\mid > 3 \\epsilon\\) on r\u00e9p\u00e8te les op\u00e9rations pr\u00e9c\u00e9dentes Pour la m\u00e9thode de Simpson : tant que \\(\\mid I_{2h} - I_h \\mid > 15 \\epsilon\\) on r\u00e9p\u00e8te les op\u00e9rations pr\u00e9c\u00e9dentes Voici une fonction Python impl\u00e9mentant l'acc\u00e9l\u00e9ration de Romberg pour la m\u00e9thode des trap\u00e8zes composite. Elle prend en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. epsilon la pr\u00e9cision souhait\u00e9e sur la valeur de l'int\u00e9grale. def romberg_trapezes(f,a,b,epsilon): #Initialiser le nombre de sous-intervalles d'int\u00e9gration M : M = 1 #Initialiser les aires estim\u00e9es pour M et 2M sous-intervalles d'int\u00e9gration : aire_M = trapezes_composite(f,a,b,M) aire_2M = trapezes_composite(f,a,b,2*M) #Boucler tant que la condition sur l'erreur d'int\u00e9gration n'est pas atteinte : while abs(aire_M-aire_2M)>(3*epsilon): #Multiplier le nombre de sous-intervalles d'int\u00e9gration par 2 : M *= 2 #Estimer l'aire pour M et 2M sous-intervalles d'int\u00e9gration : aire_M = trapezes_composite(f,a,b,M) aire_2M = trapezes_composite(f,a,b,2*M) return aire_2M Voici une application de l'acc\u00e9l\u00e9ration de Romberg \u00e0 notre probl\u00e8me, avec la m\u00e9thode des trap\u00e8zes composite, en demandant une pr\u00e9cision de \\(1 mm^6/m^3\\) : Pour obtenir une pr\u00e9cision de \\(10^{-3}\\) sur la valeur de \\(Z = 2337.49 mm^6/m^3\\) avec la m\u00e9thode des trap\u00e8zes composite, on trouve par la m\u00e9thode de Romberg qu'il faut \\(M=1024\\) .","title":"Acc\u00e9l\u00e9ration de Romberg"},{"location":"Chap4_Integration_numerique/#methodes-de-gauss","text":"","title":"M\u00e9thodes de Gauss"},{"location":"Chap4_Integration_numerique/#principe_1","text":"Les m\u00e9thodes de Newton-Cotes fixent les pivots de quadrature et utilisent des poids assurant un degr\u00e9 de pr\u00e9cision de \\(n\\) ou \\(n+1\\) . L'id\u00e9e des m\u00e9thodes de Gauss est de choisir les pivots pour que le degr\u00e9 de pr\u00e9cision de la formule soit le plus \u00e9lev\u00e9 possible. Soit la formule de quadrature \u00e0 \\((n+1)\\) points : \\(I = \\int_{a}^{b} f(x) dx \\approx I_n = \\displaystyle\\sum_{i=0}^{n} w_i f(x_i)\\) Les \\((n+1)\\) pivots et les \\((n+1)\\) poids, donc les \\(2(n+1)\\) param\u00e8tres, sont ajust\u00e9s pour optimiser la pr\u00e9cision de la m\u00e9thode. Pour que la m\u00e9thode soit exacte pour les polyn\u00f4mes de degr\u00e9 \\(\\leq 2n+1\\) on cherche donc les \\(2(n+1)\\) param\u00e8tres tels que : \\(\\int_{a}^{b} x^k dx = \\displaystyle\\sum_{i=0}^{n} w_i x_i^k\\) avec \\(k=0,1,...,2n+1\\)","title":"Principe"},{"location":"Chap4_Integration_numerique/#methode-a-1-point-n0","text":"A l'ordre 1 ( \\(n=0\\) ), on a 2 inconnues \u00e0 ajuster : le point \\(x_0\\) et le coefficient \\(w_0\\) . Soit \\(I_0 = w_0 f(x_0)\\) On va ajuster ces param\u00e8tres pour que la m\u00e9thode soit exacte dans l'ensemble des polyn\u00f4mes de degr\u00e9 \\(\\leq 1\\) . On r\u00e9sout donc le syst\u00e8me de 2 \u00e9quations \u00e0 2 inconnues : \\(\\begin{cases} \\int_{a}^{b} 1 dx = w_0\\\\ \\int_{a}^{b} x dx = w_0 x_0\\\\ \\end{cases}\\) On obtient la formule optimale de Gauss : \\(I_0 = (b-a) f(\\frac{a+b}{2})\\) On retrouve alors la m\u00e9thode du point milieu de Newton-Cotes.","title":"M\u00e9thode \u00e0 1 point (n=0)"},{"location":"Chap4_Integration_numerique/#methode-a-2-points-n1","text":"A l'ordre 2 ( \\(n=1\\) ), on a 4 inconnues \u00e0 ajuster : les 2 points \\(x_0\\) et \\(x_1\\) , et les 2 coefficients \\(w_0\\) et \\(w_1\\) . Soit \\(I_1 = w_0 f(x_0) + w_1 f(x_1)\\) On va ajuster ces param\u00e8tres pour que la m\u00e9thode soit exacte dans l'ensemble des polyn\u00f4mes de degr\u00e9 \\(\\leq 3\\) (\u00e0 comparer au degr\u00e9 \\(\\leq 1\\) de la m\u00e9thode des trap\u00e8zes). On r\u00e9sout donc le syst\u00e8me de 4 \u00e9quations \u00e0 4 inconnues : \\(\\begin{cases} \\int_{a}^{b} 1 dx = w_0 + w_1\\\\ \\int_{a}^{b} x dx = w_0 x_0 + w_1 x_1\\\\ \\int_{a}^{b} x^2 dx = w_0 x_0^2 + w_1 x_1^2\\\\ \\int_{a}^{b} x^3 dx = w_0 x_0^3 + w_1 x_1^3 \\end{cases}\\) On obtient la formule optimale de Gauss : \\(I_1 = \\frac{b-a}{2} (f(\\frac{-1}{\\sqrt{3}} \\frac{b-a}{2} + \\frac{a+b}{2})+f(\\frac{1}{\\sqrt{3}} \\frac{b-a}{2} + \\frac{a+b}{2}))\\)","title":"M\u00e9thode \u00e0 2 points (n=1)"},{"location":"Chap4_Integration_numerique/#generalisation-n-quelconque","text":"On peut g\u00e9n\u00e9raliser les formules trouv\u00e9es pr\u00e9c\u00e9demment pour un nombre de points quelconque. Les points et les poids sont d\u00e9termin\u00e9s pour l'int\u00e9grale d'un polyn\u00f4me entre -1 et 1, puis par un changement de variable \\(x_i = \\frac{b-a}{2} y_i + \\frac{a+b}{2}\\) et \\(w_i = \\frac{b-a}{2} v_i\\) on obtient la formule g\u00e9n\u00e9rale suivante : \\(I_n = \\frac{b-a}{2} \\displaystyle\\sum_{i=1}^{n} v_i f(\\frac{b-a}{2} y_i + \\frac{a+b}{2})\\) Voici les points et les poids \\(y_i\\) d\u00e9termin\u00e9s pour des ordres de 0 \u00e0 3 : Ordre \\(n\\) Nombre de points Points \\(y_i\\) Poids \\(v_i\\) 0 1 0 2 1 2 \\(-\\sqrt{\\frac{1}{3}}\\) et \\(\\sqrt{\\frac{1}{3}}\\) 1 et 1 2 3 0, \\(-\\sqrt{\\frac{3}{5}}\\) et \\(\\sqrt{\\frac{3}{5}}\\) \\(\\frac{8}{9}\\) , \\(\\frac{5}{9}\\) et \\(\\frac{5}{9}\\) 3 4 \\(\\sqrt{\\frac{3}{7}-\\frac{2}{7}\\sqrt{\\frac{6}{5}}}\\) , \\(-\\sqrt{\\frac{3}{7}-\\frac{2}{7}\\sqrt{\\frac{6}{5}}}\\) , \\(\\sqrt{\\frac{3}{7}+\\frac{2}{7}\\sqrt{\\frac{6}{5}}}\\) et \\(-\\sqrt{\\frac{3}{7}+\\frac{2}{7}\\sqrt{\\frac{6}{5}}}\\) \\(\\frac{18+\\sqrt{30}}{36}\\) , \\(\\frac{18+\\sqrt{30}}{36}\\) , \\(\\frac{18-\\sqrt{30}}{36}\\) et \\(\\frac{18-\\sqrt{30}}{36}\\) NB : Pour les curieux, les points \\(y_i\\) et les poids \\(v_i\\) sont d\u00e9termin\u00e9s gr\u00e2ce \u00e0 ce qu'on appelle les \"polyn\u00f4mes de Legendre\". Les m\u00e9thodes de Gauss sont plus performantes que les m\u00e9thodes de Newton-Cotes en termes de pr\u00e9cision pour un m\u00eame temps de calcul : l'erreur d\u00e9croit exponentiellement avec \\(n\\) . Toutefois, la th\u00e9orie derri\u00e8re ces m\u00e9thodes est plus difficile , et leur programmation est plus lourde si l'on doit d\u00e9terminer les points / poids soi-m\u00eame.","title":"G\u00e9n\u00e9ralisation (n quelconque)"},{"location":"Chap4_Integration_numerique/#algorithmes_2","text":"Voici une fonction Python impl\u00e9mentant la m\u00e9thode de Gauss pour des points / poids donn\u00e9s. Elle prend en entr\u00e9e : f la fonction Python \u00e0 int\u00e9grer. a la borne inf\u00e9rieure de l'intervalle d'int\u00e9gration. b la borne sup\u00e9rieure de l'intervalle d'int\u00e9gration. y_i la liste des points d\u00e9termin\u00e9s pour l'ordre de la m\u00e9thode choisie. v_i la liste des poids d\u00e9termin\u00e9s pour l'ordre de la m\u00e9thode choisie. def gauss(f,a,b,y_i,v_i): #Initialiser la somme de la formule de quadrature : somme = 0 #Boucles sur les points / poids : for i in range(len(y_i)): #Sommer l'\u00e9valuation de f au point choisi, pond\u00e9r\u00e9e par le poids choisi : somme += v_i[i]*f(y_i[i]*(b-a)/2+((a+b)/2)) #Calculer l'estimation de l'aire sous la courbe : aire = somme*(b-a)/2 return aire On peut appliquer la m\u00e9thode de Gauss d'ordre 2 \u00e0 notre exemple avec la commande Python : I_gauss_ord2 = gauss(f,1,3,[0,-(3/5)**0.5,(3/5)**0.5],[8/9,5/9,5/9])","title":"Algorithmes"},{"location":"Chap4_Integration_numerique/#exemples_1","text":"En appliquant l'algorithme pr\u00e9c\u00e9dent pour diff\u00e9rents ordres \u00e0 notre probl\u00e8me exemple, on trouve les valeurs suivantes : M\u00e9thode de Gauss d'ordre Estimation de Z ( \\(mm^6 m^{-3}\\) ) \\(n=0\\) 2954.01 \\(n=1\\) 2285.50 \\(n=2\\) 2338.36 \\(n=3\\) 2337.64 Ces estimations sont \u00e0 comparer \u00e0 la valeur th\u00e9orique \\(Z = 2337.49 mm^6/m^3\\) . Notre fonction \u00e0 int\u00e9grer n'\u00e9tant pas un polyn\u00f4me de degr\u00e9 \\(\\leq 2n+1\\) , on ne s'attendait pas \u00e0 avoir un r\u00e9sultat exact. N\u00e9anmoins, on peut observer comme attendu que le r\u00e9sultat converge beaucoup plus rapidement avec \\(n\\) que les m\u00e9thodes de Newton-Cotes. Exercice : Avec la fonction Python pr\u00e9c\u00e9dente, calculez l'erreur absolue commise par les m\u00e9thodes de Gauss pour \\(n\\) allant de 0 \u00e0 3. Tracez ces point sur un graphique. Comment l'erreur semble diminuer avec \\(n\\) ? Ce r\u00e9sultat est-il attendu ?","title":"Exemples"},{"location":"Chap4_Integration_numerique/#conclusions","text":"On appelle formule de quadrature une formule permettant d'approcher l'int\u00e9grale d'une fonction \\(f\\) sur un intervalle. De type interpolation , elle s'exprime comme une combinaison lin\u00e9aire de valeurs de \\(f\\) (pivots) et de poids. Le degr\u00e9 de pr\u00e9cision d'une formule de quadrature est le degr\u00e9 maximal des polyn\u00f4mes pouvant \u00eatre int\u00e9gr\u00e9s exactement. Les m\u00e9thodes de Newton-Cotes s'appuient sur des formules de quadrature de type interpolation de Lagrange . Les plus connues sont les m\u00e9thodes du point milieu , des trap\u00e8zes et de Simpson . Les pivots sont \u00e9quidistants et les poids ajust\u00e9s. Elles sont exactes pour les polyn\u00f4mes de degr\u00e9 \\(\\leq n+1\\) . Les formules de Newton-Cotes ne sont pas utilis\u00e9es au-del\u00e0 de l'ordre 6. On utilise leur version composite . La pr\u00e9cision d\u00e9pend alors de la taille des sous-intervalles. Les m\u00e9thodes de Gauss ajustent les pivots et les poids. Elles sont exactes pour les polyn\u00f4mes de degr\u00e9 \\(\\leq 2n+1\\) , et on les dit donc optimales . Leur th\u00e9orie / programmation est cependant plus lourde. NB : Pour les curieux, il existe aussi des m\u00e9thodes d'int\u00e9gration num\u00e9rique qui ne s'appuient pas sur des formules de quadrature. Par exemple, la m\u00e9thode de Monte-Carlo.","title":"Conclusions"},{"location":"Chap5_Systemes_lineaires/","text":"Chapitre V : R\u00e9solution de syst\u00e8mes d'\u00e9quations lin\u00e9aires Ce chapitre porte sur les m\u00e9thodes num\u00e9riques pour la r\u00e9solution d'un syst\u00e8me lin\u00e9aire d'\u00e9quations. Position du probl\u00e8me Motivation Notre but est de r\u00e9soudre un syst\u00e8me lin\u00e9aire : un ensemble d'\u00e9quations portant sur les m\u00eames inconnues. Un syst\u00e8me de \\(m\\) \u00e9quations lin\u00e9aires \u00e0 \\(n\\) inconnues peut s'\u00e9crire sous la forme suivante : \\(\\begin{cases} a_{1,1} x_1 + a_{1,2} x_2 + ... + a_{1,n} x_n = b_1\\\\ a_{2,1} x_1 + a_{2,2} x_2 + ... + a_{2,n} x_n = b_2\\\\ ...\\\\ a_{m,1} x_1 + a_{m,2} x_2 + ... + a_{m,n} x_n = b_m \\end{cases}\\) avec \\(x_1,x_2,...,x_n\\) les inconnues et \\(a_{i,j}\\) les coefficients du syst\u00e8me. On peut \u00e9galement \u00e9crire ce syst\u00e8me sous forme matricielle : \\(A x = b\\) avec \\(A\\) une matrice de coefficients r\u00e9els de taille \\(m \\times n\\) , \\(x\\) est un vecteur de taille \\(n\\) contenant les variables r\u00e9elles recherch\u00e9es, et \\(b\\) est un vecteur contenant \\(m\\) r\u00e9els. \\(A = \\begin{pmatrix} a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\ a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m,1} & a_{m,2} &\\cdots & a_{m,n} \\end{pmatrix}\\) \\(x = \\begin{pmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{pmatrix}\\) \\(b = \\begin{pmatrix} b_1\\\\ b_2\\\\ \\vdots\\\\ b_n \\end{pmatrix}\\) Si \\(m>n\\) , on dit le syst\u00e8me sur-d\u00e9termin\u00e9 . Si \\(m<n\\) , on dit le syst\u00e8me sous-d\u00e9termin\u00e9 . Solution, rang et d\u00e9terminant Face \u00e0 un syst\u00e8me lin\u00e9aire, il y a 3 cas possibles : Le syst\u00e8me n'a pas de solution. Le syst\u00e8me a une infinit\u00e9 de solution. Le syst\u00e8me a une solution unique. On peut savoir dans quel cas on se trouve avec le rang de la matrice \\(A\\) . D\u00e9finition Le rang d'une matrice \\(A\\) est son nombre de vecteurs lignes ou colonnes lin\u00e9airement ind\u00e9pendants. Si \\(A\\) est de dimensions \\(m \\times n\\) , alors \\(rang(A) \\leq min(m,n)\\) . Th\u00e9or\u00e8me de Rouch\u00e9-Fonten\u00e9 Le syst\u00e8me lin\u00e9aire \\(A x = b\\) avec \\(A\\) une matrice de taille \\(m \\times n\\) , \\(x\\) un vecteur de taille \\(n\\) , et \\(b\\) un vecteur de taille \\(m\\) , admet une solution si et seulement si : \\(rang(A) = rang([A \\mid b])\\) Si de plus, \\(rang(A) = n\\) , alors le syst\u00e8me admet une unique solution . Sinon, le syst\u00e8me admet une infinit\u00e9 de solutions. Dans le cas o\u00f9 la matrice \\(A\\) est carr\u00e9e, on a m\u00eame le th\u00e9or\u00e8me suivant : Th\u00e9or\u00e8me Lorsque la matrice \\(A\\) est carr\u00e9e de dimension \\(n \\times n\\) , avec \\(n\\) la taille du vecteur \\(x\\) , le syst\u00e8me lin\u00e9aire \\(A x = b\\) admet une unique solution si et seulement si : le d\u00e9terminant de \\(A\\) not\u00e9 \\(det(A)\\) est non nul. ( \\(A\\) est alors inversible, et on note \\(A^{-1}\\) son inverse) On dit alors que le syst\u00e8me est de Cramer et on peut \u00e9crire : \\(x = A^{-1} b\\) Le syst\u00e8me homog\u00e8ne \\(A x = 0\\) admet toujours le vecteur nul comme solution : Si \\(det(A) \\neq 0\\) c'est l'unique solution. Sinon il y en a une infinit\u00e9. Dans le cas d'un syst\u00e8me non homog\u00e8ne ( \\(b \\neq 0\\) ), si \\(det(A) = 0\\) : Soit \\(rang(A) = rang([A \\mid b])\\) alors il y a une infinit\u00e9 de solutions. Soit \\(rang(A) \\neq rang([A \\mid b])\\) alors il n'y a pas de solution. Conditionnement M\u00eame quand un syst\u00e8me lin\u00e9aire admet une solution unique, cette solution peut ne pas \u00eatre \"stable\". Un syst\u00e8me est dit \" mal conditionn\u00e9 \" si la solution est extr\u00eamement sensible aux perturbations des coefficients \\(A + \\Delta A\\) et des seconds membres \\(b + \\Delta b\\) . Un d\u00e9terminant petit est souvent indicateur d'un mauvais conditionnement. Pour quantifier la sensibilit\u00e9 de l'erreur relative sur la solution du syst\u00e8me lin\u00e9aire \\(A x = b\\) aux variation de \\(A\\) et \\(b\\) , on peut estimer le conditionnement de la matrice A : \\(\\kappa(A) = \\|A\\| \\|A^{-1}\\|\\) avec une norme matricielle \u00e0 d\u00e9finir. L'erreur relative sur la solution est inf\u00e9rieure \u00e0 l'erreur relative sur les donn\u00e9es multiplis\u00e9e par \\(\\kappa(A)\\) : Si \\(\\kappa(A)\\) est petit (de l'ordre de l'unit\u00e9) on dit que le conditionnement est bon. Si \\(\\kappa(A) >> 1\\) le syst\u00e8me est dit mal conditionn\u00e9. Le calcul du conditionnement d\u00e9pend du choix de la norme : La norme infinie : \\(\\|A\\|_{\\infty} = max_{1 \\leq i \\leq n} \\displaystyle\\sum_{j=1}^{n} |a_{i,j}|\\) La norme 1 : \\(\\|A\\|_1 = max_{1 \\leq j \\leq n} \\displaystyle\\sum_{i=1}^{n} |a_{i,j}|\\) La norme 2 : \\(\\|A\\|_2 = \\|A^T\\|_2 = \\sqrt{\\rho(A A^T)} = \\sqrt{\\rho(A^T A)}\\) \\(\\rho(A)\\) est le rayon spectral de \\(A\\) , que l'on d\u00e9finit comme : \\(\\rho(A) = max_{1 \\leq i \\leq n} |\\lambda_i|\\) avec \\(\\lambda_i\\) les valeurs propres de \\(A\\) . On utilisera surtout le conditionnement de \\(A\\) au sens de la norme 1 et de la norme 2 : \\(\\kappa_1(A) = \\|A\\|_1 \\|A^{-1}\\|_1\\) \\(\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2\\) Pour une matrice carr\u00e9e \\(A\\) d'ordre \\(n\\) inversible, le conditionnement v\u00e9rifie les propri\u00e9t\u00e9s suivantes : \\(\\kappa(A) \\geq 1\\) \\(\\forall \\alpha \\in \\mathbb{R}\\) , \\(\\kappa(\\alpha A) = \\kappa(A)\\) \\(\\kappa(A) = \\kappa(A^{-1})\\) Si on note \\(\\sigma_{min}^2\\) et \\(\\sigma_{max}^2\\) la plus petite et la plus grande valeur propre de \\(A A^T\\) : \\(\\kappa_2(A) = \\frac{\\sigma_{max}}{\\sigma_{min}}\\) Si \\(A\\) est une matrice r\u00e9elle sym\u00e9trique ( \\(A = A^T\\) ), si \\(\\lambda_{min}\\) et \\(\\lambda_{max}\\) sont la plus petite et la plus grande valeur propre de \\(A\\) en valeur absolue, on a : \\(\\kappa_2(A) = \\mid \\frac{\\lambda_{max}}{\\lambda_{min}} \\mid\\) Si \\(A\\) est une matrice orthogonale ( \\(A A^T = A^T A = I\\) ) alors \\(\\kappa_2(A) = 1\\) Effet d'une perturbation de \\(b\\) Si on a une perturbation \\(\\Delta b\\) sur \\(b\\) induisant une erreur \\(\\Delta x\\) sur \\(x\\) , alors on aura la majoration : \\(\\frac{\\Vert \\Delta x \\Vert}{\\Vert x \\Vert} \\leq \\kappa(A) \\frac{\\Vert \\Delta b \\Vert}{\\Vert b \\Vert}\\) Effet d'une perturbation de \\(A\\) Si on a une perturbation \\(\\Delta A\\) sur \\(A\\) induisant une erreur \\(\\Delta x\\) sur \\(x\\) , alors on aura la majoration : \\(\\frac{\\Vert \\Delta x \\Vert}{\\Vert x + \\Delta x \\Vert} \\leq \\kappa(A) \\frac{\\Vert \\Delta A \\Vert}{\\Vert A \\Vert}\\) Exemple de probl\u00e8me Au cours de ce chapitre, nous appliquerons les diff\u00e9rentes m\u00e9thodes d'int\u00e9gration \u00e0 un m\u00eame exemple : Le positionnement par satellites GPS . Nous exprimerons ici les positions en km, avec des coordonn\u00e9es dans le rep\u00e8re cart\u00e9sien ECEF (Earth-Centered Earth-Fixed), ayant pour origine le centre de la Terre. Soit un r\u00e9cepteur au sol dont on veut connaitre la position \\((x_r,y_r,z_r)\\) dans ce rep\u00e8re cart\u00e9sien. Soient 4 satellites de la constellation GPS, dont la position est connue dans ce m\u00eame rep\u00e8re : \\((x_{s1},y_{s1},z_{s1})\\) , \\((x_{s2},y_{s2},z_{s2})\\) , \\((x_{s3},y_{s3},z_{s3})\\) and \\((x_{s4},y_{s4},z_{s4})\\) . Chaque satellite \u00e9met un signal, qui est re\u00e7u avec un certain temps de retard par le r\u00e9cepteur. Ces temps de retard \\((t_1,t_2,t_3,t_4)\\) sont mesur\u00e9s par le r\u00e9cepteur. On admet que les signaux \u00e9mis par chaque satellite se d\u00e9placent \u00e0 vitesse constante jusqu'au r\u00e9cepteur : \\(c = 3.10^5 km/s\\) . La distance euclidienne entre chaque satellite et le r\u00e9cepteur doit \u00eatre \u00e9gale au temps de retard du signal multipli\u00e9 par sa vitesse. On en d\u00e9duit facilement que les diff\u00e9rentes variables sont li\u00e9es par le syst\u00e8me de 4 \u00e9quations suivant : \\(\\begin{cases} (x_r-x_{s1})^2 + (y_r-y_{s1})^2 + (z_r-z_{s1})^2 = (c t_1)^2\\\\ (x_r-x_{s2})^2 + (y_r-y_{s2})^2 + (z_r-z_{s2})^2 = (c t_2)^2\\\\ (x_r-x_{s3})^2 + (y_r-y_{s3})^2 + (z_r-z_{s3})^2 = (c t_3)^2\\\\ (x_r-x_{s4})^2 + (y_r-y_{s4})^2 + (z_r-z_{s4})^2 = (c t_4)^2 \\end{cases}\\) Que l'on peut d\u00e9velopper : \\(\\begin{cases} x_r^2 - 2 x_{s1} x_r + x_{s1}^2 + y_r^2 - 2 y_{s1} y_r + y_{s1}^2 + z_r^2 - 2 z_{s1} z_r + z_{s1}^2 = (c t_1)^2\\\\ x_r^2 - 2 x_{s2} x_r + x_{s2}^2 + y_r^2 - 2 y_{s2} y_r + y_{s2}^2 + z_r^2 - 2 z_{s2} z_r + z_{s2}^2 = (c t_2)^2\\\\ x_r^2 - 2 x_{s3} x_r + x_{s3}^2 + y_r^2 - 2 y_{s3} y_r + y_{s3}^2 + z_r^2 - 2 z_{s3} z_r + z_{s3}^2 = (c t_3)^2\\\\ x_r^2 - 2 x_{s4} x_r + x_{s4}^2 + y_r^2 - 2 y_{s4} y_r + y_{s4}^2 + z_r^2 - 2 z_{s4} z_r + z_{s4}^2 = (c t_4)^2 \\end{cases}\\) En soustrayant la 1\u00e8re \u00e9quation aux 3 autres, on r\u00e9duit le syst\u00e8me \u00e0 3 \u00e9quations : \\(\\begin{cases} x_{s2}^2 + y_{s2}^2 + z_{s2}^2 - 2 x_{s2} x_r - 2 y_{s2} y_r - 2 z_{s2} z_r - x_{s1}^2 - y_{s1}^2 - z_{s1}^2 + 2 x_{s1} x_r + 2 y_{s1} y_r + 2 z_{s1} z_r = (c t_2)^2-(c t_1)^2\\\\ x_{s3}^2 + y_{s3}^2 + z_{s3}^2 - 2 x_{s3} x_r - 2 y_{s3} y_r - 2 z_{s3} z_r - x_{s1}^2 - y_{s1}^2 - z_{s1}^2 + 2 x_{s1} x_r + 2 y_{s1} y_r + 2 z_{s1} z_r = (c t_3)^2-(c t_1)^2\\\\ x_{s4}^2 + y_{s4}^2 + z_{s4}^2 - 2 x_{s4} x_r - 2 y_{s4} y_r - 2 z_{s4} z_r - x_{s1}^2 - y_{s1}^2 - z_{s1}^2 + 2 x_{s1} x_r + 2 y_{s1} y_r + 2 z_{s1} z_r = (c t_4)^2-(c t_1)^2 \\end{cases}\\) Qui revient en regroupant les termes : \\(\\begin{cases} x_{s2}^2 - x_{s1}^2 + y_{s2}^2 - y_{s1}^2 + z_{s2}^2 - z_{s1}^2 - 2 (x_{s2}-x_{s1}) x_r - 2 (y_{s2}-y_{s1}) y_r - 2 (z_{s2}-z_{s1}) z_r = (c t_2)^2-(c t_1)^2\\\\ x_{s3}^2 - x_{s1}^2 + y_{s3}^2 - y_{s1}^2 + z_{s3}^2 - z_{s1}^2 - 2 (x_{s3}-x_{s1}) x_r - 2 (y_{s3}-y_{s1}) y_r - 2 (z_{s3}-z_{s1}) z_r = (c t_3)^2-(c t_1)^2\\\\ x_{s4}^2 - x_{s1}^2 + y_{s4}^2 - y_{s1}^2 + z_{s4}^2 - z_{s1}^2 - 2 (x_{s4}-x_{s1}) x_r - 2 (y_{s4}-y_{s1}) y_r - 2 (z_{s4}-z_{s1}) z_r = (c t_4)^2-(c t_1)^2 \\end{cases}\\) Et en r\u00e9arrangeant on obtient finalement le syst\u00e8me lin\u00e9aire : \\(\\begin{cases} (x_{s2}-x_{s1}) x_r + (y_{s2}-y_{s1}) y_r + (z_{s2}-z_{s1}) z_r = \\frac{1}{2} (x_{s2}^2 - x_{s1}^2 + y_{s2}^2 - y_{s1}^2 + z_{s2}^2 - z_{s1}^2 - (c t_2)^2 + (c t_1)^2)\\\\ (x_{s3}-x_{s1}) x_r + (y_{s3}-y_{s1}) y_r + (z_{s3}-z_{s1}) z_r = \\frac{1}{2} (x_{s3}^2 - x_{s1}^2 + y_{s3}^2 - y_{s1}^2 + z_{s3}^2 - z_{s1}^2 - (c t_3)^2 + (c t_1)^2)\\\\ (x_{s4}-x_{s1}) x_r + (y_{s4}-y_{s1}) y_r + (z_{s4}-z_{s1}) z_r = \\frac{1}{2} (x_{s4}^2 - x_{s1}^2 + y_{s4}^2 - y_{s1}^2 + z_{s4}^2 - z_{s1}^2 - (c t_4)^2 + (c t_1)^2) \\end{cases}\\) avec 3 \u00e9quations et 3 inconnues \\((x_r,y_r,z_r)\\) . On peut \u00e9crire ce syst\u00e8me sous la forme matricielle \\(A x = b\\) : \\(\\begin{pmatrix} x_{s2}-x_{s1} & y_{s2}-y_{s1} & z_{s2}-z_{s1} \\\\ x_{s3}-x_{s1} & y_{s3}-y_{s1} & z_{s3}-z_{s1} \\\\ x_{s4}-x_{s1} & y_{s4}-y_{s1} & z_{s4}-z_{s1} \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} (x_{s2}^2 - x_{s1}^2 + y_{s2}^2 - y_{s1}^2 + z_{s2}^2 - z_{s1}^2 - (c t_2)^2 + (c t_1)^2)\\\\ \\frac{1}{2} (x_{s3}^2 - x_{s1}^2 + y_{s3}^2 - y_{s1}^2 + z_{s3}^2 - z_{s1}^2 - (c t_3)^2 + (c t_1)^2)\\\\ \\frac{1}{2} (x_{s4}^2 - x_{s1}^2 + y_{s4}^2 - y_{s1}^2 + z_{s4}^2 - z_{s1}^2 - (c t_4)^2 + (c t_1)^2) \\end{pmatrix}\\) Admettons que le r\u00e9cepteur GPS se trouve aux coordonn\u00e9es ECEF \\((x_r,y_r,z_r) = (4205,158,4777)\\) , correspondant approximativement \u00e0 la position de l'UFR des Sciences de l'UVSQ. Si la position des 4 satellites est : \\((x_{s1},y_{s1},z_{s1}) = (14000,4000,25000)\\) \\((x_{s2},y_{s2},z_{s2}) = (9000,-14000,21000)\\) \\((x_{s3},y_{s3},z_{s3}) = (24000,6000,15000)\\) \\((x_{s4},y_{s4},z_{s4}) = (10000,16000,19000)\\) Alors le temps de retard associ\u00e9 \u00e0 chaque satellite sera approximativement : \\(t_1 = 0.0759878 s\\) \\(t_2 = 0.0735321 s\\) \\(t_3 = 0.0767739 s\\) \\(t_4 = 0.0735485 s\\) (Il est \u00e0 noter que les positions des satellites ont \u00e9t\u00e9 choisies pour \u00eatre r\u00e9alistes des satellites GPS). Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) C'est ce syst\u00e8me d'\u00e9quations lin\u00e9aires que nous chercherons \u00e0 r\u00e9soudre pour essayer de retrouver la position \\((x_r,y_r,z_r)\\) du r\u00e9cepteur. On peut d\u00e9j\u00e0 v\u00e9rifier que ce syst\u00e8me a bien une unique racine : \\(A\\) est carr\u00e9e de dimensions \\(3 \\times 3\\) , \\(x\\) et \\(b\\) sont de taille 3. \\(det(A) = (-5000 \\times 2000 \\times -6000) + (10000 \\times 12000 \\times -4000) + (-18000 \\times -10000 \\times -4000)\\) \\(- (-4000 \\times 2000 \\times -4000) - (-5000 \\times 12000 \\times -10000) - (10000 \\times -18000 \\times -6000)\\) \\(= -2852000000000 \\neq 0\\) Il s'agit donc d'un syst\u00e8me de Cramer : on a bien unicit\u00e9 de la solution . On peut \u00e9galement v\u00e9rifier si le syst\u00e8me est bien conditionn\u00e9 : \\(\\kappa_1(A) \\approx 5.03\\) \\(\\kappa_2(A) \\approx 2.36\\) Dans les 2 cas, le conditionnement est de l'ordre de l'unit\u00e9 : un a donc un bon conditionnement . Dans la suite de ce chapitre, on utilisera Numpy sous Python pour d\u00e9finir / manipuler les matrices \\(A\\) et \\(b\\) : import numpy as np On d\u00e9finira le vecteur de la position du r\u00e9cepteur GPS \u00e0 retrouver avec : pos_rec = np.array([4205,158,4777],dtype=np.float64) On d\u00e9finira ensuite les vecteurs de positions des satellites GPS avec : pos_sat1 = np.array([14000,4000,25000],dtype=np.float64) #Coordonn\u00e9es du satellite GPS 1 pos_sat2 = np.array([9000,-14000,21000],dtype=np.float64) #Coordonn\u00e9es du satellite GPS 2 pos_sat3 = np.array([24000,6000,15000],dtype=np.float64) #Coordonn\u00e9es du satellite GPS 3 pos_sat4 = np.array([10000,16000,19000],dtype=np.float64) #Coordonn\u00e9es du satellite GPS 4 On pourra alors en d\u00e9duire les temps de retard associ\u00e9s : t1 = ((sum((pos_rec-pos_sat1)**2))**0.5)/3e5 #Temps de retard associ\u00e9 au satellite GPS 1 t2 = ((sum((pos_rec-pos_sat2)**2))**0.5)/3e5 #Temps de retard associ\u00e9 au satellite GPS 2 t3 = ((sum((pos_rec-pos_sat3)**2))**0.5)/3e5 #Temps de retard associ\u00e9 au satellite GPS 3 t4 = ((sum((pos_rec-pos_sat4)**2))**0.5)/3e5 #Temps de retard associ\u00e9 au satellite GPS 4 Et pour finir, on pourra d\u00e9finir les matrices \\(A\\) et \\(b\\) du syst\u00e8me lin\u00e9aire \u00e0 r\u00e9soudre : A = np.vstack((pos_sat2-pos_sat1,pos_sat3-pos_sat1,pos_sat4-pos_sat1)) b_row1 = 0.5*(sum(pos_sat2**2)-sum(pos_sat1**2)-(3e5*t2)**2+(3e5*t1)**2) b_row2 = 0.5*(sum(pos_sat3**2)-sum(pos_sat1**2)-(3e5*t3)**2+(3e5*t1)**2) b_row3 = 0.5*(sum(pos_sat4**2)-sum(pos_sat1**2)-(3e5*t4)**2+(3e5*t1)**2) b = np.array([b_row1,b_row2,b_row3]) La r\u00e8gle de Cramer Th\u00e9or\u00e8me La r\u00e8gle de Cramer (ou m\u00e9thode de Cramer) est un th\u00e9or\u00e8me d'alg\u00e8bre lin\u00e9aire qui donne la solution d'un syst\u00e8me de Cramer : Th\u00e9or\u00e8me de la r\u00e8gle de Cramer Le syst\u00e8me de Cramer \\(A x = b\\) avec \\(A\\) matrice carr\u00e9e de taille \\(n \\times n\\) \\(x\\) vecteur de taille \\(n\\) \\(b\\) vecteur de taille \\(n\\) admet une solution si et seulement si \\(A\\) est inversible. Cette solution est donn\u00e9e par : \\(x_i = \\frac{det(A_i)}{det(A)}\\) pour \\(i=1,...,n\\) o\u00f9 \\(A_i\\) est la matrice carr\u00e9e form\u00e9e en rempla\u00e7ant la i-\u00e8me colonne de \\(A\\) par le vecteur \\(b\\) . Lorsque le syst\u00e8me n'est pas de Cramer (donc si \\(det(A)=0\\) ) : Si le d\u00e9terminant d'une des matrices \\(A_i\\) est nul alors le syst\u00e8me n'a pas de solution. La r\u00e9ciproque est fausse : il peut arriver qu'un syst\u00e8me n'ait pas de solution alors que tous les \\(det(A_i)\\) sont non-nuls. Cette m\u00e9thode est tr\u00e8s couteuse en nombre d'op\u00e9rations et devient donc inapplicable \u00e0 de grands syst\u00e8mes (plus de 4 \u00e9quations). Algorithme (n=3) Dans cette section, nous pr\u00e9senterons les algorithmes permettant d'appliquer la m\u00e9thode de Cramer dans le cas d'une matrice de dimensions \\((3 \\times 3)\\) : 3 \u00e9quations et 3 inconnues. Le d\u00e9terminant d'une matrice de dimensions \\(3 \\times 3\\) peut \u00eatre calcul\u00e9 \u00e0 l'aide de la fonction Python suivante. Cette fonction prend en entr\u00e9e : A la matrice de dimensions \\(3 \\times 3\\) dont on veut trouver le d\u00e9terminant. Elle se base simplement sur la formule du d\u00e9terminant d'une matrice \\(3 \\times 3\\) . def det_3(A): return A[0,0]*A[1,1]*A[2,2]+A[0,1]*A[1,2]*A[2,0]+A[0,2]*A[1,0]*A[2,1]-A[0,2]*A[1,1]*A[2,0]-A[0,1]*A[1,0]*A[2,2]-A[0,0]*A[1,2]*A[2,1] Voici l'algorithme de Cramer pour une matrice de dimensions \\(3 \\times 3\\) sous la forme d'une fonction Python. Cette fonction prend en entr\u00e9e : A la matrice de dimensions \\(3 \\times 3\\) des coefficients du syst\u00e8me. b le vecteur de dimension 3 du second membre du syst\u00e8me. def cramer_3(A,b): #V\u00e9rification des dimensions de A (3x3) et b (3) : if (np.shape(A)!=(3,3))or(len(b)!=3): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Calculer le d\u00e9terminant de A : det_A = det_3(A) #V\u00e9rifier que le syst\u00e8me admet bien une unique solution : if det_A==0: raise ValueError(\"Le syst\u00e8me n'admet pas une solution unique !\") #Initialiser le vecteur qui contiendra les 3 solutions du syst\u00e8me : x = np.array([0,0,0],dtype=np.float64) #Boucle sur les 3 colonnes de la matrice A : for i in range(3): #Remplir la matrice A_i avec les \u00e9l\u00e9ments de A : A_i = np.copy(A) #Remplacer la i-\u00e8me colonne de A_i avec les \u00e9l\u00e9ments de b : A_i[:,i] = b #Calculer la valeur de la i-\u00e8me inconnue du syst\u00e8me : x[i] = det_3(A_i)/det_A #Renvoyer le vecteur contenant les 3 solutions du syst\u00e8me : return x Exemple Avant d'appliquer la m\u00e9thode Cramer \u00e0 notre probl\u00e8me exemple, il convient de v\u00e9rifier que celle-ci est bien applicable. On rappelle que nous avons montr\u00e9 pr\u00e9c\u00e9demment que nous avons ici affaire \u00e0 un syst\u00e8me de Cramer car : \\(A\\) est carr\u00e9e de dimensions \\(3 \\times 3\\) , \\(x\\) et \\(b\\) sont de taille 3. \\(det(A) = -2852000000000 \\neq 0\\) La solution est par cons\u00e9quent unique, et nous pouvons appliquer la m\u00e9thode de Cramer. Tout d'abord, nous construisons \\(A_1\\) , \\(A_2\\) et \\(A_3\\) : \\(A_1 = \\begin{pmatrix} -42977000 & -18000 & -4000 \\\\ -5404000 & 2000 & -10000 \\\\ -43586000 & 12000 & -6000 \\end{pmatrix}\\) \\(A_2 = \\begin{pmatrix} -5000 & -42977000 & -4000 \\\\ 10000 & -5404000 & -10000 \\\\ -4000 & -43586000 & -6000 \\end{pmatrix}\\) \\(A_3 = \\begin{pmatrix} -5000 & -18000 & -42977000 \\\\ 10000 & 2000 & -5404000 \\\\ -4000 & 12000 & -43586000 \\end{pmatrix}\\) On peut alors calculer que : \\(det(A_1) = -11992660000000000\\) \\(det(A_2) = -450616000000000\\) \\(det(A_3) = -13624004000000000\\) On en d\u00e9duit que : \\(x_r = \\frac{det(A_1)}{det(A)} = \\frac{-11992660000000000}{-2852000000000} = 4205\\) \\(y_r = \\frac{det(A_2)}{det(A)} = \\frac{-450616000000000}{-2852000000000} = 158\\) \\(z_r = \\frac{det(A_3)}{det(A)} = \\frac{-13624004000000000}{-2852000000000} = 4777\\) Exercice : Introduisez une erreur de 10 km dans les valeurs de \\(A\\) , et appliquez de nouveau la m\u00e9thode de Cramer au syst\u00e8me. Comment ces erreurs se r\u00e9percutent-elles sur l'estimation de \\((x_r,y_r,z_r)\\) ? Ce r\u00e9sultat \u00e9tait-il attendu d'apr\u00e8s le conditionnement de \\(A\\) ? M\u00e9thodes directes d'\u00e9limination Propri\u00e9t\u00e9s des syst\u00e8mes lin\u00e9aires Les m\u00e9thodes dites \"d' \u00e9limination \" pour la r\u00e9solution de syst\u00e8mes lin\u00e9aires se basent sur 4 grandes propri\u00e9t\u00e9s de ces syst\u00e8mes. La solution d'un syst\u00e8me lin\u00e9aire \\(A x = b\\) reste inchang\u00e9e lorsque l'on applique les op\u00e9rations suivantes : Permutation de lignes Permuter 2 lignes de \\(A\\) et les \u00e9l\u00e9ments correspondants de \\(b\\) revient \u00e0 permuter 2 \u00e9quations. Permutation de colonnes Permuter 2 colonnes de \\(A\\) et les \u00e9l\u00e9ments correspondants de \\(x\\) revient \u00e0 permuter 2 inconnues. Addition d'une ligne \u00e0 une autre Ajouter une ligne de \\(A\\) \u00e0 une autre, et ajouter les \u00e9l\u00e9ments correspondants de \\(b\\) , revient \u00e0 additionner une \u00e9quation \u00e0 une autre. Multiplication d'une ligne par un r\u00e9el non nul Multiplier une ligne de \\(A\\) et les \u00e9l\u00e9ments correspondants de \\(b\\) par un r\u00e9el non nul revient \u00e0 multipler une \u00e9quation par ce r\u00e9el. L'id\u00e9e derri\u00e8re les m\u00e9thodes d'\u00e9limination est d'utiliser ces op\u00e9ration pour construire une matrice \\(A^*\\) modifi\u00e9e, triangulaire ou diagonale , afin de se ramener \u00e0 un syst\u00e8me simple \u00e0 r\u00e9soudre . Pivot de Gauss Id\u00e9e L'algorithme du pivot de Gauss a pour but de transformer le syst\u00e8me en un syst\u00e8me triangulaire \u00e0 l'aide d'op\u00e9rations sur les lignes (et \u00e9ventuellement sur les colonnes). Il s'agit donc d'une m\u00e9thode de triangularisation . Une fois la matrice triangularis\u00e9e, le syst\u00e8me \u00e0 r\u00e9soudre devient : \\(\\begin{cases} a_{1,1}^* x_1 + a_{1,2}^* x_2 + ... + a_{1,n}^* x_n = b_1^*\\\\ a_{2,2}^* x_2 + a_{2,3}^* x_3 + ... + a_{2,n}^* x_n = b_2^*\\\\ ...\\\\ a_{n-1,n-1}^* x_{n-1} + a_{n-1,n}^* x_n = b_{n-1}^*\\\\ a_{n,n}^* x_n = b_n^* \\end{cases}\\) o\u00f9 les \\(a_{i,j}*\\) sont les coefficients de la matrice modifi\u00e9e \\(A^*\\) , et les \\(b_i^*\\) les \u00e9l\u00e9ments du vecteur modifi\u00e9 \\(b^*\\) . Pour r\u00e9soudre ce syst\u00e8me, il suffit alors d'effectuer les calculs de \" remont\u00e9e \" suivants : \\(\\begin{cases} x_n = \\frac{b_n^*}{a_{n,n}^*}\\\\ x_{n-1} = \\frac{1}{a_{n-1,n-1}^*} (b_{n-1}^* - a_{n-1,n}^* x_n)\\\\ ...\\\\ x_i = \\frac{1}{a_{i,i}^*} (b_i^* - \\displaystyle\\sum_{j=i+1}^{n} a_{i,j}^* x_j)\\\\ ...\\\\ x_1 = \\frac{1}{a_{1,1}^*} (b_1^* - \\displaystyle\\sum_{j=2}^{n} a_{1,j}^* x_j) \\end{cases}\\) Pour triangulariser la matrice \\(A\\) , on r\u00e9p\u00e8te ces op\u00e9rations pour chaque colonne \\(j\\) : Op\u00e9rations du pivot de Gauss - On choisit une valeur non-nulle dans la colonne \\(j\\) , d'indice sup\u00e9rieur ou \u00e9gal \u00e0 \\(j\\) , que l'on appellera pivot . - On ram\u00e8ne le pivot sur la ligne \\(j\\) en effectuant si n\u00e9cessaire un changement de ligne. - On effectue les op\u00e9rations suivantes sur les lignes d'indice \\(j < k \\leq n\\) : \\(L_k = L_k - \\frac{a_{k,j}}{a_{jj}} L_j\\) On passe \u00e0 la colonne suivante, jusqu'\u00e0 l'avant-derni\u00e8re. Pour r\u00e9duire les erreurs li\u00e9es aux arrondis, on peut adopter plusieurs strat\u00e9gies pour le choix du pivot : Sans pivotage : on ne r\u00e9alise ni permutations de lignes, ni permutations de colonnes. Le pivot est toujours s\u00e9l\u00e9ctionn\u00e9 sur la diagonale de la matrice. Le pivot partiel : on choisi le pivot comme \u00e9tant l'\u00e9l\u00e9ment de valeur absolue maximale de la colonne sur ou sous la diagonale. Cette strat\u00e9gie n'implique que des permutations de lignes. Le pivot total : on choisi le pivot comme \u00e9tant l'\u00e9l\u00e9ment de valeur absolue maximale sur toute la portion de matrice non-triangularis\u00e9e. Cette strat\u00e9gie implique des permutations de lignes et de colonnes. Choisir le pivot le plus grand possible assure que les coefficients de \\(A\\) et \\(A^*\\) soient de m\u00eame magnitude relative , r\u00e9duisant ainsi la propagation des erreurs d'arrondis. L'algorithme du pivot partiel est le plus commun\u00e9ment utilis\u00e9. La triangularisation d'une matrice \\(A\\) de dimensions \\(n \\times n\\) requiert de l'ordre de \\(\\frac{2 n^3}{3}\\) op\u00e9rations. La remont\u00e9e requiert de l'ordre de \\(n^2\\) op\u00e9rations. Algorithmes Voici sous la forme de fonctions Python les algorithmes d'\u00e9limination de Gauss sans pivotage, avec pivot partiel et avec pivot total. Toutes ces fonctions prennent en entr\u00e9e un syst\u00e8me de Cramer : A la matrice des coefficients du syst\u00e8me. b le vecteur du second membre du syst\u00e8me. Voici l'algorithme d'\u00e9limination de Gauss sans pivotage : def gauss_sans_pivot(A,b): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Copier A et b pour ne pas modifier les matrices originales : A_2 = np.copy(A) b_2 = np.copy(b) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur sur la diagonale de la j-\u00e8me colonne : pivot = A_2[j,j] #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Op\u00e9rations sur les lignes de A et b en utilisant le pivot : b_2[k] = b_2[k] - b_2[j]*A_2[k,j]/pivot A_2[k,:] = A_2[k,:] - A_2[j,:]*A_2[k,j]/pivot #Renvoyer les matrices A et b modifi\u00e9es : return A_2,b_2 Voici l'algorithme d'\u00e9limination de Gauss avec pivot partiel : def gauss_pivot_partiel(A,b): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Copier A et b pour ne pas modifier les matrices originales : A_2 = np.copy(A) b_2 = np.copy(b) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur maximale en absolu sur la colonne, #sur la j-i\u00e8me ligne ou en dessous : idx_pivot = np.argmax(abs(A_2[j:,j]))+j #Indice de la ligne du pivot pivot = A_2[idx_pivot,j] #Valeur du pivot #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Si le pivot n'est pas sur la j-i\u00e8me ligne, \u00e9changer la j-i\u00e8me et la #ligne du pivot : if idx_pivot!=j: A_2[[j,idx_pivot]] = A_2[[idx_pivot,j]] #Pour la matrice A b_2[[j,idx_pivot]] = b_2[[idx_pivot,j]] #Pour le vecteur b #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Op\u00e9rations sur les lignes de A et b en utilisant le pivot : b_2[k] = b_2[k] - b_2[j]*A_2[k,j]/pivot A_2[k,:] = A_2[k,:] - A_2[j,:]*A_2[k,j]/pivot #Renvoyer les matrices A et b modifi\u00e9es : return A_2,b_2 Voici l'algorithme d'\u00e9limination de Gauss avec pivot total : def gauss_pivot_total(A,b): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Copier A et b pour ne pas modifier les matrices originales : A_2 = np.copy(A) b_2 = np.copy(b) #Cr\u00e9er un vecteur contenant l'ordre des inconnues dans x (de 0 \u00e0 n-1) : idx_x = np.arange(n) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur maximale en absolu sur la #portion de matrice non-triangularis\u00e9e : idx_pivot = np.argmax(abs(A_2[j:,j:])) #Indice du pivot ligne_pivot = idx_pivot//(n-j)+j colonne_pivot = idx_pivot%(n-j)+j pivot = A_2[ligne_pivot,colonne_pivot] #Valeur du pivot #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Si le pivot n'est pas sur la j-i\u00e8me ligne, \u00e9changer la j-i\u00e8me et la #ligne du pivot : if ligne_pivot!=j: A_2[[j,ligne_pivot]] = A_2[[ligne_pivot,j]] #Pour la matrice A b_2[[j,ligne_pivot]] = b_2[[ligne_pivot,j]] #Pour le vecteur b #Si le pivot n'est pas sur la j-i\u00e8me colonne, \u00e9changer la j-i\u00e8me et la #colonne du pivot : if colonne_pivot!=j: A_2[:,[j,colonne_pivot]] = A_2[:,[colonne_pivot,j]] #Pour la matrice A idx_x[[j,colonne_pivot]] = idx_x[[colonne_pivot,j]] #Pour les \u00e9l\u00e9ments de x #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Op\u00e9rations sur les lignes de A et b en utilisant le pivot : b_2[k] = b_2[k] - b_2[j]*A_2[k,j]/pivot A_2[k,:] = A_2[k,:] - A_2[j,:]*A_2[k,j]/pivot #D\u00e9terminer les indices de x permettant d'obtenir les inconnues dans l'ordre : ordre_x = np.argsort(idx_x) #Renvoyer les matrices A et b modifi\u00e9es, et l'ordre des inconnues : return A_2,b_2,ordre_x Une fois, le syst\u00e8me triangularis\u00e9 par une des m\u00e9thodes d'\u00e9limination de Gauss, il faut lui appliquer l'algorithme de remont\u00e9e pour le r\u00e9soudre. Voici l'algorithme de remont\u00e9e sous la forme d'une fonction Python : def remontee(A,b): #R\u00e9cup\u00e9rer le nombre n d'\u00e9quations / inconnues du syst\u00e8me : n = len(A) #Initialiser le vecteur qui contiendra les solutions du syst\u00e8me : x = np.zeros(n,dtype=np.float64) #Boucle sur les lignes de la matrice A, de n \u00e0 1 : for i in range(n-1,-1,-1): #D\u00e9termination de la i-\u00e8me inconnue : x[i] = (b[i]-sum(A[i,i+1:n]*x[i+1:n]))/A[i,i] #Renvoyer le vecteur contenant les solutions du syst\u00e8me : return x Dans le cas du pivot total, pour r\u00e9cup\u00e9rer les inconnues dans l'ordre, il faudra utiliser le vecteur retourn\u00e9 par la fonction \"gauss_pivot_total\" : A_2,b_2,ordre_x = gauss_pivot_total(A,b) x_2 = remontee(A_2,b_2) x_2 = x_2[ordre_x] Exemple Nous allons appliquer chacun des 3 algorithmes d'\u00e9limination de Gauss (sans pivotage, avec pivot partiel, avec pivot total) \u00e0 notre probl\u00e8me exemple. On rappelle que nous avons initialement le syst\u00e8me de Cramer \\(A x = b\\) suivant : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) Sans pivotage : 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant sur la diagonale : -5000. On r\u00e9alise les op\u00e9rations suivantes : \\(L_2 = L_2 - L_1 \\times \\frac{10000}{-5000}\\) \\(L_3 = L_3 - L_1 \\times \\frac{-4000}{-5000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 0 & -34000 & -18000 \\\\ 0 & 26400 & -2800 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -91358000\\\\ -9204400 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9l\u00e9ctionne le pivot comme \u00e9tant sur la diagonale : -34000. On r\u00e9alise l'op\u00e9ration suivante : \\(L_3 = L_3 - L_2 \\times \\frac{26400}{-34000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 0 & -34000 & -18000 \\\\ 0 & 0 & -16776.47 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -91358000\\\\ -80141200 \\end{pmatrix}\\) On obtient bien un syst\u00e8me triangulaire auquel on peut appliquer l'algorithme de remont\u00e9e. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On en d\u00e9duit alors les solutions par l'algorithme de remont\u00e9e : \\(\\begin{cases} z_r = \\frac{-80141200}{-16776.47} = 4777\\\\ y_r = \\frac{1}{-34000} (-91358000 - (-18000 \\times z_r)) = 158\\\\ x_r = \\frac{1}{-5000} (-42977000 - (-18000 \\times y_r) - (-4000 \\times z_r)) = 4205 \\end{cases}\\) Pivot partiel : 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : 10000. On \u00e9change la ligne 1 et la ligne 2 pour faire passer le pivot sur la diagonale. Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 10000 & 2000 & -10000 \\\\ -5000 & -18000 & -4000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -5404000\\\\ -42977000\\\\ -43586000 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_2 = L_2 - L_1 \\times \\frac{-5000}{10000}\\) \\(L_3 = L_3 - L_1 \\times \\frac{-4000}{10000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 10000 & 2000 & -10000 \\\\ 0 & -17000 & -9000 \\\\ 0 & 12800 & -10000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -5404000\\\\ -45679000\\\\ -45747600 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : -17000. On r\u00e9alise l'op\u00e9ration suivante : \\(L_3 = L_3 - L_2 \\times \\frac{12800}{-17000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 10000 & 2000 & -10000 \\\\ 0 & -17000 & -9000 \\\\ 0 & 0 & -16776.47 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -5404000\\\\ -45679000\\\\ -80141200 \\end{pmatrix}\\) On obtient bien un syst\u00e8me triangulaire auquel on peut appliquer l'algorithme de remont\u00e9e. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On en d\u00e9duit alors les solutions par l'algorithme de remont\u00e9e : \\(\\begin{cases} z_r = \\frac{-80141200}{-16776.47} = 4777\\\\ y_r = \\frac{1}{-17000} (-45679000 - (-9000 \\times z_r)) = 158\\\\ x_r = \\frac{1}{10000} (-5404000 - (2000 \\times y_r) - (-10000 \\times z_r)) = 4205 \\end{cases}\\) Pivot total : 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la portion de matrice non-triangularis\u00e9e : -18000. On \u00e9change la colonne 1 et la colonne 2 pour faire passer le pivot sur la diagonale. Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -18000 & -5000 & -4000 \\\\ 2000 & 10000 & -10000 \\\\ 12000 & -4000 & -6000 \\end{pmatrix} \\begin{pmatrix} y_r\\\\ x_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_2 = L_2 - L_1 \\times \\frac{2000}{-18000}\\) \\(L_3 = L_3 - L_1 \\times \\frac{12000}{-18000}\\) \\(\\begin{pmatrix} -18000 & -5000 & -4000 \\\\ 0 & 9444.44 & -10444.44 \\\\ 0 & -7333.33 & -8666.67 \\end{pmatrix} \\begin{pmatrix} y_r\\\\ x_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -10179222.22\\\\ -72237333.33 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la portion de matrice non-triangularis\u00e9e : -10444.44. On \u00e9change la colonne 2 et la colonne 3 pour faire passer le pivot sur la diagonale. Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -18000 & -4000 & -5000 \\\\ 0 & -10444.44 & 9444.44 \\\\ 0 & -8666.67 & -7333.33 \\end{pmatrix} \\begin{pmatrix} y_r\\\\ z_r\\\\ x_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -10179222.22\\\\ -72237333.33 \\end{pmatrix}\\) On r\u00e9alise l'op\u00e9ration suivante : \\(L_3 = L_3 - L_2 \\times \\frac{-8666.67}{-10444.44}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -18000 & -4000 & -5000 \\\\ 0 & -10444.44 & 9444.44 \\\\ 0 & 0 & -15170.21 \\end{pmatrix} \\begin{pmatrix} y_r\\\\ z_r\\\\ x_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -10179222.22\\\\ -63790744.68 \\end{pmatrix}\\) On obtient bien un syst\u00e8me triangulaire auquel on peut appliquer l'algorithme de remont\u00e9e. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On en d\u00e9duit alors les solutions par l'algorithme de remont\u00e9e : \\(\\begin{cases} x_r = \\frac{-63790744.68}{-15170.21} = 4205\\\\ z_r = \\frac{1}{-10444.44} (-10179222.22 - (9444.44 \\times x_r)) = 4777\\\\ y_r = \\frac{1}{-18000} (-63790744.68 - (-4000 \\times z_r) - (-5000 \\times x_r)) = 158 \\end{cases}\\) Exercice : Pourquoi choisir le pivot le plus grand possible ? Dans le cas de notre probl\u00e8me exemple, les coefficients de \\(A\\) sont tous du m\u00eame ordre de grandeur, et nous n'avons pas de gros probl\u00e8mes d'arrondis. Le r\u00e9sutat obtenu est donc le m\u00eame pour les 3 m\u00e9thodes d'\u00e9limination de Gauss (sans pivotage, avec pivot partiel, avec pivot total). Essayez \u00e0 la main d'appliquer au syst\u00e8me suivant l'\u00e9limination de Gauss sans pivotage puis avec pivot partiel, pour 4 chiffres significatifs sur tous les calculs : \\(\\begin{cases} 1.308 x_1 + 4.951 x_2 = 6.259\\\\ 27.05 x_1 + 1.020 x_2 = 28.07 \\end{cases}\\) La solution de ce syst\u00e8me est \u00e9vidente : \\(x_1 = 1.000\\) et \\(x_2 = 1.000\\) . Retrouvez-vous ce r\u00e9sultat avec la m\u00e9thode sans pivotage ? Avec pivot partiel ? Quelle est la cause de cette diff\u00e9rence ? Elimination de Gauss-Jordan Id\u00e9e L'algorithme de Gauss-Jordan a pour but de transformer le syst\u00e8me en un syst\u00e8me \u00e9chelonn\u00e9 r\u00e9duit \u00e0 l'aide d'op\u00e9rations \u00e9l\u00e9mentaires sur les lignes (et \u00e9ventuellement sur les colonnes). L'id\u00e9e est de pousser plus loin les \u00e9liminations que la m\u00e9thode de Gauss, pour construire une matrice \\(A^*\\) de la forme : \\(\\begin{pmatrix} 1 & * & 0 & 0 & * & 0\\\\ 0 & 0 & 1 & 0 & * & 0\\\\ 0 & 0 & 0 & 1 & * & 0\\\\ 0 & 0 & 0 & 0 & 0 & 1\\\\ 0 & 0 & 0 & 0 & 0 & 0\\\\ \\end{pmatrix}\\) Il faut donc ajouter \u00e0 l'\u00e9limination de Gauss vue pr\u00e9c\u00e9demment : Une division de la ligne du pivot par le pivot, pour que le pivot soit \u00e9gal \u00e0 1. Des op\u00e9rations sur les lignes au-dessus de la ligne du pivot. Il s'agit d'une m\u00e9thode de diagonalisation . Si la matrice \\(A\\) est carr\u00e9e inversible de taille \\(n \\times n\\) , sa forme \u00e9chelonn\u00e9e r\u00e9duite est la matrice identit\u00e9 de taille \\(n \\times n\\) . Le nombre d'op\u00e9rations de l'algorithme de Gauss-Jordan est de l'ordre de \\(n^3\\) au lieu de \\(\\frac{2}{3} n^3\\) pour l'\u00e9limination de Gauss. Mais avec l'\u00e9limination de Gauss-Jordan, la r\u00e9solution du syst\u00e8me est imm\u00e9diate : la solution est directement \\(x = b^*\\) . Algorithme Comme pour l'\u00e9limination de Gauss, l'\u00e9limination de Gauss-Jordan peut se d\u00e9cliner sous 3 formes suivant la strat\u00e9gie choix du pivot : sans pivotage, avec pivot partiel, avec pivot total. Nous donnerons ici l'algorithme de l'\u00e9limination de Gauss-Jordan avec pivot partiel, qui est le plus commun\u00e9ment utilis\u00e9. Le voici sous la forme d'une fonction Python, qui prend en entr\u00e9e un syst\u00e8me de Cramer : A la matrice des coefficients du syst\u00e8me. b le vecteur du second membre du syst\u00e8me. def gauss_jordan(A,b): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Copier A et b pour ne pas modifier les matrices originales : A_2 = np.copy(A) b_2 = np.copy(b) #Boucle sur les colonnes de la matrice A : for j in range(n): #S\u00e9lection du pivot comme \u00e9tant la valeur maximale en absolu sur la colonne, #sur la j-i\u00e8me ligne ou en dessous : idx_pivot = np.argmax(abs(A_2[j:,j]))+j #Indice de la ligne du pivot pivot = A_2[idx_pivot,j] #Valeur du pivot #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Division de la ligne du pivot par le pivot, pour que le pivot soit \u00e9gal \u00e0 1 : A_2[idx_pivot,:] = A_2[idx_pivot,:]/pivot #Pour la matrice A b_2[idx_pivot] = b_2[idx_pivot]/pivot #Pour le vecteur b #Si le pivot n'est pas sur la j-i\u00e8me ligne, \u00e9changer la j-i\u00e8me et la #ligne du pivot : if idx_pivot!=j: A_2[[j,idx_pivot]] = A_2[[idx_pivot,j]] #Pour la matrice A b_2[[j,idx_pivot]] = b_2[[idx_pivot,j]] #Pour le vecteur b #Boucle sur toutes les lignes sauf celle du pivot : for k in range(n): if k!=j: #Op\u00e9rations sur les lignes de A et b : b_2[k] = b_2[k] - b_2[j]*A_2[k,j] A_2[k,:] = A_2[k,:] - A_2[j,:]*A_2[k,j] #Renvoyer les matrices A et b modifi\u00e9es : return A_2,b_2 Il n'y a pas besoin d'un algorithme de remont\u00e9e ici, puisque les solutions seront directement les valeurs du vecteur \"b_2\" en sortie. Exemple Nous allons appliquer l'algorithme d'\u00e9limination de Gauss-Jordan (avec pivot partiel) \u00e0 notre probl\u00e8me exemple. On rappelle que nous avons initialement le syst\u00e8me de Cramer \\(A x = b\\) suivant : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : 10000. On divise la ligne du pivot par le pivot : \\(L_2 = \\frac{L_2}{10000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 1 & 0.2 & -1 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -540.4\\\\ -43586000 \\end{pmatrix}\\) On \u00e9change la ligne 1 et la ligne 2 pour faire passer le pivot sur la diagonale. Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0.2 & -1 \\\\ -5000 & -18000 & -4000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -540.4\\\\ -42977000\\\\ -43586000 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_2 = L_2 - L_1 \\times -5000\\) \\(L_3 = L_3 - L_1 \\times -4000\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0.2 & -1 \\\\ 0 & -17000 & -9000 \\\\ 0 & 12800 & -10000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -540.4\\\\ -45679000\\\\ -45747600 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : -17000. On divise la ligne du pivot par le pivot : \\(L_2 = \\frac{L_2}{-17000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0.2 & -1 \\\\ 0 & 1 & 0.5294 \\\\ 0 & 12800 & -10000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -540.4\\\\ 2687\\\\ -45747600 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_1 = L_1 - L_2 \\times 0.2\\) \\(L_3 = L_3 - L_2 \\times 12800\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0 & -1.1059 \\\\ 0 & 1 & 0.5294 \\\\ 0 & 0 & -16776.47 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -1077.8\\\\ 2687\\\\ -80141200 \\end{pmatrix}\\) 3\u00e8me it\u00e9ration : nous continuons avec la colonne 3. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : -16776.47. On divise la ligne du pivot par le pivot : \\(L_3 = \\frac{L_3}{-16776.47}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0 & -1.1059 \\\\ 0 & 1 & 0.5294 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -1077.8\\\\ 2687\\\\ 4777 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_1 = L_1 - L_3 \\times -1.1059\\) \\(L_2 = L_2 - L_3 \\times 0.5294\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} 4205\\\\ 158\\\\ 4777 \\end{pmatrix}\\) On trouve alors directement la solution du syst\u00e8me : \\(x = \\begin{pmatrix} 4205\\\\ 158\\\\ 4777 \\end{pmatrix}\\) Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : Exercice : En vous inspirant des fonctions Python pr\u00e9c\u00e9dentes, impl\u00e9mentez la m\u00e9thode de Gauss-Jordan avec pivot total, puis appliquez-la \u00e0 notre probl\u00e8me exemple. V\u00e9rifiez que vous retrouvez bien le r\u00e9sultat attendu. M\u00e9thodes directes de factorisation / d\u00e9composition Equivalence \u00e9limination - factorisation Les op\u00e9rations d'\u00e9limination : On remarque qu'appliquer l'op\u00e9ration \\(L_2 = L_2 - \\frac{a_{2,1}}{a_{1,1}} L_1\\) \u00e0 un syst\u00e8me \\(A x = b\\) de dimensions \\(n \\times n\\) revient \u00e0 multiplier \\(A\\) et \\(b\\) par la matrice : \\(M = \\begin{pmatrix} 1 & 0 & 0 \\\\ -a_{2,1}/a_{1,1} & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) Si on notre \\(A^{(k)}\\) et \\(b^{(k)}\\) les matrices avant l'op\u00e9ration, et \\(A^{(k+1)}\\) et \\(b^{(k+1)}\\) les matrices apr\u00e8s l'op\u00e9ration, on a : \\(A^{(k+1)} = M A^{(k)}\\) et \\(b^{(k+1)} = M b^{(k)}\\) Les permutations : On remarque que permutter les lignes \\(L_2\\) et \\(L_3\\) d'un syst\u00e8me \\(A x = b\\) de dimension \\(n \\times n\\) revient \u00e0 multiplier \\(A\\) et \\(b\\) par la matrice : \\(P = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}\\) Si on notre \\(A^{(k)}\\) et \\(b^{(k)}\\) les matrices avant l'op\u00e9ration, et \\(A^{(k+1)}\\) et \\(b^{(k+1)}\\) les matrices apr\u00e8s l'op\u00e9ration, on a : \\(A^{(k+1)} = P A^{(k)}\\) et \\(b^{(k+1)} = P b^{(k)}\\) Equivalence Les algorithmes d'\u00e9limination de Gauss peuvent donc s'\u00e9crire comme une succession de multiplications par : - Des matrices triangulaires \\(M\\) . - Des matrices de permutation \\(P\\) . La repr\u00e9sentation matricelle de l'\u00e9limation de Gauss : On peut voir l'algorithme de l'\u00e9limination de Gauss comme la transformation d'un syst\u00e8me \\(A x = b\\) en un syst\u00e8me \\(U x = c\\) avec \\(U\\) une matrice triangulaire sup\u00e9rieure . On a vu qu'\u00e0 chaque \u00e9tape \\(k\\) , les op\u00e9rations de l'algorithme sont \u00e9quivalentes \u00e0 multiplier \\(A^{(k)}\\) et \\(b^{(k)}\\) par une matrice du type : \\(M_k = \\begin{pmatrix} 1 & 0 & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & 1 & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & m_{k+1,k} & 0 & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & m_{n,k} & 0 & \\cdots & 0 & 1 \\end{pmatrix}\\) avec les \\(m_{i,k} = - \\frac{a_{i,k}}{a_{k,k}}\\) pour \\(i>k\\) . A la 1\u00e8re \u00e9tape on a \\(A^{(1)}=A\\) , et \u00e0 la derni\u00e8re \u00e9tape on a \\(A^{(n)}=U\\) . On en d\u00e9duit que l'on peut exprimer \\(U\\) comme : \\(U = A^{(n)} = M_{n-1} A^{(n-1)} = M_{n-1} M_{n-2} A^{(n-2)} = ... = M_{n-1} M_{n-2} ... M_1 A^{(1)} = M A\\) en notant \\(M = M_{n-1} M_{n-2} ... M_1\\) Si on pose \\(L = M^{-1}\\) , alors on peut \u00e9crire \\(A = L U\\) avec \\(L\\) et \\(U\\) des matrices triangulaires inf\u00e9rieure (L) et sup\u00e9rieure (U) . Les matrices \\(M_k\\) sont inversibles, et leurs inverses sont les matrices triangulaires inf\u00e9rieures : \\(L_k = \\begin{pmatrix} 1 & 0 & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & 1 & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & -m_{k+1,k} & 0 & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & -m_{n,k} & 0 & \\cdots & 0 & 1 \\end{pmatrix}\\) Il en r\u00e9sulte que \\(L = M^{-1} = L_{n-1} L_{n-2} ... L_1\\) . Dans le cadre des strat\u00e9gies avec pivot partiel ou total, on ajoute les permutations : \\(A = P L U\\) . D'o\u00f9 les d\u00e9compositions \"LU\" et \"PLU\" pr\u00e9sent\u00e9es dans la section suivante. D\u00e9composition LU et PLU Id\u00e9e D\u00e9composition LU : On appelle d\u00e9composition LU (ou factorisation LU) d'une matrice carr\u00e9e \\(A\\) la recherche d'une matrice triangulaire inf\u00e9rieure \\(L\\) et d'une matrice triangulaire sup\u00e9rieure \\(U\\) telles que : \\(A = L U\\) Si \\(A\\) est inversible, alors \\(L\\) et \\(U\\) le sont aussi, et leurs termes diagonaux sont non-nuls. Alors, r\u00e9soudre \\(A x = b\\) revient \u00e0 r\u00e9soudre \\(L U x = b\\) , soit 2 syst\u00e8mes triangulaires : \\(L y = b\\) et \\(U x = y\\) Ces 2 syst\u00e8mes sont faciles \u00e0 r\u00e9soudre : \\(L y = b\\) peut \u00eatre r\u00e9solu avec un algorithme de descente : \\(\\begin{cases} y_1 = \\frac{1}{l_{1,1}} b_1\\\\ y_i = \\frac{1}{l_{i,i}} (b_i - \\displaystyle\\sum_{j=1}^{i-1} l_{i,j} y_j), i=2,...,n \\end{cases}\\) \\(U x = y\\) peut \u00eatre r\u00e9solu avec un algorithme de remont\u00e9e : \\(\\begin{cases} x_n = \\frac{1}{u_{n,n}} y_n\\\\ x_i = \\frac{1}{u_{i,i}} (y_i - \\displaystyle\\sum_{j=i+1}^{n} u_{i,j} x_j), i=n-1,...,1 \\end{cases}\\) Chacun de ces algorithmes n\u00e9cessite \\(n^2\\) op\u00e9rations. On devine alors un des grands avantages de la d\u00e9composition LU compar\u00e9e l'\u00e9limination de Gauss : si on a \\(n\\) syst\u00e8mes \\(A x_1 = b_1\\) , \\(A x_2 = b_2\\) , ... , \\(A x_n = b_n\\) \u00e0 r\u00e9soudre, il est inutile d'appliquer \\(n\\) fois l'\u00e9limination de Gauss. Une fois la d\u00e9composition LU obtenue pour un syst\u00e8me, il suffit d'appliquer les algorithmes de descente / remont\u00e9e aux autres, ce qui est beaucoup moins co\u00fbteux en calculs . La factorisation \\(L U\\) , quand elle existe, n'est pas unique : le syst\u00e8me \\(A = L U\\) est sous-d\u00e9termin\u00e9. On peut rendre la solution unique en donnant des conditions suppl\u00e9mentaires. Par exemple, on peut fixer les \u00e9l\u00e9ments diagonaux de \\(L\\) \u00e0 1. C'est que l'on appelle la factorisation de Gauss . La d\u00e9composition LU (sans pivotage) n'existe pas toujours , m\u00eame si \\(A\\) est inversible. Existence de la d\u00e9composition LU La d\u00e9composition LU existe si et seulement si : toutes les sous-matrices principales \\(A_k = (a_{i,j})_{1 \\leq i,j \\leq k}\\) d'ordre 1 \u00e0 \\(n-1\\) sont inversibles. Si toutes les sous-matrices principales d'ordre 1 \u00e0 \\(n\\) sont inversibles, elle est m\u00eame unique . Il est \u00e0 noter que le fait qu'une sous-matrice principale de \\(A\\) ne soit pas inversible ne signifie pas n\u00e9cessairement que \\(A\\) n'est pas inversible. Pour les matrice inversibles pour lesquelles il n'existe pas de d\u00e9composition LU, d'autres m\u00e9thodes de triangularisation existent, comme la d\u00e9composition PLU . D\u00e9composition PLU : Elle ajoute \u00e0 la d\u00e9composition LU les permutations (pivot partiel ou total), avec une matrice de permutation \\(P\\) : \\(P A = L U\\) Cette fois-ci, r\u00e9soudre \\(A x = b\\) revient \u00e0 r\u00e9soudre \\(P^{-1} L U x = b\\) , soit \\(L U x = P b\\) . On doit donc appliquer les algorithmes de descente et de remont\u00e9e aux syst\u00e8mes triangulaires : \\(L y = P b\\) et \\(U x = y\\) Existence de la d\u00e9composition PLU La d\u00e9composition PLU existe toujours si \\(A\\) est carr\u00e9e et r\u00e9guli\u00e8re (inversible). Autres int\u00e9r\u00eats de la d\u00e9composition LU : En plus de permettre la r\u00e9solution du syst\u00e8me d'\u00e9quation, les d\u00e9compositions LU et PLU permettent de calculer le d\u00e9terminant de \\(A\\) rapidement : \\(det(A) = det(LU) = \\displaystyle\\prod_{k=1}^{n} u_{k,k}\\) \\(det(A) = det(P^{-1}LU) = (-1)^p \\displaystyle\\prod_{k=1}^{n} u_{k,k}\\) avec \\(p\\) permutations La d\u00e9composition LU facilite aussi le calcul de l'inverse de \\(A\\) : En effet, recherche \\(X = A^{-1}\\) revient \u00e0 r\u00e9soudre \\(A X = I\\) , et donc \u00e0 r\u00e9soudre \\(L U X = I\\) . On peut alors trouver \\(A^{-1}\\) en r\u00e9solvant le syst\u00e8me : \\(\\begin{cases} L Y = I\\\\ U X = Y \\end{cases}\\) D\u00e9terminer les \u00e9l\u00e9ments de \\(L\\) et \\(U\\) par la factorisation de Gauss requiert \\(2 n^3/2\\) op\u00e9rations (sans compter les permutations). Il faut ensuite de l'ordre de \\(2 n^2\\) op\u00e9rations pour les algorithmes de remont\u00e9e et de descente. Algorithme Nous donnerons dans cette section les algorithmes pour les d\u00e9compositions LU puis PLU. Voici sous la forme d'une fonction Python l'algorithme de la d\u00e9composition LU. Cette fonction prend en entr\u00e9e un syst\u00e8me de Cramer : A la matrice des coefficients du syst\u00e8me. b le vecteur du second membre du syst\u00e8me. def decomposition_LU(A): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) : if (m!=n): raise ValueError(\"La matrice A n'est pas carr\u00e9e\") #Copier A pour ne pas modifier la matrice originale : U = np.copy(A) #Initialiser la matrice L comme la matrice identit\u00e9 (nxn) : L = np.eye(n) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur sur la diagonale de la j-\u00e8me colonne : pivot = U[j,j] #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Sauvegarde du coefficient d'\u00e9limination de Gauss dans L : L[k,j] = U[k,j]/pivot #Op\u00e9rations d'\u00e9limination de Gauss sur les lignes de A en #utilisant le pivot : U[k,:] = U[k,:] - U[j,:]*L[k,j] #Renvoyer les matrices L et U : return L,U Voici sous la forme d'une fonction Python l'algorithme de descente li\u00e9e \u00e0 la d\u00e9composition LU. Cette fonction prend en entr\u00e9e : L la matrice \\(L\\) obtenue par la d\u00e9composition LU de \\(A\\) . b le vecteur du second membre du syst\u00e8me. def descente(L,b): #R\u00e9cup\u00e9rer le nombre n d'\u00e9quations / inconnues du syst\u00e8me Ly=b : n = len(L) #Initialiser le vecteur qui contiendra les solutions du syst\u00e8me Ly=b : y = np.zeros(n,dtype=np.float64) #Boucle sur les lignes de la matrice L, de 1 \u00e0 n : for i in range(n): #D\u00e9termination de la i-\u00e8me inconnue : y[i] = (b[i]-sum(L[i,:i]*y[:i]))/L[i,i] #Renvoyer le vecteur contenant les solutions du syst\u00e8me Ly=b : return y Voici sous la forme d'une fonction Python l'algorithme de remont\u00e9e li\u00e9e \u00e0 la d\u00e9composition LU. Cette fonction prend en entr\u00e9e : U la matrice \\(U\\) obtenue par la d\u00e9composition LU de \\(A\\) . y le vecteur des solution du syst\u00e8me \\(L y = b\\) obtenue par l'algorithme de descente. def remontee(U,y): #R\u00e9cup\u00e9rer le nombre n d'\u00e9quations / inconnues du syst\u00e8me Ux=y : n = len(U) #Initialiser le vecteur qui contiendra les solutions du syst\u00e8me Ux=y : x = np.zeros(n,dtype=np.float64) #Boucle sur les lignes de la matrice U, de n \u00e0 1 : for i in range(n-1,-1,-1): #D\u00e9termination de la i-\u00e8me inconnue : x[i] = (y[i]-sum(U[i,i+1:n]*x[i+1:n]))/U[i,i] #Renvoyer le vecteur contenant les solutions du syst\u00e8me Ux=y : return x Voici maintenant sous la forme d'une fonction Python l'algorithme de la d\u00e9composition PLU. Comme pour la d\u00e9composition LU, cette fonction prend en entr\u00e9e un syst\u00e8me de Cramer : A la matrice des coefficients du syst\u00e8me. b le vecteur du second membre du syst\u00e8me. def decomposition_PLU(A): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) : if (m!=n): raise ValueError(\"La matrice A n'est pas carr\u00e9e\") #Copier A pour ne pas modifier la matrice originale : U = np.copy(A) #Initialiser la matrice L comme la matrice identit\u00e9 (nxn) : L = np.eye(n) #Initialiser la matrice de permutation P comme la matrice identit\u00e9 (nxn) : P = np.eye(n) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur maximale en absolu sur la colonne, #sur la j-i\u00e8me ligne ou en dessous : idx_pivot = np.argmax(abs(U[j:,j]))+j #Indice de la ligne du pivot pivot = U[idx_pivot,j] #Valeur du pivot #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Si le pivot n'est pas sur la j-i\u00e8me ligne, \u00e9changer la j-i\u00e8me et la #ligne du pivot : if idx_pivot!=j: U[[j,idx_pivot]] = U[[idx_pivot,j]] #Pour la matrice A P[[j,idx_pivot]] = P[[idx_pivot,j]] #Pour la matrice P #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Sauvegarde du coefficient d'\u00e9limination de Gauss dans L : L[k,j] = U[k,j]/pivot #Op\u00e9rations d'\u00e9limination de Gauss sur les lignes de A en #utilisant le pivot : U[k,:] = U[k,:] - U[j,:]*L[k,j] #Renvoyer les matrices P, L et U : return P,L,U Apr\u00e8s une d\u00e9composition PLU, il ne faudra pas oublier d'appliquer l'algorithme de descente \u00e0 \\(P b\\) au lieu de \\(b\\) . Exemple Nous allons appliquer l'algorithme de d\u00e9composion LU, puis PLU \u00e0 notre probl\u00e8me exemple D\u00e9composition LU : Appliquons d'abord l'algorithme de d\u00e9composion LU. On rappelle que nous avons initialement le syst\u00e8me de Cramer \\(A x = b\\) suivant : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) Nous initialisons \\(L\\) et \\(U\\) de la mani\u00e8re suivante : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix}\\) 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant sur la diagonale de \\(U\\) : -5000. On r\u00e9alise les op\u00e9rations suivantes : On ajoute \\(\\frac{10000}{-5000} = -2\\) en ligne 2 dans \\(L\\) . On applique \\(L_2 = L_2 - L_1 \\times -2\\) sur \\(U\\) . On ajoute \\(\\frac{-4000}{-5000} = 0.8\\) en ligne 3 dans \\(L\\) . On applique \\(L_3 = L_3 - L_1 \\times 0.8\\) sur \\(U\\) . \\(L\\) et \\(U\\) deviennent alors : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0.8 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 0 & -34000 & -18000 \\\\ 0 & 26400 & -2800 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9l\u00e9ctionne le pivot comme \u00e9tant sur la diagonale de \\(U\\) : -34000. On r\u00e9alise les op\u00e9rations suivantes : On ajoute \\(\\frac{26400}{-34000} = -0.7765\\) en ligne 3 dans \\(L\\) . On applique \\(L_3 = L_3 - L_2 \\times -0.7765\\) sur \\(U\\) . \\(L\\) et \\(U\\) deviennent alors : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0.8 & -0.7764 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 0 & -34000 & -18000 \\\\ 0 & 0 & -16776.47 \\end{pmatrix}\\) On retrouve bien pour \\(U\\) la matrice triangulaire obtenue avec l'\u00e9limination de Gauss sans pivotage, et pour \\(L\\) les coefficients ayant servi \u00e0 l'\u00e9limination. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On peut v\u00e9rifier que \\(det(A) = -5000 \\times -34000 \\times -16776.47 \\approx -2852000000000\\) On d\u00e9duit les solutions du syst\u00e8me par les algorithmes : de descente pour \\(L y = b\\) \\(\\begin{cases} -42977000\\\\ -5404000 - (-2 \\times -42977000) = -91358000\\\\ -43586000 - (0.8 \\times -42977000) - (-0.7764 \\times -91358000) = -80141200 \\end{cases}\\) puis de remont\u00e9e pour \\(U x = y\\) \\(\\begin{cases} z_r = \\frac{-80141200}{-16776.47} = 4777\\\\ y_r = \\frac{1}{-34000} (-91358000 - (-18000 \\times z_r)) = 158\\\\ x_r = \\frac{1}{-5000} (-42977000 - (-4000 \\times y_r) - (-18000 \\times z_r)) = 4205 \\end{cases}\\) R\u00e9-utilisation de la d\u00e9composition LU : Imaginons maintenant qu'un r\u00e9cepteur situ\u00e9 au Beffroi de Lille, de coordonn\u00e9es ECEF approximatives \\((x_r,y_r,z_r) = (4048,217,4908)\\) , utilise les m\u00eames satellites GPS de m\u00eames coordonn\u00e9es ECEF pour se positionner. Le vecteur \\(b\\) devient alors : \\(\\begin{pmatrix} -43778000\\\\ -8166000\\\\ -43036000 \\end{pmatrix}\\) Les coefficients de la matrice \\(A\\) ne d\u00e9pendant que de la position des satellites GPS, ils restent inchang\u00e9s. On peut donc r\u00e9-utiliser la d\u00e9composition LU pr\u00e9c\u00e9dente pour r\u00e9soudre ce syst\u00e8me : \\(\\begin{cases} -43778000\\\\ -8166000 - (-2 \\times -43778000) = -95722000\\\\ -43036000 - (0.8 \\times -43778000) - (-0.7764 \\times -95722000) = -82338917.65 \\end{cases}\\) puis \\(\\begin{cases} z_r = \\frac{-82338917.65}{-16776.47} = 4048\\\\ y_r = \\frac{1}{-34000} (-95722000 - (-18000 \\times z_r)) = 217\\\\ x_r = \\frac{1}{-5000} (-43778000 - (-4000 \\times y_r) - (-18000 \\times z_r)) = 4908 \\end{cases}\\) On retrouve bien la position de notre r\u00e9cepteur lillois. D\u00e9composition PLU : Appliquons \u00e0 pr\u00e9sent au syst\u00e8me l'algorithme de d\u00e9composion PLU. On rappelle que nous avons initialement le syst\u00e8me de Cramer \\(A x = b\\) suivant : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) Nous initialisons \\(P\\) , \\(L\\) et \\(U\\) de la mani\u00e8re suivante : \\(P = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix}\\) 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale de \\(U\\) : 10000. On \u00e9change la ligne 1 et la ligne 2 pour faire passer le pivot sur la diagonale. \\(U\\) et \\(P\\) deviennent alors : \\(P = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} 10000 & 2000 & -10000 \\\\ -5000 & -18000 & -4000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : On ajoute \\(\\frac{-5000}{10000} = -0.5\\) en ligne 2 dans \\(L\\) . On applique \\(L_2 = L_2 - L_1 \\times -0.5\\) sur \\(U\\) . On ajoute \\(\\frac{-5000}{10000} = -0.4\\) en ligne 3 dans \\(L\\) . On applique \\(L_3 = L_3 - L_1 \\times -0.4\\) sur \\(U\\) . \\(L\\) et \\(U\\) deviennent alors : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ -0.5 & 1 & 0 \\\\ -0.4 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} 10000 & 2000 & -10000 \\\\ 0 & -17000 & -9000 \\\\ 0 & 12800 & -10000 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale de \\(U\\) : -17000. On r\u00e9alise les op\u00e9rations suivantes : On ajoute \\(\\frac{12800}{-17000} = -0.7529\\) en ligne 3 dans \\(L\\) . On applique \\(L_3 = L_3 - L_2 \\times -0.7529\\) sur \\(U\\) . \\(L\\) et \\(U\\) deviennent alors : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ -0.5 & 1 & 0 \\\\ -0.4 & -0.7529 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} 10000 & 2000 & -10000 \\\\ 0 & -17000 & -9000 \\\\ 0 & 0 & -16776.47 \\end{pmatrix}\\) On retrouve bien pour \\(U\\) la matrice triangulaire obtenue avec l'\u00e9limination de Gauss avec pivot partiel, et pour \\(L\\) les coefficients ayant servi \u00e0 l'\u00e9limination. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On peut v\u00e9rifier que \\(det(A) = (-1)^1 \\times 10000 \\times -17000 \\times -16776.47 \\approx -2852000000000\\) On d\u00e9duit les solutions du syst\u00e8me : Tout d'abord, on calcule \\(P b = \\begin{pmatrix} -5404000\\\\ -42977000\\\\ -43586000 \\end{pmatrix}\\) puis on applique \u00e0 \\(L y = P b\\) l' algorithme de descente \\(\\begin{cases} -5404000\\\\ -42977000 - (-0.5 \\times -5404000) = -45679000\\\\ -43586000 - (-0.4 \\times -5404000) - (-0.7529 \\times -45679000) = -80141200 \\end{cases}\\) et on applique l'algorithme de remont\u00e9e \u00e0 \\(U x =y\\) \\(\\begin{cases} z_r = \\frac{-80141200}{-16776.47} = 4777\\\\ y_r = \\frac{1}{-17000} (-45679000 - (-9000 \\times z_r)) = 158\\\\ x_r = \\frac{1}{10000} (-5404000 - (2000 \\times y_r) - (-10000 \\times z_r)) = 4205 \\end{cases}\\) Exercice : Imaginons maintenant qu'un r\u00e9cepteur situ\u00e9 au Cirque de Gavarnie, de coordonn\u00e9es ECEF approximatives \\((x_r,y_r,z_r) = (4695,0,4303)\\) , utilise les m\u00eames satellites GPS de m\u00eames coordonn\u00e9es ECEF pour se positionner. En vous servant des programmes Python pr\u00e9c\u00e9dents, d\u00e9terminez les nouvelles valeurs de \\(b\\) , puis utilisez la d\u00e9composition PLU obtenue pr\u00e9c\u00e9demment pour estimer la position du r\u00e9cepteur. Vous devriez retrouver la position ECEF du Cirque de Gavarnie. Autres d\u00e9compositions (QR et Cholesky) Il existe d'autres types de d\u00e9composition moins co\u00fbteux en temps de calcul, qui peuvent s'appliquer \u00e0 des syst\u00e8mes particulier. Nous pr\u00e9senterons rapidement ici les d\u00e9compositions QR et de Cholesky. D\u00e9composition QR Th\u00e9or\u00e8me Si \\(A\\) est une matrice r\u00e9elle inversible , il existe un unique couple \\((Q,R)\\) avec \\(Q\\) une matrice orthogonale (c'est-\u00e0-dire \\(QQ^T=Q^TQ=I\\) ) et \\(R\\) une matrice triangulaire sup\u00e9rieure dont les \u00e9l\u00e9ments diagonaux sont positifs, tel que \\(A = QR\\) La d\u00e9composition QR consiste \u00e0 d\u00e9composer la matrice \\(A\\) de cette fa\u00e7on : \\(A = QR\\) . R\u00e9soudre le syst\u00e8me \\(A x = b\\) revient alors \u00e0 r\u00e9soudre : \\(\\begin{cases} Q y = b\\\\ R x = y \\end{cases}\\) soit \\(\\begin{cases} y = Q^{-1} b = Q^T b\\\\ R x = y \\end{cases}\\) D\u00e9composition de Cholesky Th\u00e9or\u00e8me Si \\(A\\) est sym\u00e9trique d\u00e9finie positive , (c'est-\u00e0-dire une matrice carr\u00e9e \u00e9gale \u00e0 sa transpos\u00e9e, positive et inversible) il existe une matrice triangulaire inf\u00e9rieure \\(L\\) telle que \\(A = LL^T\\) La d\u00e9composition de Cholesky consiste \u00e0 d\u00e9composer la matrice \\(A\\) de cette fa\u00e7on : \\(A = LL^T\\) . On peut imposer que les \u00e9l\u00e9ments diagonaux de \\(L\\) soient positifs, pour obtenir l' unicit\u00e9 de la factorisation. R\u00e9soudre le syst\u00e8me \\(A x = b\\) revient alors \u00e0 appliquer les algorithmes de remont\u00e9e et de descente pour r\u00e9soudre \\(LL^T x = b\\) . M\u00e9thodes it\u00e9ratives Les m\u00e9thodes directes donnent la solution du syst\u00e8me \\(A x = b\\) en un nombre fini d'op\u00e9rations, mais : Si la taille du syst\u00e8me est \u00e9lev\u00e9e, le nombre d'op\u00e9rations est important, ce qui augmente les erreurs. Si la taille du syst\u00e8me est \u00e9lev\u00e9e, le nombre de coefficients \u00e0 mettre en m\u00e9moire l'est aussi. Elles utilisent des propri\u00e9t\u00e9s math\u00e9matiques n\u00e9cessitant un calcul exact : il est donc difficile de tenir compte des erreurs de calculs. Les m\u00e9thodes it\u00e9ratives sont g\u00e9n\u00e9ralement plus rapides et n\u00e9cessitent moins de m\u00e9moire. L'id\u00e9e est de construire une suite \\((x^{(n)})_{n \\geq 0}\\) qui converge vers la solution \\(x\\) . Pour construire une telle suite, on va s'appuyer sur la lin\u00e9arit\u00e9 du probl\u00e8me en d\u00e9composant \\(A\\) en une matrice facilement inversible et un reste . Principe et convergence Les m\u00e9thodes it\u00e9ratives vont donc d\u00e9composer la matrice \\(A\\) du syst\u00e8me d'\u00e9quations de la mani\u00e8re suivante : \\(A = M-(M-A) = M-N\\) avec \\(M\\) facilement inversible . Alors, r\u00e9soudre \\(A x = b\\) revient \u00e0 r\u00e9soudre \\(M x = N x + b\\) . On calcule la suite de vecteurs \\((x^{(k)})_k\\) \u00e0 partir d'un vecteur de d\u00e9part \\(x^{(0)}\\) et de la relation de r\u00e9currence : \\(M x^{(k+1)} = N x^{(k)} + b\\) soit \\(x^{(k+1)} = M^{-1} N x^{(k)} + M^{-1} b\\) En posant \\(C = M^{-1} N\\) et \\(d = M^{-1} b\\) , la suite devient : \\(x^{(k+1)} = C x^{(k)} + d\\) La solution \\(x\\) est alors le point fixe de la fonction lin\u00e9aire \\(g(x) = C x + d\\) . Reste alors \u00e0 v\u00e9rifier sa convergence. Th\u00e9or\u00e8me Soit \\(C\\) une matrice carr\u00e9e de taille \\(n \\times n\\) , s'il existe une norme matricielle telle que \\(\\lVert C \\lVert < 1\\) , alors : - \\(g(x) = C x + d\\) admet un point fixe unique \\(x\\) . - La suite \\((x^{(k)})_k\\) telle que \\(x^{(k+1)} = g(x^{(k)})\\) converge vers ce point fixe quel que soit vecteur de d\u00e9part \\(x^{(0)}\\) . \\(C\\) est appel\u00e9e matrice d'it\u00e9ration . Le vecteur d'erreur absolue \u00e0 l'it\u00e9ration \\(k\\) est : \\(e^{(k)} = x^{(k)} - x\\) On en d\u00e9duit que : \\(e^{(k)} = x^{(k)} - x = (C x^{k-1} + d) - x = C x^{(k-1)} - C x = C (x^{(k-1)} - x) = C e^{(k-1)}\\) d'o\u00f9 \\(e^{(k)} = C^k e^{(0)}\\) On peut donc majorer l'erreur de la mani\u00e8re suivante : \\(\\|e^{(k)}\\| \\leq \\|C\\|^k \\|e^{(0)}\\|\\) d'o\u00f9 \\(\\lim\\limits_{k \\to \\infty} e^{(k)} = 0\\) et donc \\(\\lim\\limits_{k \\to \\infty} x^{(k)} = x\\) . Plus \\(\\|C\\|\\) est petit, moins il est n\u00e9cessaire d'effectuer des it\u00e9rations pour r\u00e9duire l'erreur initiale d'un facteur donn\u00e9. Convergence Pour \u00e9tablir la convergence de \\(x^{(k+1)} = C x^{(k)} + d\\) , il suffit de montrer que \\(C\\) v\u00e9rifie : \\(\\rho(C) = max_{1 \\leq i \\leq n} \\lVert \\lambda_i \\lVert < 1\\) Cette condition suffisante est aussi n\u00e9cessaire , car on a toujours \\(\\rho(C) \\leq \\|C\\|\\) . On peut calculer \u00e0 chaque it\u00e9ration le vecteur r\u00e9sidu : \\(r^{(k)} = b - A x^{(k)}\\) Voici alors 2 crit\u00e8res d'arr\u00eat possibles : \\(\\|r^{(k)}\\| \\leq \\epsilon \\|b\\|\\) ou \\(\\|x^{(k)}-x^{(k-1)}\\| \\leq \\epsilon \\|x^{(k-1)}\\|\\) L'avantage des m\u00e9thodes it\u00e9ratives est leur co\u00fbt : de l'ordre de \\(n^2\\) , \u00e0 comparer au \\(\\frac{2}{3} n^3\\) des m\u00e9thodes directes. Toute la difficult\u00e9 des m\u00e9thodes it\u00e9rative est dans leurs conditions de convergence , qui restreignent leur application \u00e0 certains syst\u00e8mes. M\u00e9thode de Jacobi Id\u00e9e La m\u00e9thode de Jacobi d\u00e9compose la matrice \\(A\\) en \\(A = M-N\\) et calcule la suite \\(x^{(k+1)} = M^{-1} N x^{(k)} + M^{-1} b\\) avec : \\(M = D\\) avec \\(D\\) la matrice des \u00e9l\u00e9ments diagonaux de \\(A\\) . \\(N = E+F\\) avec \\(-E\\) la matrice des \u00e9l\u00e9ments sous-diagonaux de \\(A\\) , et \\(-F\\) la matrice des \u00e9l\u00e9ments sur-diagonaux de \\(A\\) . On d\u00e9compose donc \\(A\\) en \\(A = D-E-F\\) avec \\(D\\) diagonale , avec \\(E\\) triangulaire inf\u00e9rieure , et avec \\(F\\) triangulaire sup\u00e9rieure . \\(D = \\begin{pmatrix} a_{1,1} & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & a_{2,2} & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & a_{3,3} & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & \\cdots & a_{n-1,n-1} & 0\\\\ 0 & 0 & 0 & \\cdots & 0 & a_{n,n} \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 & \\cdots & 0 & 0 & 0\\\\ a_{2,1} & 0 & 0 & \\cdots & 0 & 0 & 0\\\\ a_{3,1} & a_{3,2} & 0 & \\cdots & 0 & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ a_{n-2,1} & a_{n-2,2} & a_{n-2,3} & \\cdots & 0 & 0 & 0\\\\ a_{n-1,1} & a_{n-1,2} & a_{n-1,3} & \\cdots & a_{n-1,n-2} & 0 & 0\\\\ a_{n,1} & a_{n,2} & a_{n,3} & \\cdots & a_{n,n-2} & a_{n,n-1} & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & a_{1,2} & a_{1,3} & a_{1,4} & \\cdots & a_{1,n-1} & a_{1,n}\\\\ 0 & 0 & a_{2,3} & a_{2,4} & \\cdots & a_{2,n-1} & a_{2,n}\\\\ 0 & 0 & 0 & a_{3,4} & \\cdots & a_{3,n-1} & a_{3,n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & 0 & \\cdots & a_{n-2,n-1} & a_{n-2,n}\\\\ 0 & 0 & 0 & 0 & \\cdots & 0 & a_{n-1,n}\\\\ 0 & 0 & 0 & 0 & \\cdots & 0 & 0 \\end{pmatrix}\\) On appelle matrice de Jacobi : \\(J = M^{-1} N = D^{-1} (E+F)\\) La suite dont on cherche la limite est alors : \\(x^{(k+1)} = D^{-1} (E+F) x^{(k)} + D^{-1} b\\) On remarque que \\(D^{-1} (E+F) = D^{-1} (D-A) = I - D^{-1}A\\) \\(D^{-1} = \\begin{pmatrix} 1/a_{1,1} & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 1/a_{2,2} & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & 1/a_{3,3} & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & \\cdots & 1/a_{n-1,n-1} & 0\\\\ 0 & 0 & 0 & \\cdots & 0 & 1/a_{n,n} \\end{pmatrix}\\) et \\(D^{-1} (E+F) = \\begin{pmatrix} 0 & -a_{1,2}/a_{1,1} & -a_{1,3}/a_{1,1} & \\cdots & -a_{1,n-1}/a_{1,1} & -a_{1,n}/a_{1,1}\\\\ -a_{2,1}/a_{2,2} & 0 & -a_{2,3}/a_{2,2} & \\cdots & -a_{2,n-1}/a_{2,2} & -a_{2,n}/a_{2,2}\\\\ -a_{3,1}/a_{3,3} & -a_{3,2}/a_{3,3} & 0 & \\cdots & -a_{3,n-1}/a_{3,3} & -a_{3,n}/a_{3,3}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ -a_{n-1,1}/a_{n-1,n-1} & -a_{n-1,2}/a_{n-1,n-1} & -a_{n-1,3}/a_{n-1,n-1} & \\cdots & 0 & -a_{n-1,n}/a_{n-1,n-1}\\\\ -a_{n,1}/a_{n,n} & -a_{n,2}/a_{n,n} & -a_{n,3}/a_{n,n} & \\cdots & -a_{n,n-1}/a_{n,n} & 0 \\end{pmatrix}\\) On peut facilement en d\u00e9duire qu'\u00e0 l'it\u00e9ration \\(k\\) , et pour chaque ligne \\(i\\) : \\(D^{-1} (E+F) x_i^{(k)} = - \\frac{\\displaystyle\\sum_{j=1,j \\neq i}^{n} a_{i,j} x_j^{(k)}}{a_{i,i}}\\) et \\(D^{-1} b_i = \\frac{b_i}{a_{i,i}}\\) D'o\u00f9 \\(x_i^{(k)} = \\frac{1}{a_{i,i}} (b_i - \\displaystyle\\sum_{j=1,j \\neq i}^{n} a_{i,j} x_j^{(k-1)})\\) Une condition n\u00e9cessaire et suffisante de convergence : \\(D\\) doit \u00eatre inversible (tous les \u00e9l\u00e9ments diagonaux de \\(A\\) doivent \u00eatre non-nuls). \\(\\rho(J) < 1\\) (la valeur propre de \\(J\\) de plus grand module est < 1). Dans la pratique, on peut utiliser le th\u00e9or\u00e8me suivant : Th\u00e9or\u00e8me de convergence La m\u00e9thode de Jacobi converge quelque soit \\(x^{(0)}\\) pour les syst\u00e8mes lin\u00e9aires dont \\(A\\) est \u00e0 diagonale strictement dominante , c'est-\u00e0-dire : \\(\\forall 1 \\leq i \\leq n\\) , \\(\\mid a_{i,i} \\mid > \\displaystyle\\sum_{j=1,j \\neq i}^{n} \\mid a_{i,j} \\mid\\) La m\u00e9thode de Jacobi converge lentement , mais on peut facilement la rendre plus rapide : c'est l'id\u00e9e de la m\u00e9thode de Gauss-Seidel, pr\u00e9sent\u00e9e dans la suite de ce chapitre. Algorithme Voici sous la forme d'une fonction Python l'algorithme de la m\u00e9thode de Jacobi. Elle prend en entr\u00e9e : A et b les matrices du syst\u00e8me lin\u00e9aire \u00e0 r\u00e9soudre. x_0 la valeur initiale de la suite convergeant vers la solution. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la solution du syst\u00e8me. r_n le r\u00e9sidu. def jacobi(A,b,x_0,n_max,e): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #R\u00e9cup\u00e9rer les valeurs sur la diagonale de A : A_diag = np.diag(A) #Cr\u00e9er la matrice diagonale D, ayant les m\u00eames valeurs que la diagonale de A : D = np.diag(A_diag) #Cr\u00e9er la matrice EF, ayant des z\u00e9ros sur sa diagonale et les m\u00eames valeurs #que A partout ailleurs : EF = A-D #V\u00e9rifier si la matrice A est \u00e0 diagonale strictement dominante : for i in range(n): if sum(abs(EF[:,i]))>=abs(A_diag[i]): print(\"Attention : la matrice A n'est pas \u00e0 diagonale strictement dominante\") #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n-1 x_n = (b-np.dot(EF,x_n_old))/A_diag #Estimation de la solution \u00e0 l'it\u00e9ration n r_n = np.dot(A,x_n)-b #R\u00e9sidu #It\u00e9rations de l'algorithme de Jacobi #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(np.linalg.norm(x_n-x_n_old,ord=2)>e)and(np.linalg.norm(r_n,ord=2)>e): #Mettre \u00e0 jour l'estimation de la solution : x_n_old = np.copy(x_n) #It\u00e9ration n x_n = (b-np.dot(EF,x_n_old))/A_diag #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour le r\u00e9sidu : r_n = np.dot(A,x_n)-b #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : return x_n,r_n Exemple On rappelle que la matrice \\(A\\) de notre probl\u00e8me exemple est : \\(A = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix}\\) Cette matrice n'est pas \u00e0 diagonale strictement dominante : \\(\\mid -5000 \\mid \\leq \\mid -18000 \\mid + \\mid -4000 \\mid\\) \\(\\mid 2000 \\mid \\leq \\mid 10000 \\mid + \\mid -10000 \\mid\\) \\(\\mid -6000 \\mid \\leq \\mid -4000 \\mid + \\mid 12000 \\mid\\) La convergence de la m\u00e9thode de Jacobi n'est donc pas assur\u00e9e pour une initialisation quelconque. Et en effet, en appliquant l'algorithme de Jacobi \u00e0 notre syst\u00e8me, on observe que la suite diverge. Ceci illustre bien la limite des m\u00e9thodes it\u00e9ratives : leurs conditions de convergence. Mais admettons que notre r\u00e9cepteur situ\u00e9 \u00e0 l'UFR des Sciences de l'UVSQ utilise les signaux provenant de 4 autres satellites, de positions ECEF : \\((x_{s1},y_{s1},z_{s1}) = (15000,13000,18000)\\) \\((x_{s2},y_{s2},z_{s2}) = (1000,6000,24000)\\) \\((x_{s3},y_{s3},z_{s3}) = (19000,2000,19000)\\) \\((x_{s4},y_{s4},z_{s4}) = (12000,12000,26000)\\) Le syst\u00e8me \u00e0 r\u00e9soudre devient alors : \\(\\begin{pmatrix} -14000 & -7000 & 6000 \\\\ 4000 & -11000 & 1000 \\\\ -3000 & -1000 & 8000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -31314000\\\\ 19859000\\\\ 25443000 \\end{pmatrix}\\) Cette fois-ci, \\(A\\) est \u00e0 diagonale strictement dominante : \\(\\mid -14000 \\mid > \\mid -7000 \\mid + \\mid 6000 \\mid\\) \\(\\mid -11000 \\mid > \\mid 4000 \\mid + \\mid 1000 \\mid\\) \\(\\mid 8000 \\mid > \\mid -3000 \\mid + \\mid -1000 \\mid\\) La convergence de la m\u00e9thode de Jacobi est donc assur\u00e9e pour une initialisation quelconque. Nous allons donc appliquer la m\u00e9thode \u00e0 ce syst\u00e8me. Nous choisirons : \\(x^{(0)} =\\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}\\) On d\u00e9compose la matrice \\(A = D-E-F\\) avec : \\(D = \\begin{pmatrix} -14000 & 0 & 0 \\\\ 0 & -11000 & 0 \\\\ 0 & 0 & 8000 \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 \\\\ 4000 & 0 & 0 \\\\ -3000 & -1000 & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & -7000 & 6000 \\\\ 0 & 0 & 1000 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) On a donc : \\(D^{-1} = \\begin{pmatrix} \\frac{1}{-14000} & 0 & 0 \\\\ 0 & \\frac{1}{-11000} & 0 \\\\ 0 & 0 & \\frac{1}{8000} \\end{pmatrix}\\) \\(E+F = \\begin{pmatrix} 0 & 7000 & -6000 \\\\ -4000 & 0 & -1000 \\\\ 3000 & 1000 & 0 \\end{pmatrix}\\) Soit : \\(D^{(-1)}(E+F) = \\begin{pmatrix} 0 & \\frac{7000}{-14000} & \\frac{-6000}{-14000} \\\\ \\frac{-4000}{-10000} & 0 & \\frac{-1000}{-10000} \\\\ \\frac{3000}{8000} & \\frac{1000}{8000} & 0 \\end{pmatrix}\\) La suite de la m\u00e9thode de Jacobi convergeant vers la solution est alors : \\(\\begin{cases} x_r^{(k+1)} = \\frac{1}{-14000} (-31314000 + 7000 y_r^{(k)} - 6000 z_r^{(k)})\\\\ y_r^{(k+1)} = \\frac{1}{-11000} (19859000 - 4000 x_r^{(k)} - 1000 z_r^{(k)})\\\\ z_r^{(k+1)} = \\frac{1}{8000} (25443000 + 3000 x_r^{(k)} + 1000 y_r^{(k)}) \\end{cases}\\) Pour atteindre une pr\u00e9cision de \\(10^{-3}\\) , on a besoin d'it\u00e9rer 10 fois la m\u00e9thode de Jacobi : It\u00e9ration \\(k\\) \\(x_r^{(k)}\\) \\(y_r^{(k)}\\) \\(z_r^{(k)}\\) 0 0.0000 0.0000 0.0000 1 2236.7143 -1805.3636 3180.3750 2 4502.4140 -702.8880 3793.4724 3 4213.9322 176.7389 4780.9192 4 4197.3102 161.6044 4782.6919 5 4205.6372 155.7212 4774.5669 6 4205.0967 158.0105 4776.9541 7 4204.9751 158.0310 4777.0376 8 4205.0006 157.9943 4776.9945 9 4205.0005 157.9997 4776.9995 10 4204.9999 158.0001 4777.0001 On obtient bien la solution recherch\u00e9e avec la pr\u00e9cision attendue. Exercice : Calculez la matrice de Jacobi correspondant \u00e0 notre probl\u00e8me exemple. Est-il attendu que la m\u00e9thode ne converge pas pour ce syst\u00e8me ? D\u00e9montrez-le. M\u00e9thode de Gauss-Seidel Id\u00e9e La m\u00e9thode de Gauss-Seidel d\u00e9compose la matrice \\(A\\) en \\(A = M-N\\) et calcule la suite \\(x^{(k+1)} = M^{-1} N x^{(k)} + M^{-1} b\\) avec : \\(M = D-E\\) avec \\(D\\) la matrice des \u00e9l\u00e9ments diagonaux de \\(A\\) , et \\(-E\\) la matrice des \u00e9l\u00e9ments sous-diagonaux de \\(A\\) . \\(N = F\\) avec \\(-F\\) la matrice des \u00e9l\u00e9ments sur-diagonaux de \\(A\\) . On appelle matrice de Gauss-Seidel : \\(G = (D-E)^{-1} F = M^{-1}N\\) La suite dont on cherche la limite est alors : \\(x^{(k+1)} = (D-E)^{-1} F x^{(k)} + (D-E)^{-1} b\\) On remarque que : \\((D-E) x^{(k+1)} = F x^{(k)} + b\\) avec \\(D-E = \\begin{pmatrix} a_{1,1} & 0 & 0 & \\cdots & 0 & 0\\\\ a_{2,1} & a_{2,2} & 0 & \\cdots & 0 & 0\\\\ a_{3,1} & a_{3,2} & a_{3,3} & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ a_{n-1,1} & a_{n-1,2} & a_{n-1,3} & \\cdots & a_{n-1,n-1} & 0\\\\ a_{n,1} & a_{n,2} & a_{n,3} & \\cdots & a_{n,n-1} & a_{n,n} \\end{pmatrix}\\) et \\(F = \\begin{pmatrix} 0 & -a_{1,2} & -a_{1,3} & \\cdots & -a_{1,n-1} & -a_{1,n}\\\\ 0 & 0 & -a_{2,3} & \\cdots & -a_{2,n-1} & -a_{2,n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & \\cdots & -a_{n-2,n-1} & -a_{n-2,n}\\\\ 0 & 0 & 0 & \\cdots & 0 & -a_{n-1,n}\\\\ 0 & 0 & 0 & \\cdots & 0 & 0 \\end{pmatrix}\\) On peut facilement en d\u00e9duire qu'\u00e0 l'it\u00e9ration \\(k\\) , et pour chaque ligne \\(i\\) : \\((D-E) x_i^{(k+1)} = \\displaystyle\\sum_{j=1}^{i} a_{i,j} x_j^{(k+1)} = a_{i,i} x_i^{(k+1)} + \\displaystyle\\sum_{j=1}^{i-1} a_{i,j} x_j^{(k+1)}\\) et \\(F x_i^{(k)} = -\\displaystyle\\sum_{j=i+1}^{n} a_{i,j} x_j^{(k)}\\) D'o\u00f9 \\(x_i^{(k)} = \\frac{1}{a_{i,i}} (b_i - \\displaystyle\\sum_{j=1}^{i-1} a_{i,j} x_j^{(k)} - \\displaystyle\\sum_{j=i+1}^{n} a_{i,j} x_j^{(k-1)})\\) Une condition n\u00e9cessaire et suffisante de convergence : \\(D-E\\) doit \u00eatre inversible (tous les \u00e9l\u00e9ments diagonaux de \\(A\\) doivent \u00eatre non-nuls). \\(\\rho(G) < 1\\) (la valeur propre de \\(G\\) de plus grand module est < 1). Dans la pratique, on peut utiliser les th\u00e9or\u00e8mes suivant : Th\u00e9or\u00e8me de convergence 1 La m\u00e9thode de Gauss-Seidel converge quelque soit \\(x^{(0)}\\) pour les syst\u00e8mes lin\u00e9aires dont \\(A\\) est \u00e0 diagonale strictement dominante , c'est-\u00e0-dire : \\(\\forall 1 \\leq i \\leq n\\) , \\(\\mid a_{i,i} \\mid > \\displaystyle\\sum_{j=1,j \\neq i}^{n} \\mid a_{i,j} \\mid\\) Th\u00e9or\u00e8me de convergence 2 Si \\(A\\) est une matrice sym\u00e9trique d\u00e9finie positive alors la m\u00e9thode de Gauss-Seidel converge. Contrairement \u00e0 la m\u00e9thode de Jacobi, la mise \u00e0 jour des composantes de \\(x^{(k)}\\) se fait de mani\u00e8re s\u00e9quentielle et non en parall\u00e8le. Ceci rend la convergence de la m\u00e9thode de Gauss-Seidel plus rapide que celle de Jacobi. Cependant, la m\u00e9thode de Gauss-Seidel peut osciller autour de la solution. Pour limiter ce ph\u00e9nom\u00e8ne et donc acc\u00e9l\u00e9rer la convergence, on peut utiliser les m\u00e9thodes de relaxation , d\u00e9crite dans la suite de ce chapitre. Algorithme Voici sous la forme d'une fonction Python l'algorithme de la m\u00e9thode de Gauss-Seidel. Elle prend en entr\u00e9e : A et b les matrices du syst\u00e8me lin\u00e9aire \u00e0 r\u00e9soudre. x_0 la valeur initiale de la suite convergeant vers la solution. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la solution du syst\u00e8me. r_n le r\u00e9sidu. def gauss_seidel(A,b,x_0,n_max,e): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #R\u00e9cup\u00e9rer les valeurs sur la diagonale de A : A_diag = np.diag(A) #Cr\u00e9er la matrice diagonale D, ayant les m\u00eames valeurs que la diagonale de A : D = np.diag(A_diag) #Cr\u00e9er la matrice A-D, ayant des z\u00e9ros sur sa diagonale et les m\u00eames valeurs #que A partout ailleurs : AD = A-D #V\u00e9rifier si la matrice A est \u00e0 diagonale strictement dominante : for i in range(n): if sum(abs(AD[:,i]))>=abs(A_diag[i]): print(\"Attention : la matrice A n'est pas \u00e0 diagonale strictement dominante\") break #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n-1 x_n = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n for i in range(len(b)): x_n[i] = (b[i]-np.dot(A[i,:i],x_n[:i])-np.dot(A[i,i+1:],x_n[i+1:]))/A[i,i] r_n = np.dot(A,x_n)-b #R\u00e9sidu #It\u00e9rations de l'algorithme de Gauss-Seidel #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(np.linalg.norm(x_n-x_n_old,ord=2)>e)and(np.linalg.norm(r_n,ord=2)>e): #Mettre \u00e0 jour l'estimation de la solution : x_n_old = np.copy(x_n) #It\u00e9ration n for i in range(len(b)): x_n[i] = (b[i]-np.dot(A[i,:i],x_n[:i])-np.dot(A[i,i+1:],x_n[i+1:]))/A[i,i] #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : r_n = np.dot(A,x_n)-b #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : return x_n,r_n Exemple Nous allons cette fois-ci appliquer la m\u00e9thode de Gauss-Seidel au syst\u00e8me : \\(\\begin{pmatrix} -14000 & -7000 & 6000 \\\\ 4000 & -11000 & 1000 \\\\ -3000 & -1000 & 8000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -31314000\\\\ 19859000\\\\ 25443000 \\end{pmatrix}\\) Comme montr\u00e9 pr\u00e9c\u00e9demment, la matrice \\(A\\) est \u00e0 diagonale strictement dominante. La convergence de la m\u00e9thode de Gauss-Seidel est donc assur\u00e9e pour une initialisation quelconque. Nous allons donc appliquer la m\u00e9thode \u00e0 ce syst\u00e8me. Nous choisirons : \\(x^{(0)} =\\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}\\) On d\u00e9compose la matrice \\(A = D-E-F\\) avec : \\(D = \\begin{pmatrix} -14000 & 0 & 0 \\\\ 0 & -11000 & 0 \\\\ 0 & 0 & 8000 \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 \\\\ 4000 & 0 & 0 \\\\ -3000 & -1000 & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & -7000 & 6000 \\\\ 0 & 0 & 1000 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) On a donc : \\(D-E = \\begin{pmatrix} -14000 & 0 & 0 \\\\ 4000 & -11000 & 0 \\\\ -3000 & -1000 & 8000 \\end{pmatrix}\\) La suite de la m\u00e9thode de Gauss-Seidel convergeant vers la solution est alors : \\(\\begin{cases} x_r^{(k+1)} = \\frac{1}{-14000} (-31314000 + 7000 y_r^{(k)} - 6000 z_r^{(k)})\\\\ y_r^{(k+1)} = \\frac{1}{-11000} (19859000 - 4000 x_r^{(k+1)} - 1000 z_r^{(k)})\\\\ z_r^{(k+1)} = \\frac{1}{8000} (25443000 + 3000 x_r^{(k+1)} + 1000 y_r^{(k+1)}) \\end{cases}\\) Pour atteindre une pr\u00e9cision de \\(10^{-3}\\) , on a besoin d'it\u00e9rer 9 fois la m\u00e9thode de Gauss-Seidel : It\u00e9ration \\(k\\) \\(x_r^{(k)}\\) \\(y_r^{(k)}\\) \\(z_r^{(k)}\\) 0 0.0000 0.0000 0.0000 1 2236.7143 -992.0130 3895.1412 2 4402.0670 149.4918 4849.8366 3 4240.4698 177.5196 4792.7411 4 4201.9864 158.3352 4775.9118 5 4204.3660 157.6705 4776.7211 6 4205.0452 157.9911 4777.0158 7 4205.0112 158.0055 4777.0049 8 4204.9993 158.0002 4776.9998 9 4204.9998 157.9999 4776.9999 On obtient bien la solution recherch\u00e9e avec la pr\u00e9cision attendue, pour une it\u00e9ration de moins que la m\u00e9thode de Jacobi. Exercice : Essayez d'appliquer la m\u00e9thode Gauss-Seidel au syst\u00e8me d\u00e9finit dans le probl\u00e8me exemple. La m\u00e9thode converge-t-elle ? Ce r\u00e9sultat \u00e9tait-il pr\u00e9visible ? Prouvez-le. M\u00e9thode de relaxation Id\u00e9e La m\u00e9thode de la relaxation est une variante de l'algorithme de Gauss-Seidel. L'id\u00e9e est de calculer un vecteur \\(x_G^{(k+1)}\\) \u00e0 l'aide de de la m\u00e9thode de Gauss-Seidel, puis de calculer \\(x^{(k+1)}\\) comme la moyenne pond\u00e9r\u00e9e : \\(x^{(k+1)} = \\omega x_G^{(k+1)} + (1-\\omega) x^{(k)}\\) avec \\(\\omega > 0\\) L'algorithme de relaxation d\u00e9compose donc la matrice \\(A = M-N\\) et calcule la suite \\(x^{(k+1)} = M^{-1} N x^{(k)} + M^{-1} b\\) avec : \\(M = \\frac{1}{\\omega} D - E\\) avec \\(D\\) la matrice des \u00e9l\u00e9ments diagonaux de \\(A\\) et \\(E\\) la matrice des \u00e9l\u00e9ments sous-diagonaux de \\(A\\) . \\(N = (\\frac{1}{\\omega}-1) D + F\\) avec \\(D\\) la matrice des \u00e9l\u00e9ments diagonaux de \\(A\\) et \\(F\\) la matrice des \u00e9l\u00e9ments sur-diagonaux de \\(A\\) . On appelle \\(\\omega\\) le param\u00e8tre de relaxation . Il y a diff\u00e9rents cas de figure suivant la valeur de \\(\\omega\\) choisie : Si \\(\\omega < 1\\) on parle de sous-relaxation : la m\u00e9thode est plus \"prudente\" que Gauss-Seidel. Si \\(\\omega > 1\\) on parle de sur-relaxation : la m\u00e9thode est plus \"plus ambitieuse\" que Gauss-Seidel. Si \\(\\omega = 1\\) on retrouve la m\u00e9thode de Gauss-Seidel. La matrice d'it\u00e9ration est ici : \\(C = (D - \\omega E)^{-1} ((1-\\omega) D + \\omega F)\\) On cherche \u00e0 optimiser \\(\\omega\\) pour que \\(\\rho(C)\\) soit minimal. On sait qu'\u00e0 l'it\u00e9ration \\(k\\) , et pour chaque ligne \\(i\\) : \\(x_i^{(k)} = \\omega x_Gi^{(k)} + (1-\\omega) x_i^{(k-1)}\\) On peut donc en d\u00e9duire que : \\(x_i^{(k)} = \\frac{\\omega}{a_{i,i}} (b_i - \\displaystyle\\sum_{j=1}^{i-1} a_{i,j} x_j^{(k)} - \\displaystyle\\sum_{j=i+1}^{n} a_{i,j} x_j^{(k-1)} - (\\frac{1}{\\omega}-1) a_{i,i} x_i^{(k-1)})\\) Voici un th\u00e9or\u00e8me utile pour v\u00e9rifier la convergence de la m\u00e9thode : Th\u00e9or\u00e8me - Si la m\u00e9thode de la relaxation converge, alors \\(0 < \\omega < 2\\) . - Si \\(A\\) est \u00e0 diagonale strictement dominante, la m\u00e9thode de la relaxation converge pour tout vecteur de d\u00e9part \\(x^{(0)}\\) si \\(0 < \\omega \\leq 1\\) . - Si \\(A\\) est sym\u00e9trique d\u00e9finie positive, la m\u00e9thode de relaxation converge pour tout vecteur de d\u00e9part \\(x^{(0)}\\) si \\(0 < \\omega < 2\\) . Algorithme Voici sous la forme d'une fonction Python l'algorithme de la m\u00e9thode de la relaxation. Elle prend en entr\u00e9e : A et b les matrices du syst\u00e8me lin\u00e9aire \u00e0 r\u00e9soudre. omega le param\u00e8tre de relaxation. x_0 la valeur initiale de la suite convergeant vers la solution. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la solution du syst\u00e8me. r_n le r\u00e9sidu. def relaxation(A,b,omega,x_0,n_max,e): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #R\u00e9cup\u00e9rer les valeurs sur la diagonale de A : A_diag = np.diag(A) #Cr\u00e9er la matrice diagonale D, ayant les m\u00eames valeurs que la diagonale de A : D = np.diag(A_diag) #Cr\u00e9er la matrice A-D, ayant des z\u00e9ros sur sa diagonale et les m\u00eames valeurs #que A partout ailleurs : AD = A-D #V\u00e9rifier si la matrice A est \u00e0 diagonale strictement dominante : for i in range(n): if sum(abs(AD[:,i]))>=abs(A_diag[i]): print(\"Attention : la matrice A n'est pas \u00e0 diagonale strictement dominante\") break #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n-1 x_n = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n for i in range(len(b)): x_n[i] = omega*(b[i]-np.dot(A[i,:i],x_n[:i])-np.dot(A[i,i+1:],x_n_old[i+1:]))/A[i,i] + (1-omega)*x_n_old[i] r_n = np.dot(A,x_n)-b #R\u00e9sidu #It\u00e9rations de l'algorithme de la relaxation #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(np.linalg.norm(x_n-x_n_old,ord=2)>e)and(np.linalg.norm(r_n,ord=2)>e): #Mettre \u00e0 jour l'estimation de la solution : x_n_old = np.copy(x_n) #It\u00e9ration n for i in range(len(b)): x_n[i] = omega*(b[i]-np.dot(A[i,:i],x_n[:i])-np.dot(A[i,i+1:],x_n_old[i+1:]))/A[i,i] + (1-omega)*x_n_old[i] #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : r_n = np.dot(A,x_n)-b #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : return x_n,r_n Exemple On peut d\u00e9montrer que pour le syst\u00e8me de notre probl\u00e8me-exemple, la m\u00e9thode de la relaxation diverge pour tout param\u00e8tre \\(\\omega\\) dans \\(]0,2]\\) . Consid\u00e9rons donc \u00e0 nouveau le syst\u00e8me : \\(\\begin{pmatrix} -14000 & -7000 & 6000 \\\\ 4000 & -11000 & 1000 \\\\ -3000 & -1000 & 8000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -31314000\\\\ 19859000\\\\ 25443000 \\end{pmatrix}\\) On a montr\u00e9 pr\u00e9c\u00e9demment que dans ce cas on peut d\u00e9composer la matrice \\(A = D-E-F\\) avec : \\(D = \\begin{pmatrix} -14000 & 0 & 0 \\\\ 0 & -11000 & 0 \\\\ 0 & 0 & 8000 \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 \\\\ 4000 & 0 & 0 \\\\ -3000 & -1000 & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & -7000 & 6000 \\\\ 0 & 0 & 1000 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) On peut alors d\u00e9terminer pour diff\u00e9rentes valeurs du param\u00e8tre de relaxation \\(\\omega\\) sur \\(]0,2]\\) le rayon spectral de la matrice d'it\u00e9ration \\(C = (D - \\omega E)^{-1} ((1-\\omega) D + \\omega F)\\) : On observe que le rayon spectral est minimal pour \\(\\omega \\approx 1\\) . On en d\u00e9duit que la m\u00e9thode de Gauss-Seidel est optimale pour la r\u00e9solution de ce syst\u00e8me. Mais ce n'est pas toujours le cas ! Mettons que la position ECEF du satellite 1 ne soit pas \\((x_{s1},y_{s1},z_{s1}) = (15000,13000,18000)\\) , mais plut\u00f4t \\((x_{s1},y_{s1},z_{s1}) = (23000,7000,20000)\\) . Le syst\u00e8me \u00e0 r\u00e9soudre devient alors : \\(\\begin{pmatrix} -22000 & -1000 & 4000 \\\\ -4000 & -5000 & -1000 \\\\ -11000 & 5000 & 6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -73560000\\\\ -22387000\\\\ -16803000 \\end{pmatrix}\\) On remarque que \\(A\\) n'est pas \u00e0 diagonale strictement dominante. Il faudra donc v\u00e9rifier que le rayon spectral de la matrice d'it\u00e9ration choisie est inf\u00e9rieur \u00e0 1 pour assurer la convergence de la m\u00e9thode de la relaxation. On d\u00e9compose la matrice \\(A = D-E-F\\) avec : \\(D = \\begin{pmatrix} -22000 & 0 & 0 \\\\ 0 & -5000 & 0 \\\\ 0 & 0 & 6000 \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 \\\\ -4000 & 0 & 0 \\\\ -11000 & 5000 & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & -1000 & 4000 \\\\ 0 & 0 & -1000 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) On a donc : \\(D-E = \\begin{pmatrix} -22000 & 0 & 0 \\\\ -4000 & -5000 & 0 \\\\ -11000 & 5000 & 6000 \\end{pmatrix}\\) Si nous d\u00e9terminons pour diff\u00e9rentes valeurs de \\(\\omega\\) sur \\(]0,2]\\) le rayon spectral de la matrice d'it\u00e9ration \\(C = (D - \\omega E)^{-1} ((1-\\omega) D + \\omega F)\\) de ce syst\u00e8me, nous obtenons : Cette fois-ci le rayon spectral est minimal pour \\(\\omega \\approx 1.25\\) . On en d\u00e9duit que la formule de r\u00e9currence optimale pour r\u00e9soudre ce syst\u00e8me est : \\(x^{(k+1)} = 1.25 \\times x_G^{(k+1)} - 0.25 \\times x^{(k)}\\) Ce qui donne : \\(\\begin{cases} x_r^{(k+1)} = \\frac{1.25}{-22000} (-73560000 + 1000 y_r^{(k)} - 4000 z_r^{(k)}) - 0.25 x_r^{(k)}\\\\ y_r^{(k+1)} = \\frac{1.25}{-4000} (-22387000 + 4000 x_r^{(k+1)} + 1000 z_r^{(k)}) - 0.25 y_r^{(k)}\\\\ z_r^{(k+1)} = \\frac{1.25}{-11000} (-16803000 + 11000 x_r^{(k+1)} - 5000 y_r^{(k+1)}) - 0.25 z_r^{(k)} \\end{cases}\\) On peut v\u00e9rifier que cette formule converge plus rapidement que la m\u00e9thode de Gauss-Seidel ( \\(\\omega = 1\\) ). Nous choisirons encore : \\(x^{(0)} =\\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}\\) Dans le cas de Gauss-Seidel ( \\(\\omega = 1\\) ), pour obtenir la solution avec une pr\u00e9cision de \\(10^{-3}\\) , 39 it\u00e9rations sont n\u00e9cessaires : It\u00e9ration \\(k\\) \\(x_r^{(k)}\\) \\(y_r^{(k)}\\) \\(z_r^{(k)}\\) 0 0.0000 0.0000 0.0000 1 3343.6364 1802.4909 1827.4242 2 3593.9639 1236.7440 2757.8138 3 3788.8414 894.7641 3400.0725 4 3921.1603 660.4573 3837.9128 5 4011.4179 500.6831 4136.5302 6 4072.9744 391.7144 4340.1911 7 4114.9568 317.3963 4479.0906 8 4143.5894 266.7104 4573.8218 9 4163.1171 232.1419 4638.4298 10 4176.4353 208.5658 4682.4933 11 4185.5185 192.4865 4712.5452 12 4191.7134 181.5203 4733.0410 13 4195.9383 174.0411 4747.0194 14 4198.8198 168.9403 4756.5528 15 4200.7850 165.4614 4763.0547 16 4202.1253 163.0888 4767.4892 17 4203.0394 161.4706 4770.5135 18 4203.6629 160.3670 4772.5761 19 4204.0881 159.6143 4773.9828 20 4204.3780 159.1010 4774.9423 21 4204.5758 158.7509 4775.5966 22 4204.7107 158.5121 4776.0429 23 4204.8027 158.3493 4776.3472 24 4204.8654 158.2382 4776.5548 25 4204.9082 158.1625 4776.6964 26 4204.9374 158.1108 4776.7929 27 4204.9573 158.0756 4776.8588 28 4204.9709 158.0515 4776.9037 29 4204.9801 158.0351 4776.9343 30 4204.9865 158.0240 4776.9552 31 4204.9908 158.0163 4776.9694 32 4204.9937 158.0112 4776.9792 33 4204.9957 158.0076 4776.9858 34 4204.9971 158.0052 4776.9903 35 4204.9980 158.0035 4776.9934 36 4204.9986 158.0024 4776.9955 37 4204.9991 158.0016 4776.9969 38 4204.9994 158.0011 4776.9979 39 4204.9996 158.0008 4776.9986 Pour atteindre la m\u00eame pr\u00e9cision sur la solution avec notre formule optimale, seules 15 it\u00e9rations sont n\u00e9cessaires : It\u00e9ration \\(k\\) \\(x_r^{(k)}\\) \\(y_r^{(k)}\\) \\(z_r^{(k)}\\) 0 0.0000 0.0000 0.0000 1 4179.5455 1417.2045 4601.2453 2 4099.8737 -7.7361 4752.6660 3 4235.1679 175.3496 4834.1459 4 4209.4599 134.9162 4796.9799 5 4209.7375 154.0385 4786.9883 6 4206.3108 155.1825 4780.4417 7 4205.6146 157.2294 4778.3508 8 4205.1971 157.6578 4777.4705 9 4205.0771 157.8908 4777.1728 10 4205.0262 157.9579 4777.0607 11 4205.0096 157.9857 4777.0218 12 4205.0034 157.9948 4777.0077 13 4205.0012 157.9982 4777.0027 14 4205.0004 157.9993 4777.0010 15 4205.0002 157.9998 4777.0003 On peut v\u00e9rifier pour diff\u00e9rentes valeurs de \\(\\omega\\) qu'il s'agit bien du nombre d'it\u00e9rations minimum possible. Exercice : Pour le dernier cas pr\u00e9sent\u00e9 dans cet exemple, calculez le rayon spectral de la matrice de convergence pour \\(\\omega = 1\\) et \\(\\omega = 1.25\\) . Calculez le ratio entre ces 2 valeurs. On a trouv\u00e9 pr\u00e9c\u00e9demment que pour \\(\\omega = 1.25\\) , la m\u00e9thode converge environ 2.7 fois plus rapidement que pour Gauss-Seidel. Qu'en concluez vous ? Conclusion En fonction du rang , et du d\u00e9terminant de la matrice \\(A\\) , un syst\u00e8me lin\u00e9aire \\(A x = b\\) admet une unique, z\u00e9ro ou une infinit\u00e9 de solutions . M\u00eame si un syst\u00e8me admet une solution, elle peut-\u00eatre tr\u00e8s sensible aux perturbations de \\(A\\) ou \\(b\\) . La stabilit\u00e9 d'une solution peut \u00eatre estim\u00e9e en calculant le conditionnement de \\(A\\) . La r\u00e8gle de Cramer donne une expression de explicite de la solution d'un syst\u00e8me, mais elle est inapplicable au-del\u00e0 de 4 inconnues. Les m\u00e9thodes directes d'\u00e9limination transforment la matrice \\(A\\) par une suite d'op\u00e9rations arithm\u00e9tiques afin de se ramener \u00e0 un syst\u00e8me plus simple \u00e0 r\u00e9soudre : triangulaire ou diagonal . Les m\u00e9thodes directes de factorisation d\u00e9composent la matrice \\(A\\) en le produit d'une matrice triangulaire sup\u00e9rieure et une matrice triangulaire inf\u00e9rieure , afin de rendre la r\u00e9solution plus simple. Les m\u00e9thodes it\u00e9ratives construisent une suite convergeant vers la solution \\(x\\) \u00e0 partir de \\(A\\) et \\(b\\) pour n'importe quelle initialisation. Les m\u00e9thodes directes donnent une solution pour tout syst\u00e8me de Cramer, mais sont lourdes en calculs / m\u00e9moire et les erreurs peuvent \u00eatre amplifi\u00e9es \u00e0 chaque op\u00e9ration. Les m\u00e9thodes it\u00e9ratives requi\u00e8rent un nombre fini d'op\u00e9rations, mais elles sont donc soumises aux erreurs de troncature, et les conditions de leur convergence ne sont pas \u00e9videntes.","title":"V. Syst\u00e8mes lin\u00e9aires"},{"location":"Chap5_Systemes_lineaires/#chapitre-v-resolution-de-systemes-dequations-lineaires","text":"Ce chapitre porte sur les m\u00e9thodes num\u00e9riques pour la r\u00e9solution d'un syst\u00e8me lin\u00e9aire d'\u00e9quations.","title":"Chapitre V : R\u00e9solution de syst\u00e8mes d'\u00e9quations lin\u00e9aires"},{"location":"Chap5_Systemes_lineaires/#position-du-probleme","text":"","title":"Position du probl\u00e8me"},{"location":"Chap5_Systemes_lineaires/#motivation","text":"Notre but est de r\u00e9soudre un syst\u00e8me lin\u00e9aire : un ensemble d'\u00e9quations portant sur les m\u00eames inconnues. Un syst\u00e8me de \\(m\\) \u00e9quations lin\u00e9aires \u00e0 \\(n\\) inconnues peut s'\u00e9crire sous la forme suivante : \\(\\begin{cases} a_{1,1} x_1 + a_{1,2} x_2 + ... + a_{1,n} x_n = b_1\\\\ a_{2,1} x_1 + a_{2,2} x_2 + ... + a_{2,n} x_n = b_2\\\\ ...\\\\ a_{m,1} x_1 + a_{m,2} x_2 + ... + a_{m,n} x_n = b_m \\end{cases}\\) avec \\(x_1,x_2,...,x_n\\) les inconnues et \\(a_{i,j}\\) les coefficients du syst\u00e8me. On peut \u00e9galement \u00e9crire ce syst\u00e8me sous forme matricielle : \\(A x = b\\) avec \\(A\\) une matrice de coefficients r\u00e9els de taille \\(m \\times n\\) , \\(x\\) est un vecteur de taille \\(n\\) contenant les variables r\u00e9elles recherch\u00e9es, et \\(b\\) est un vecteur contenant \\(m\\) r\u00e9els. \\(A = \\begin{pmatrix} a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\ a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m,1} & a_{m,2} &\\cdots & a_{m,n} \\end{pmatrix}\\) \\(x = \\begin{pmatrix} x_1\\\\ x_2\\\\ \\vdots\\\\ x_n \\end{pmatrix}\\) \\(b = \\begin{pmatrix} b_1\\\\ b_2\\\\ \\vdots\\\\ b_n \\end{pmatrix}\\) Si \\(m>n\\) , on dit le syst\u00e8me sur-d\u00e9termin\u00e9 . Si \\(m<n\\) , on dit le syst\u00e8me sous-d\u00e9termin\u00e9 .","title":"Motivation"},{"location":"Chap5_Systemes_lineaires/#solution-rang-et-determinant","text":"Face \u00e0 un syst\u00e8me lin\u00e9aire, il y a 3 cas possibles : Le syst\u00e8me n'a pas de solution. Le syst\u00e8me a une infinit\u00e9 de solution. Le syst\u00e8me a une solution unique. On peut savoir dans quel cas on se trouve avec le rang de la matrice \\(A\\) . D\u00e9finition Le rang d'une matrice \\(A\\) est son nombre de vecteurs lignes ou colonnes lin\u00e9airement ind\u00e9pendants. Si \\(A\\) est de dimensions \\(m \\times n\\) , alors \\(rang(A) \\leq min(m,n)\\) . Th\u00e9or\u00e8me de Rouch\u00e9-Fonten\u00e9 Le syst\u00e8me lin\u00e9aire \\(A x = b\\) avec \\(A\\) une matrice de taille \\(m \\times n\\) , \\(x\\) un vecteur de taille \\(n\\) , et \\(b\\) un vecteur de taille \\(m\\) , admet une solution si et seulement si : \\(rang(A) = rang([A \\mid b])\\) Si de plus, \\(rang(A) = n\\) , alors le syst\u00e8me admet une unique solution . Sinon, le syst\u00e8me admet une infinit\u00e9 de solutions. Dans le cas o\u00f9 la matrice \\(A\\) est carr\u00e9e, on a m\u00eame le th\u00e9or\u00e8me suivant : Th\u00e9or\u00e8me Lorsque la matrice \\(A\\) est carr\u00e9e de dimension \\(n \\times n\\) , avec \\(n\\) la taille du vecteur \\(x\\) , le syst\u00e8me lin\u00e9aire \\(A x = b\\) admet une unique solution si et seulement si : le d\u00e9terminant de \\(A\\) not\u00e9 \\(det(A)\\) est non nul. ( \\(A\\) est alors inversible, et on note \\(A^{-1}\\) son inverse) On dit alors que le syst\u00e8me est de Cramer et on peut \u00e9crire : \\(x = A^{-1} b\\) Le syst\u00e8me homog\u00e8ne \\(A x = 0\\) admet toujours le vecteur nul comme solution : Si \\(det(A) \\neq 0\\) c'est l'unique solution. Sinon il y en a une infinit\u00e9. Dans le cas d'un syst\u00e8me non homog\u00e8ne ( \\(b \\neq 0\\) ), si \\(det(A) = 0\\) : Soit \\(rang(A) = rang([A \\mid b])\\) alors il y a une infinit\u00e9 de solutions. Soit \\(rang(A) \\neq rang([A \\mid b])\\) alors il n'y a pas de solution.","title":"Solution, rang et d\u00e9terminant"},{"location":"Chap5_Systemes_lineaires/#conditionnement","text":"M\u00eame quand un syst\u00e8me lin\u00e9aire admet une solution unique, cette solution peut ne pas \u00eatre \"stable\". Un syst\u00e8me est dit \" mal conditionn\u00e9 \" si la solution est extr\u00eamement sensible aux perturbations des coefficients \\(A + \\Delta A\\) et des seconds membres \\(b + \\Delta b\\) . Un d\u00e9terminant petit est souvent indicateur d'un mauvais conditionnement. Pour quantifier la sensibilit\u00e9 de l'erreur relative sur la solution du syst\u00e8me lin\u00e9aire \\(A x = b\\) aux variation de \\(A\\) et \\(b\\) , on peut estimer le conditionnement de la matrice A : \\(\\kappa(A) = \\|A\\| \\|A^{-1}\\|\\) avec une norme matricielle \u00e0 d\u00e9finir. L'erreur relative sur la solution est inf\u00e9rieure \u00e0 l'erreur relative sur les donn\u00e9es multiplis\u00e9e par \\(\\kappa(A)\\) : Si \\(\\kappa(A)\\) est petit (de l'ordre de l'unit\u00e9) on dit que le conditionnement est bon. Si \\(\\kappa(A) >> 1\\) le syst\u00e8me est dit mal conditionn\u00e9. Le calcul du conditionnement d\u00e9pend du choix de la norme : La norme infinie : \\(\\|A\\|_{\\infty} = max_{1 \\leq i \\leq n} \\displaystyle\\sum_{j=1}^{n} |a_{i,j}|\\) La norme 1 : \\(\\|A\\|_1 = max_{1 \\leq j \\leq n} \\displaystyle\\sum_{i=1}^{n} |a_{i,j}|\\) La norme 2 : \\(\\|A\\|_2 = \\|A^T\\|_2 = \\sqrt{\\rho(A A^T)} = \\sqrt{\\rho(A^T A)}\\) \\(\\rho(A)\\) est le rayon spectral de \\(A\\) , que l'on d\u00e9finit comme : \\(\\rho(A) = max_{1 \\leq i \\leq n} |\\lambda_i|\\) avec \\(\\lambda_i\\) les valeurs propres de \\(A\\) . On utilisera surtout le conditionnement de \\(A\\) au sens de la norme 1 et de la norme 2 : \\(\\kappa_1(A) = \\|A\\|_1 \\|A^{-1}\\|_1\\) \\(\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2\\) Pour une matrice carr\u00e9e \\(A\\) d'ordre \\(n\\) inversible, le conditionnement v\u00e9rifie les propri\u00e9t\u00e9s suivantes : \\(\\kappa(A) \\geq 1\\) \\(\\forall \\alpha \\in \\mathbb{R}\\) , \\(\\kappa(\\alpha A) = \\kappa(A)\\) \\(\\kappa(A) = \\kappa(A^{-1})\\) Si on note \\(\\sigma_{min}^2\\) et \\(\\sigma_{max}^2\\) la plus petite et la plus grande valeur propre de \\(A A^T\\) : \\(\\kappa_2(A) = \\frac{\\sigma_{max}}{\\sigma_{min}}\\) Si \\(A\\) est une matrice r\u00e9elle sym\u00e9trique ( \\(A = A^T\\) ), si \\(\\lambda_{min}\\) et \\(\\lambda_{max}\\) sont la plus petite et la plus grande valeur propre de \\(A\\) en valeur absolue, on a : \\(\\kappa_2(A) = \\mid \\frac{\\lambda_{max}}{\\lambda_{min}} \\mid\\) Si \\(A\\) est une matrice orthogonale ( \\(A A^T = A^T A = I\\) ) alors \\(\\kappa_2(A) = 1\\) Effet d'une perturbation de \\(b\\) Si on a une perturbation \\(\\Delta b\\) sur \\(b\\) induisant une erreur \\(\\Delta x\\) sur \\(x\\) , alors on aura la majoration : \\(\\frac{\\Vert \\Delta x \\Vert}{\\Vert x \\Vert} \\leq \\kappa(A) \\frac{\\Vert \\Delta b \\Vert}{\\Vert b \\Vert}\\) Effet d'une perturbation de \\(A\\) Si on a une perturbation \\(\\Delta A\\) sur \\(A\\) induisant une erreur \\(\\Delta x\\) sur \\(x\\) , alors on aura la majoration : \\(\\frac{\\Vert \\Delta x \\Vert}{\\Vert x + \\Delta x \\Vert} \\leq \\kappa(A) \\frac{\\Vert \\Delta A \\Vert}{\\Vert A \\Vert}\\)","title":"Conditionnement"},{"location":"Chap5_Systemes_lineaires/#exemple-de-probleme","text":"Au cours de ce chapitre, nous appliquerons les diff\u00e9rentes m\u00e9thodes d'int\u00e9gration \u00e0 un m\u00eame exemple : Le positionnement par satellites GPS . Nous exprimerons ici les positions en km, avec des coordonn\u00e9es dans le rep\u00e8re cart\u00e9sien ECEF (Earth-Centered Earth-Fixed), ayant pour origine le centre de la Terre. Soit un r\u00e9cepteur au sol dont on veut connaitre la position \\((x_r,y_r,z_r)\\) dans ce rep\u00e8re cart\u00e9sien. Soient 4 satellites de la constellation GPS, dont la position est connue dans ce m\u00eame rep\u00e8re : \\((x_{s1},y_{s1},z_{s1})\\) , \\((x_{s2},y_{s2},z_{s2})\\) , \\((x_{s3},y_{s3},z_{s3})\\) and \\((x_{s4},y_{s4},z_{s4})\\) . Chaque satellite \u00e9met un signal, qui est re\u00e7u avec un certain temps de retard par le r\u00e9cepteur. Ces temps de retard \\((t_1,t_2,t_3,t_4)\\) sont mesur\u00e9s par le r\u00e9cepteur. On admet que les signaux \u00e9mis par chaque satellite se d\u00e9placent \u00e0 vitesse constante jusqu'au r\u00e9cepteur : \\(c = 3.10^5 km/s\\) . La distance euclidienne entre chaque satellite et le r\u00e9cepteur doit \u00eatre \u00e9gale au temps de retard du signal multipli\u00e9 par sa vitesse. On en d\u00e9duit facilement que les diff\u00e9rentes variables sont li\u00e9es par le syst\u00e8me de 4 \u00e9quations suivant : \\(\\begin{cases} (x_r-x_{s1})^2 + (y_r-y_{s1})^2 + (z_r-z_{s1})^2 = (c t_1)^2\\\\ (x_r-x_{s2})^2 + (y_r-y_{s2})^2 + (z_r-z_{s2})^2 = (c t_2)^2\\\\ (x_r-x_{s3})^2 + (y_r-y_{s3})^2 + (z_r-z_{s3})^2 = (c t_3)^2\\\\ (x_r-x_{s4})^2 + (y_r-y_{s4})^2 + (z_r-z_{s4})^2 = (c t_4)^2 \\end{cases}\\) Que l'on peut d\u00e9velopper : \\(\\begin{cases} x_r^2 - 2 x_{s1} x_r + x_{s1}^2 + y_r^2 - 2 y_{s1} y_r + y_{s1}^2 + z_r^2 - 2 z_{s1} z_r + z_{s1}^2 = (c t_1)^2\\\\ x_r^2 - 2 x_{s2} x_r + x_{s2}^2 + y_r^2 - 2 y_{s2} y_r + y_{s2}^2 + z_r^2 - 2 z_{s2} z_r + z_{s2}^2 = (c t_2)^2\\\\ x_r^2 - 2 x_{s3} x_r + x_{s3}^2 + y_r^2 - 2 y_{s3} y_r + y_{s3}^2 + z_r^2 - 2 z_{s3} z_r + z_{s3}^2 = (c t_3)^2\\\\ x_r^2 - 2 x_{s4} x_r + x_{s4}^2 + y_r^2 - 2 y_{s4} y_r + y_{s4}^2 + z_r^2 - 2 z_{s4} z_r + z_{s4}^2 = (c t_4)^2 \\end{cases}\\) En soustrayant la 1\u00e8re \u00e9quation aux 3 autres, on r\u00e9duit le syst\u00e8me \u00e0 3 \u00e9quations : \\(\\begin{cases} x_{s2}^2 + y_{s2}^2 + z_{s2}^2 - 2 x_{s2} x_r - 2 y_{s2} y_r - 2 z_{s2} z_r - x_{s1}^2 - y_{s1}^2 - z_{s1}^2 + 2 x_{s1} x_r + 2 y_{s1} y_r + 2 z_{s1} z_r = (c t_2)^2-(c t_1)^2\\\\ x_{s3}^2 + y_{s3}^2 + z_{s3}^2 - 2 x_{s3} x_r - 2 y_{s3} y_r - 2 z_{s3} z_r - x_{s1}^2 - y_{s1}^2 - z_{s1}^2 + 2 x_{s1} x_r + 2 y_{s1} y_r + 2 z_{s1} z_r = (c t_3)^2-(c t_1)^2\\\\ x_{s4}^2 + y_{s4}^2 + z_{s4}^2 - 2 x_{s4} x_r - 2 y_{s4} y_r - 2 z_{s4} z_r - x_{s1}^2 - y_{s1}^2 - z_{s1}^2 + 2 x_{s1} x_r + 2 y_{s1} y_r + 2 z_{s1} z_r = (c t_4)^2-(c t_1)^2 \\end{cases}\\) Qui revient en regroupant les termes : \\(\\begin{cases} x_{s2}^2 - x_{s1}^2 + y_{s2}^2 - y_{s1}^2 + z_{s2}^2 - z_{s1}^2 - 2 (x_{s2}-x_{s1}) x_r - 2 (y_{s2}-y_{s1}) y_r - 2 (z_{s2}-z_{s1}) z_r = (c t_2)^2-(c t_1)^2\\\\ x_{s3}^2 - x_{s1}^2 + y_{s3}^2 - y_{s1}^2 + z_{s3}^2 - z_{s1}^2 - 2 (x_{s3}-x_{s1}) x_r - 2 (y_{s3}-y_{s1}) y_r - 2 (z_{s3}-z_{s1}) z_r = (c t_3)^2-(c t_1)^2\\\\ x_{s4}^2 - x_{s1}^2 + y_{s4}^2 - y_{s1}^2 + z_{s4}^2 - z_{s1}^2 - 2 (x_{s4}-x_{s1}) x_r - 2 (y_{s4}-y_{s1}) y_r - 2 (z_{s4}-z_{s1}) z_r = (c t_4)^2-(c t_1)^2 \\end{cases}\\) Et en r\u00e9arrangeant on obtient finalement le syst\u00e8me lin\u00e9aire : \\(\\begin{cases} (x_{s2}-x_{s1}) x_r + (y_{s2}-y_{s1}) y_r + (z_{s2}-z_{s1}) z_r = \\frac{1}{2} (x_{s2}^2 - x_{s1}^2 + y_{s2}^2 - y_{s1}^2 + z_{s2}^2 - z_{s1}^2 - (c t_2)^2 + (c t_1)^2)\\\\ (x_{s3}-x_{s1}) x_r + (y_{s3}-y_{s1}) y_r + (z_{s3}-z_{s1}) z_r = \\frac{1}{2} (x_{s3}^2 - x_{s1}^2 + y_{s3}^2 - y_{s1}^2 + z_{s3}^2 - z_{s1}^2 - (c t_3)^2 + (c t_1)^2)\\\\ (x_{s4}-x_{s1}) x_r + (y_{s4}-y_{s1}) y_r + (z_{s4}-z_{s1}) z_r = \\frac{1}{2} (x_{s4}^2 - x_{s1}^2 + y_{s4}^2 - y_{s1}^2 + z_{s4}^2 - z_{s1}^2 - (c t_4)^2 + (c t_1)^2) \\end{cases}\\) avec 3 \u00e9quations et 3 inconnues \\((x_r,y_r,z_r)\\) . On peut \u00e9crire ce syst\u00e8me sous la forme matricielle \\(A x = b\\) : \\(\\begin{pmatrix} x_{s2}-x_{s1} & y_{s2}-y_{s1} & z_{s2}-z_{s1} \\\\ x_{s3}-x_{s1} & y_{s3}-y_{s1} & z_{s3}-z_{s1} \\\\ x_{s4}-x_{s1} & y_{s4}-y_{s1} & z_{s4}-z_{s1} \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} (x_{s2}^2 - x_{s1}^2 + y_{s2}^2 - y_{s1}^2 + z_{s2}^2 - z_{s1}^2 - (c t_2)^2 + (c t_1)^2)\\\\ \\frac{1}{2} (x_{s3}^2 - x_{s1}^2 + y_{s3}^2 - y_{s1}^2 + z_{s3}^2 - z_{s1}^2 - (c t_3)^2 + (c t_1)^2)\\\\ \\frac{1}{2} (x_{s4}^2 - x_{s1}^2 + y_{s4}^2 - y_{s1}^2 + z_{s4}^2 - z_{s1}^2 - (c t_4)^2 + (c t_1)^2) \\end{pmatrix}\\) Admettons que le r\u00e9cepteur GPS se trouve aux coordonn\u00e9es ECEF \\((x_r,y_r,z_r) = (4205,158,4777)\\) , correspondant approximativement \u00e0 la position de l'UFR des Sciences de l'UVSQ. Si la position des 4 satellites est : \\((x_{s1},y_{s1},z_{s1}) = (14000,4000,25000)\\) \\((x_{s2},y_{s2},z_{s2}) = (9000,-14000,21000)\\) \\((x_{s3},y_{s3},z_{s3}) = (24000,6000,15000)\\) \\((x_{s4},y_{s4},z_{s4}) = (10000,16000,19000)\\) Alors le temps de retard associ\u00e9 \u00e0 chaque satellite sera approximativement : \\(t_1 = 0.0759878 s\\) \\(t_2 = 0.0735321 s\\) \\(t_3 = 0.0767739 s\\) \\(t_4 = 0.0735485 s\\) (Il est \u00e0 noter que les positions des satellites ont \u00e9t\u00e9 choisies pour \u00eatre r\u00e9alistes des satellites GPS). Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) C'est ce syst\u00e8me d'\u00e9quations lin\u00e9aires que nous chercherons \u00e0 r\u00e9soudre pour essayer de retrouver la position \\((x_r,y_r,z_r)\\) du r\u00e9cepteur. On peut d\u00e9j\u00e0 v\u00e9rifier que ce syst\u00e8me a bien une unique racine : \\(A\\) est carr\u00e9e de dimensions \\(3 \\times 3\\) , \\(x\\) et \\(b\\) sont de taille 3. \\(det(A) = (-5000 \\times 2000 \\times -6000) + (10000 \\times 12000 \\times -4000) + (-18000 \\times -10000 \\times -4000)\\) \\(- (-4000 \\times 2000 \\times -4000) - (-5000 \\times 12000 \\times -10000) - (10000 \\times -18000 \\times -6000)\\) \\(= -2852000000000 \\neq 0\\) Il s'agit donc d'un syst\u00e8me de Cramer : on a bien unicit\u00e9 de la solution . On peut \u00e9galement v\u00e9rifier si le syst\u00e8me est bien conditionn\u00e9 : \\(\\kappa_1(A) \\approx 5.03\\) \\(\\kappa_2(A) \\approx 2.36\\) Dans les 2 cas, le conditionnement est de l'ordre de l'unit\u00e9 : un a donc un bon conditionnement . Dans la suite de ce chapitre, on utilisera Numpy sous Python pour d\u00e9finir / manipuler les matrices \\(A\\) et \\(b\\) : import numpy as np On d\u00e9finira le vecteur de la position du r\u00e9cepteur GPS \u00e0 retrouver avec : pos_rec = np.array([4205,158,4777],dtype=np.float64) On d\u00e9finira ensuite les vecteurs de positions des satellites GPS avec : pos_sat1 = np.array([14000,4000,25000],dtype=np.float64) #Coordonn\u00e9es du satellite GPS 1 pos_sat2 = np.array([9000,-14000,21000],dtype=np.float64) #Coordonn\u00e9es du satellite GPS 2 pos_sat3 = np.array([24000,6000,15000],dtype=np.float64) #Coordonn\u00e9es du satellite GPS 3 pos_sat4 = np.array([10000,16000,19000],dtype=np.float64) #Coordonn\u00e9es du satellite GPS 4 On pourra alors en d\u00e9duire les temps de retard associ\u00e9s : t1 = ((sum((pos_rec-pos_sat1)**2))**0.5)/3e5 #Temps de retard associ\u00e9 au satellite GPS 1 t2 = ((sum((pos_rec-pos_sat2)**2))**0.5)/3e5 #Temps de retard associ\u00e9 au satellite GPS 2 t3 = ((sum((pos_rec-pos_sat3)**2))**0.5)/3e5 #Temps de retard associ\u00e9 au satellite GPS 3 t4 = ((sum((pos_rec-pos_sat4)**2))**0.5)/3e5 #Temps de retard associ\u00e9 au satellite GPS 4 Et pour finir, on pourra d\u00e9finir les matrices \\(A\\) et \\(b\\) du syst\u00e8me lin\u00e9aire \u00e0 r\u00e9soudre : A = np.vstack((pos_sat2-pos_sat1,pos_sat3-pos_sat1,pos_sat4-pos_sat1)) b_row1 = 0.5*(sum(pos_sat2**2)-sum(pos_sat1**2)-(3e5*t2)**2+(3e5*t1)**2) b_row2 = 0.5*(sum(pos_sat3**2)-sum(pos_sat1**2)-(3e5*t3)**2+(3e5*t1)**2) b_row3 = 0.5*(sum(pos_sat4**2)-sum(pos_sat1**2)-(3e5*t4)**2+(3e5*t1)**2) b = np.array([b_row1,b_row2,b_row3])","title":"Exemple de probl\u00e8me"},{"location":"Chap5_Systemes_lineaires/#la-regle-de-cramer","text":"","title":"La r\u00e8gle de Cramer"},{"location":"Chap5_Systemes_lineaires/#theoreme","text":"La r\u00e8gle de Cramer (ou m\u00e9thode de Cramer) est un th\u00e9or\u00e8me d'alg\u00e8bre lin\u00e9aire qui donne la solution d'un syst\u00e8me de Cramer : Th\u00e9or\u00e8me de la r\u00e8gle de Cramer Le syst\u00e8me de Cramer \\(A x = b\\) avec \\(A\\) matrice carr\u00e9e de taille \\(n \\times n\\) \\(x\\) vecteur de taille \\(n\\) \\(b\\) vecteur de taille \\(n\\) admet une solution si et seulement si \\(A\\) est inversible. Cette solution est donn\u00e9e par : \\(x_i = \\frac{det(A_i)}{det(A)}\\) pour \\(i=1,...,n\\) o\u00f9 \\(A_i\\) est la matrice carr\u00e9e form\u00e9e en rempla\u00e7ant la i-\u00e8me colonne de \\(A\\) par le vecteur \\(b\\) . Lorsque le syst\u00e8me n'est pas de Cramer (donc si \\(det(A)=0\\) ) : Si le d\u00e9terminant d'une des matrices \\(A_i\\) est nul alors le syst\u00e8me n'a pas de solution. La r\u00e9ciproque est fausse : il peut arriver qu'un syst\u00e8me n'ait pas de solution alors que tous les \\(det(A_i)\\) sont non-nuls. Cette m\u00e9thode est tr\u00e8s couteuse en nombre d'op\u00e9rations et devient donc inapplicable \u00e0 de grands syst\u00e8mes (plus de 4 \u00e9quations).","title":"Th\u00e9or\u00e8me"},{"location":"Chap5_Systemes_lineaires/#algorithme-n3","text":"Dans cette section, nous pr\u00e9senterons les algorithmes permettant d'appliquer la m\u00e9thode de Cramer dans le cas d'une matrice de dimensions \\((3 \\times 3)\\) : 3 \u00e9quations et 3 inconnues. Le d\u00e9terminant d'une matrice de dimensions \\(3 \\times 3\\) peut \u00eatre calcul\u00e9 \u00e0 l'aide de la fonction Python suivante. Cette fonction prend en entr\u00e9e : A la matrice de dimensions \\(3 \\times 3\\) dont on veut trouver le d\u00e9terminant. Elle se base simplement sur la formule du d\u00e9terminant d'une matrice \\(3 \\times 3\\) . def det_3(A): return A[0,0]*A[1,1]*A[2,2]+A[0,1]*A[1,2]*A[2,0]+A[0,2]*A[1,0]*A[2,1]-A[0,2]*A[1,1]*A[2,0]-A[0,1]*A[1,0]*A[2,2]-A[0,0]*A[1,2]*A[2,1] Voici l'algorithme de Cramer pour une matrice de dimensions \\(3 \\times 3\\) sous la forme d'une fonction Python. Cette fonction prend en entr\u00e9e : A la matrice de dimensions \\(3 \\times 3\\) des coefficients du syst\u00e8me. b le vecteur de dimension 3 du second membre du syst\u00e8me. def cramer_3(A,b): #V\u00e9rification des dimensions de A (3x3) et b (3) : if (np.shape(A)!=(3,3))or(len(b)!=3): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Calculer le d\u00e9terminant de A : det_A = det_3(A) #V\u00e9rifier que le syst\u00e8me admet bien une unique solution : if det_A==0: raise ValueError(\"Le syst\u00e8me n'admet pas une solution unique !\") #Initialiser le vecteur qui contiendra les 3 solutions du syst\u00e8me : x = np.array([0,0,0],dtype=np.float64) #Boucle sur les 3 colonnes de la matrice A : for i in range(3): #Remplir la matrice A_i avec les \u00e9l\u00e9ments de A : A_i = np.copy(A) #Remplacer la i-\u00e8me colonne de A_i avec les \u00e9l\u00e9ments de b : A_i[:,i] = b #Calculer la valeur de la i-\u00e8me inconnue du syst\u00e8me : x[i] = det_3(A_i)/det_A #Renvoyer le vecteur contenant les 3 solutions du syst\u00e8me : return x","title":"Algorithme (n=3)"},{"location":"Chap5_Systemes_lineaires/#exemple","text":"Avant d'appliquer la m\u00e9thode Cramer \u00e0 notre probl\u00e8me exemple, il convient de v\u00e9rifier que celle-ci est bien applicable. On rappelle que nous avons montr\u00e9 pr\u00e9c\u00e9demment que nous avons ici affaire \u00e0 un syst\u00e8me de Cramer car : \\(A\\) est carr\u00e9e de dimensions \\(3 \\times 3\\) , \\(x\\) et \\(b\\) sont de taille 3. \\(det(A) = -2852000000000 \\neq 0\\) La solution est par cons\u00e9quent unique, et nous pouvons appliquer la m\u00e9thode de Cramer. Tout d'abord, nous construisons \\(A_1\\) , \\(A_2\\) et \\(A_3\\) : \\(A_1 = \\begin{pmatrix} -42977000 & -18000 & -4000 \\\\ -5404000 & 2000 & -10000 \\\\ -43586000 & 12000 & -6000 \\end{pmatrix}\\) \\(A_2 = \\begin{pmatrix} -5000 & -42977000 & -4000 \\\\ 10000 & -5404000 & -10000 \\\\ -4000 & -43586000 & -6000 \\end{pmatrix}\\) \\(A_3 = \\begin{pmatrix} -5000 & -18000 & -42977000 \\\\ 10000 & 2000 & -5404000 \\\\ -4000 & 12000 & -43586000 \\end{pmatrix}\\) On peut alors calculer que : \\(det(A_1) = -11992660000000000\\) \\(det(A_2) = -450616000000000\\) \\(det(A_3) = -13624004000000000\\) On en d\u00e9duit que : \\(x_r = \\frac{det(A_1)}{det(A)} = \\frac{-11992660000000000}{-2852000000000} = 4205\\) \\(y_r = \\frac{det(A_2)}{det(A)} = \\frac{-450616000000000}{-2852000000000} = 158\\) \\(z_r = \\frac{det(A_3)}{det(A)} = \\frac{-13624004000000000}{-2852000000000} = 4777\\) Exercice : Introduisez une erreur de 10 km dans les valeurs de \\(A\\) , et appliquez de nouveau la m\u00e9thode de Cramer au syst\u00e8me. Comment ces erreurs se r\u00e9percutent-elles sur l'estimation de \\((x_r,y_r,z_r)\\) ? Ce r\u00e9sultat \u00e9tait-il attendu d'apr\u00e8s le conditionnement de \\(A\\) ?","title":"Exemple"},{"location":"Chap5_Systemes_lineaires/#methodes-directes-delimination","text":"","title":"M\u00e9thodes directes d'\u00e9limination"},{"location":"Chap5_Systemes_lineaires/#proprietes-des-systemes-lineaires","text":"Les m\u00e9thodes dites \"d' \u00e9limination \" pour la r\u00e9solution de syst\u00e8mes lin\u00e9aires se basent sur 4 grandes propri\u00e9t\u00e9s de ces syst\u00e8mes. La solution d'un syst\u00e8me lin\u00e9aire \\(A x = b\\) reste inchang\u00e9e lorsque l'on applique les op\u00e9rations suivantes : Permutation de lignes Permuter 2 lignes de \\(A\\) et les \u00e9l\u00e9ments correspondants de \\(b\\) revient \u00e0 permuter 2 \u00e9quations. Permutation de colonnes Permuter 2 colonnes de \\(A\\) et les \u00e9l\u00e9ments correspondants de \\(x\\) revient \u00e0 permuter 2 inconnues. Addition d'une ligne \u00e0 une autre Ajouter une ligne de \\(A\\) \u00e0 une autre, et ajouter les \u00e9l\u00e9ments correspondants de \\(b\\) , revient \u00e0 additionner une \u00e9quation \u00e0 une autre. Multiplication d'une ligne par un r\u00e9el non nul Multiplier une ligne de \\(A\\) et les \u00e9l\u00e9ments correspondants de \\(b\\) par un r\u00e9el non nul revient \u00e0 multipler une \u00e9quation par ce r\u00e9el. L'id\u00e9e derri\u00e8re les m\u00e9thodes d'\u00e9limination est d'utiliser ces op\u00e9ration pour construire une matrice \\(A^*\\) modifi\u00e9e, triangulaire ou diagonale , afin de se ramener \u00e0 un syst\u00e8me simple \u00e0 r\u00e9soudre .","title":"Propri\u00e9t\u00e9s des syst\u00e8mes lin\u00e9aires"},{"location":"Chap5_Systemes_lineaires/#pivot-de-gauss","text":"","title":"Pivot de Gauss"},{"location":"Chap5_Systemes_lineaires/#idee","text":"L'algorithme du pivot de Gauss a pour but de transformer le syst\u00e8me en un syst\u00e8me triangulaire \u00e0 l'aide d'op\u00e9rations sur les lignes (et \u00e9ventuellement sur les colonnes). Il s'agit donc d'une m\u00e9thode de triangularisation . Une fois la matrice triangularis\u00e9e, le syst\u00e8me \u00e0 r\u00e9soudre devient : \\(\\begin{cases} a_{1,1}^* x_1 + a_{1,2}^* x_2 + ... + a_{1,n}^* x_n = b_1^*\\\\ a_{2,2}^* x_2 + a_{2,3}^* x_3 + ... + a_{2,n}^* x_n = b_2^*\\\\ ...\\\\ a_{n-1,n-1}^* x_{n-1} + a_{n-1,n}^* x_n = b_{n-1}^*\\\\ a_{n,n}^* x_n = b_n^* \\end{cases}\\) o\u00f9 les \\(a_{i,j}*\\) sont les coefficients de la matrice modifi\u00e9e \\(A^*\\) , et les \\(b_i^*\\) les \u00e9l\u00e9ments du vecteur modifi\u00e9 \\(b^*\\) . Pour r\u00e9soudre ce syst\u00e8me, il suffit alors d'effectuer les calculs de \" remont\u00e9e \" suivants : \\(\\begin{cases} x_n = \\frac{b_n^*}{a_{n,n}^*}\\\\ x_{n-1} = \\frac{1}{a_{n-1,n-1}^*} (b_{n-1}^* - a_{n-1,n}^* x_n)\\\\ ...\\\\ x_i = \\frac{1}{a_{i,i}^*} (b_i^* - \\displaystyle\\sum_{j=i+1}^{n} a_{i,j}^* x_j)\\\\ ...\\\\ x_1 = \\frac{1}{a_{1,1}^*} (b_1^* - \\displaystyle\\sum_{j=2}^{n} a_{1,j}^* x_j) \\end{cases}\\) Pour triangulariser la matrice \\(A\\) , on r\u00e9p\u00e8te ces op\u00e9rations pour chaque colonne \\(j\\) : Op\u00e9rations du pivot de Gauss - On choisit une valeur non-nulle dans la colonne \\(j\\) , d'indice sup\u00e9rieur ou \u00e9gal \u00e0 \\(j\\) , que l'on appellera pivot . - On ram\u00e8ne le pivot sur la ligne \\(j\\) en effectuant si n\u00e9cessaire un changement de ligne. - On effectue les op\u00e9rations suivantes sur les lignes d'indice \\(j < k \\leq n\\) : \\(L_k = L_k - \\frac{a_{k,j}}{a_{jj}} L_j\\) On passe \u00e0 la colonne suivante, jusqu'\u00e0 l'avant-derni\u00e8re. Pour r\u00e9duire les erreurs li\u00e9es aux arrondis, on peut adopter plusieurs strat\u00e9gies pour le choix du pivot : Sans pivotage : on ne r\u00e9alise ni permutations de lignes, ni permutations de colonnes. Le pivot est toujours s\u00e9l\u00e9ctionn\u00e9 sur la diagonale de la matrice. Le pivot partiel : on choisi le pivot comme \u00e9tant l'\u00e9l\u00e9ment de valeur absolue maximale de la colonne sur ou sous la diagonale. Cette strat\u00e9gie n'implique que des permutations de lignes. Le pivot total : on choisi le pivot comme \u00e9tant l'\u00e9l\u00e9ment de valeur absolue maximale sur toute la portion de matrice non-triangularis\u00e9e. Cette strat\u00e9gie implique des permutations de lignes et de colonnes. Choisir le pivot le plus grand possible assure que les coefficients de \\(A\\) et \\(A^*\\) soient de m\u00eame magnitude relative , r\u00e9duisant ainsi la propagation des erreurs d'arrondis. L'algorithme du pivot partiel est le plus commun\u00e9ment utilis\u00e9. La triangularisation d'une matrice \\(A\\) de dimensions \\(n \\times n\\) requiert de l'ordre de \\(\\frac{2 n^3}{3}\\) op\u00e9rations. La remont\u00e9e requiert de l'ordre de \\(n^2\\) op\u00e9rations.","title":"Id\u00e9e"},{"location":"Chap5_Systemes_lineaires/#algorithmes","text":"Voici sous la forme de fonctions Python les algorithmes d'\u00e9limination de Gauss sans pivotage, avec pivot partiel et avec pivot total. Toutes ces fonctions prennent en entr\u00e9e un syst\u00e8me de Cramer : A la matrice des coefficients du syst\u00e8me. b le vecteur du second membre du syst\u00e8me. Voici l'algorithme d'\u00e9limination de Gauss sans pivotage : def gauss_sans_pivot(A,b): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Copier A et b pour ne pas modifier les matrices originales : A_2 = np.copy(A) b_2 = np.copy(b) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur sur la diagonale de la j-\u00e8me colonne : pivot = A_2[j,j] #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Op\u00e9rations sur les lignes de A et b en utilisant le pivot : b_2[k] = b_2[k] - b_2[j]*A_2[k,j]/pivot A_2[k,:] = A_2[k,:] - A_2[j,:]*A_2[k,j]/pivot #Renvoyer les matrices A et b modifi\u00e9es : return A_2,b_2 Voici l'algorithme d'\u00e9limination de Gauss avec pivot partiel : def gauss_pivot_partiel(A,b): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Copier A et b pour ne pas modifier les matrices originales : A_2 = np.copy(A) b_2 = np.copy(b) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur maximale en absolu sur la colonne, #sur la j-i\u00e8me ligne ou en dessous : idx_pivot = np.argmax(abs(A_2[j:,j]))+j #Indice de la ligne du pivot pivot = A_2[idx_pivot,j] #Valeur du pivot #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Si le pivot n'est pas sur la j-i\u00e8me ligne, \u00e9changer la j-i\u00e8me et la #ligne du pivot : if idx_pivot!=j: A_2[[j,idx_pivot]] = A_2[[idx_pivot,j]] #Pour la matrice A b_2[[j,idx_pivot]] = b_2[[idx_pivot,j]] #Pour le vecteur b #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Op\u00e9rations sur les lignes de A et b en utilisant le pivot : b_2[k] = b_2[k] - b_2[j]*A_2[k,j]/pivot A_2[k,:] = A_2[k,:] - A_2[j,:]*A_2[k,j]/pivot #Renvoyer les matrices A et b modifi\u00e9es : return A_2,b_2 Voici l'algorithme d'\u00e9limination de Gauss avec pivot total : def gauss_pivot_total(A,b): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Copier A et b pour ne pas modifier les matrices originales : A_2 = np.copy(A) b_2 = np.copy(b) #Cr\u00e9er un vecteur contenant l'ordre des inconnues dans x (de 0 \u00e0 n-1) : idx_x = np.arange(n) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur maximale en absolu sur la #portion de matrice non-triangularis\u00e9e : idx_pivot = np.argmax(abs(A_2[j:,j:])) #Indice du pivot ligne_pivot = idx_pivot//(n-j)+j colonne_pivot = idx_pivot%(n-j)+j pivot = A_2[ligne_pivot,colonne_pivot] #Valeur du pivot #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Si le pivot n'est pas sur la j-i\u00e8me ligne, \u00e9changer la j-i\u00e8me et la #ligne du pivot : if ligne_pivot!=j: A_2[[j,ligne_pivot]] = A_2[[ligne_pivot,j]] #Pour la matrice A b_2[[j,ligne_pivot]] = b_2[[ligne_pivot,j]] #Pour le vecteur b #Si le pivot n'est pas sur la j-i\u00e8me colonne, \u00e9changer la j-i\u00e8me et la #colonne du pivot : if colonne_pivot!=j: A_2[:,[j,colonne_pivot]] = A_2[:,[colonne_pivot,j]] #Pour la matrice A idx_x[[j,colonne_pivot]] = idx_x[[colonne_pivot,j]] #Pour les \u00e9l\u00e9ments de x #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Op\u00e9rations sur les lignes de A et b en utilisant le pivot : b_2[k] = b_2[k] - b_2[j]*A_2[k,j]/pivot A_2[k,:] = A_2[k,:] - A_2[j,:]*A_2[k,j]/pivot #D\u00e9terminer les indices de x permettant d'obtenir les inconnues dans l'ordre : ordre_x = np.argsort(idx_x) #Renvoyer les matrices A et b modifi\u00e9es, et l'ordre des inconnues : return A_2,b_2,ordre_x Une fois, le syst\u00e8me triangularis\u00e9 par une des m\u00e9thodes d'\u00e9limination de Gauss, il faut lui appliquer l'algorithme de remont\u00e9e pour le r\u00e9soudre. Voici l'algorithme de remont\u00e9e sous la forme d'une fonction Python : def remontee(A,b): #R\u00e9cup\u00e9rer le nombre n d'\u00e9quations / inconnues du syst\u00e8me : n = len(A) #Initialiser le vecteur qui contiendra les solutions du syst\u00e8me : x = np.zeros(n,dtype=np.float64) #Boucle sur les lignes de la matrice A, de n \u00e0 1 : for i in range(n-1,-1,-1): #D\u00e9termination de la i-\u00e8me inconnue : x[i] = (b[i]-sum(A[i,i+1:n]*x[i+1:n]))/A[i,i] #Renvoyer le vecteur contenant les solutions du syst\u00e8me : return x Dans le cas du pivot total, pour r\u00e9cup\u00e9rer les inconnues dans l'ordre, il faudra utiliser le vecteur retourn\u00e9 par la fonction \"gauss_pivot_total\" : A_2,b_2,ordre_x = gauss_pivot_total(A,b) x_2 = remontee(A_2,b_2) x_2 = x_2[ordre_x]","title":"Algorithmes"},{"location":"Chap5_Systemes_lineaires/#exemple_1","text":"Nous allons appliquer chacun des 3 algorithmes d'\u00e9limination de Gauss (sans pivotage, avec pivot partiel, avec pivot total) \u00e0 notre probl\u00e8me exemple. On rappelle que nous avons initialement le syst\u00e8me de Cramer \\(A x = b\\) suivant : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) Sans pivotage : 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant sur la diagonale : -5000. On r\u00e9alise les op\u00e9rations suivantes : \\(L_2 = L_2 - L_1 \\times \\frac{10000}{-5000}\\) \\(L_3 = L_3 - L_1 \\times \\frac{-4000}{-5000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 0 & -34000 & -18000 \\\\ 0 & 26400 & -2800 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -91358000\\\\ -9204400 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9l\u00e9ctionne le pivot comme \u00e9tant sur la diagonale : -34000. On r\u00e9alise l'op\u00e9ration suivante : \\(L_3 = L_3 - L_2 \\times \\frac{26400}{-34000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 0 & -34000 & -18000 \\\\ 0 & 0 & -16776.47 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -91358000\\\\ -80141200 \\end{pmatrix}\\) On obtient bien un syst\u00e8me triangulaire auquel on peut appliquer l'algorithme de remont\u00e9e. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On en d\u00e9duit alors les solutions par l'algorithme de remont\u00e9e : \\(\\begin{cases} z_r = \\frac{-80141200}{-16776.47} = 4777\\\\ y_r = \\frac{1}{-34000} (-91358000 - (-18000 \\times z_r)) = 158\\\\ x_r = \\frac{1}{-5000} (-42977000 - (-18000 \\times y_r) - (-4000 \\times z_r)) = 4205 \\end{cases}\\) Pivot partiel : 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : 10000. On \u00e9change la ligne 1 et la ligne 2 pour faire passer le pivot sur la diagonale. Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 10000 & 2000 & -10000 \\\\ -5000 & -18000 & -4000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -5404000\\\\ -42977000\\\\ -43586000 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_2 = L_2 - L_1 \\times \\frac{-5000}{10000}\\) \\(L_3 = L_3 - L_1 \\times \\frac{-4000}{10000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 10000 & 2000 & -10000 \\\\ 0 & -17000 & -9000 \\\\ 0 & 12800 & -10000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -5404000\\\\ -45679000\\\\ -45747600 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : -17000. On r\u00e9alise l'op\u00e9ration suivante : \\(L_3 = L_3 - L_2 \\times \\frac{12800}{-17000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 10000 & 2000 & -10000 \\\\ 0 & -17000 & -9000 \\\\ 0 & 0 & -16776.47 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -5404000\\\\ -45679000\\\\ -80141200 \\end{pmatrix}\\) On obtient bien un syst\u00e8me triangulaire auquel on peut appliquer l'algorithme de remont\u00e9e. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On en d\u00e9duit alors les solutions par l'algorithme de remont\u00e9e : \\(\\begin{cases} z_r = \\frac{-80141200}{-16776.47} = 4777\\\\ y_r = \\frac{1}{-17000} (-45679000 - (-9000 \\times z_r)) = 158\\\\ x_r = \\frac{1}{10000} (-5404000 - (2000 \\times y_r) - (-10000 \\times z_r)) = 4205 \\end{cases}\\) Pivot total : 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la portion de matrice non-triangularis\u00e9e : -18000. On \u00e9change la colonne 1 et la colonne 2 pour faire passer le pivot sur la diagonale. Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -18000 & -5000 & -4000 \\\\ 2000 & 10000 & -10000 \\\\ 12000 & -4000 & -6000 \\end{pmatrix} \\begin{pmatrix} y_r\\\\ x_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_2 = L_2 - L_1 \\times \\frac{2000}{-18000}\\) \\(L_3 = L_3 - L_1 \\times \\frac{12000}{-18000}\\) \\(\\begin{pmatrix} -18000 & -5000 & -4000 \\\\ 0 & 9444.44 & -10444.44 \\\\ 0 & -7333.33 & -8666.67 \\end{pmatrix} \\begin{pmatrix} y_r\\\\ x_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -10179222.22\\\\ -72237333.33 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la portion de matrice non-triangularis\u00e9e : -10444.44. On \u00e9change la colonne 2 et la colonne 3 pour faire passer le pivot sur la diagonale. Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -18000 & -4000 & -5000 \\\\ 0 & -10444.44 & 9444.44 \\\\ 0 & -8666.67 & -7333.33 \\end{pmatrix} \\begin{pmatrix} y_r\\\\ z_r\\\\ x_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -10179222.22\\\\ -72237333.33 \\end{pmatrix}\\) On r\u00e9alise l'op\u00e9ration suivante : \\(L_3 = L_3 - L_2 \\times \\frac{-8666.67}{-10444.44}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -18000 & -4000 & -5000 \\\\ 0 & -10444.44 & 9444.44 \\\\ 0 & 0 & -15170.21 \\end{pmatrix} \\begin{pmatrix} y_r\\\\ z_r\\\\ x_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -10179222.22\\\\ -63790744.68 \\end{pmatrix}\\) On obtient bien un syst\u00e8me triangulaire auquel on peut appliquer l'algorithme de remont\u00e9e. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On en d\u00e9duit alors les solutions par l'algorithme de remont\u00e9e : \\(\\begin{cases} x_r = \\frac{-63790744.68}{-15170.21} = 4205\\\\ z_r = \\frac{1}{-10444.44} (-10179222.22 - (9444.44 \\times x_r)) = 4777\\\\ y_r = \\frac{1}{-18000} (-63790744.68 - (-4000 \\times z_r) - (-5000 \\times x_r)) = 158 \\end{cases}\\) Exercice : Pourquoi choisir le pivot le plus grand possible ? Dans le cas de notre probl\u00e8me exemple, les coefficients de \\(A\\) sont tous du m\u00eame ordre de grandeur, et nous n'avons pas de gros probl\u00e8mes d'arrondis. Le r\u00e9sutat obtenu est donc le m\u00eame pour les 3 m\u00e9thodes d'\u00e9limination de Gauss (sans pivotage, avec pivot partiel, avec pivot total). Essayez \u00e0 la main d'appliquer au syst\u00e8me suivant l'\u00e9limination de Gauss sans pivotage puis avec pivot partiel, pour 4 chiffres significatifs sur tous les calculs : \\(\\begin{cases} 1.308 x_1 + 4.951 x_2 = 6.259\\\\ 27.05 x_1 + 1.020 x_2 = 28.07 \\end{cases}\\) La solution de ce syst\u00e8me est \u00e9vidente : \\(x_1 = 1.000\\) et \\(x_2 = 1.000\\) . Retrouvez-vous ce r\u00e9sultat avec la m\u00e9thode sans pivotage ? Avec pivot partiel ? Quelle est la cause de cette diff\u00e9rence ?","title":"Exemple"},{"location":"Chap5_Systemes_lineaires/#elimination-de-gauss-jordan","text":"","title":"Elimination de Gauss-Jordan"},{"location":"Chap5_Systemes_lineaires/#idee_1","text":"L'algorithme de Gauss-Jordan a pour but de transformer le syst\u00e8me en un syst\u00e8me \u00e9chelonn\u00e9 r\u00e9duit \u00e0 l'aide d'op\u00e9rations \u00e9l\u00e9mentaires sur les lignes (et \u00e9ventuellement sur les colonnes). L'id\u00e9e est de pousser plus loin les \u00e9liminations que la m\u00e9thode de Gauss, pour construire une matrice \\(A^*\\) de la forme : \\(\\begin{pmatrix} 1 & * & 0 & 0 & * & 0\\\\ 0 & 0 & 1 & 0 & * & 0\\\\ 0 & 0 & 0 & 1 & * & 0\\\\ 0 & 0 & 0 & 0 & 0 & 1\\\\ 0 & 0 & 0 & 0 & 0 & 0\\\\ \\end{pmatrix}\\) Il faut donc ajouter \u00e0 l'\u00e9limination de Gauss vue pr\u00e9c\u00e9demment : Une division de la ligne du pivot par le pivot, pour que le pivot soit \u00e9gal \u00e0 1. Des op\u00e9rations sur les lignes au-dessus de la ligne du pivot. Il s'agit d'une m\u00e9thode de diagonalisation . Si la matrice \\(A\\) est carr\u00e9e inversible de taille \\(n \\times n\\) , sa forme \u00e9chelonn\u00e9e r\u00e9duite est la matrice identit\u00e9 de taille \\(n \\times n\\) . Le nombre d'op\u00e9rations de l'algorithme de Gauss-Jordan est de l'ordre de \\(n^3\\) au lieu de \\(\\frac{2}{3} n^3\\) pour l'\u00e9limination de Gauss. Mais avec l'\u00e9limination de Gauss-Jordan, la r\u00e9solution du syst\u00e8me est imm\u00e9diate : la solution est directement \\(x = b^*\\) .","title":"Id\u00e9e"},{"location":"Chap5_Systemes_lineaires/#algorithme","text":"Comme pour l'\u00e9limination de Gauss, l'\u00e9limination de Gauss-Jordan peut se d\u00e9cliner sous 3 formes suivant la strat\u00e9gie choix du pivot : sans pivotage, avec pivot partiel, avec pivot total. Nous donnerons ici l'algorithme de l'\u00e9limination de Gauss-Jordan avec pivot partiel, qui est le plus commun\u00e9ment utilis\u00e9. Le voici sous la forme d'une fonction Python, qui prend en entr\u00e9e un syst\u00e8me de Cramer : A la matrice des coefficients du syst\u00e8me. b le vecteur du second membre du syst\u00e8me. def gauss_jordan(A,b): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #Copier A et b pour ne pas modifier les matrices originales : A_2 = np.copy(A) b_2 = np.copy(b) #Boucle sur les colonnes de la matrice A : for j in range(n): #S\u00e9lection du pivot comme \u00e9tant la valeur maximale en absolu sur la colonne, #sur la j-i\u00e8me ligne ou en dessous : idx_pivot = np.argmax(abs(A_2[j:,j]))+j #Indice de la ligne du pivot pivot = A_2[idx_pivot,j] #Valeur du pivot #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Division de la ligne du pivot par le pivot, pour que le pivot soit \u00e9gal \u00e0 1 : A_2[idx_pivot,:] = A_2[idx_pivot,:]/pivot #Pour la matrice A b_2[idx_pivot] = b_2[idx_pivot]/pivot #Pour le vecteur b #Si le pivot n'est pas sur la j-i\u00e8me ligne, \u00e9changer la j-i\u00e8me et la #ligne du pivot : if idx_pivot!=j: A_2[[j,idx_pivot]] = A_2[[idx_pivot,j]] #Pour la matrice A b_2[[j,idx_pivot]] = b_2[[idx_pivot,j]] #Pour le vecteur b #Boucle sur toutes les lignes sauf celle du pivot : for k in range(n): if k!=j: #Op\u00e9rations sur les lignes de A et b : b_2[k] = b_2[k] - b_2[j]*A_2[k,j] A_2[k,:] = A_2[k,:] - A_2[j,:]*A_2[k,j] #Renvoyer les matrices A et b modifi\u00e9es : return A_2,b_2 Il n'y a pas besoin d'un algorithme de remont\u00e9e ici, puisque les solutions seront directement les valeurs du vecteur \"b_2\" en sortie.","title":"Algorithme"},{"location":"Chap5_Systemes_lineaires/#exemple_2","text":"Nous allons appliquer l'algorithme d'\u00e9limination de Gauss-Jordan (avec pivot partiel) \u00e0 notre probl\u00e8me exemple. On rappelle que nous avons initialement le syst\u00e8me de Cramer \\(A x = b\\) suivant : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : 10000. On divise la ligne du pivot par le pivot : \\(L_2 = \\frac{L_2}{10000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 1 & 0.2 & -1 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -540.4\\\\ -43586000 \\end{pmatrix}\\) On \u00e9change la ligne 1 et la ligne 2 pour faire passer le pivot sur la diagonale. Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0.2 & -1 \\\\ -5000 & -18000 & -4000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -540.4\\\\ -42977000\\\\ -43586000 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_2 = L_2 - L_1 \\times -5000\\) \\(L_3 = L_3 - L_1 \\times -4000\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0.2 & -1 \\\\ 0 & -17000 & -9000 \\\\ 0 & 12800 & -10000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -540.4\\\\ -45679000\\\\ -45747600 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : -17000. On divise la ligne du pivot par le pivot : \\(L_2 = \\frac{L_2}{-17000}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0.2 & -1 \\\\ 0 & 1 & 0.5294 \\\\ 0 & 12800 & -10000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -540.4\\\\ 2687\\\\ -45747600 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_1 = L_1 - L_2 \\times 0.2\\) \\(L_3 = L_3 - L_2 \\times 12800\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0 & -1.1059 \\\\ 0 & 1 & 0.5294 \\\\ 0 & 0 & -16776.47 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -1077.8\\\\ 2687\\\\ -80141200 \\end{pmatrix}\\) 3\u00e8me it\u00e9ration : nous continuons avec la colonne 3. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale : -16776.47. On divise la ligne du pivot par le pivot : \\(L_3 = \\frac{L_3}{-16776.47}\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0 & -1.1059 \\\\ 0 & 1 & 0.5294 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -1077.8\\\\ 2687\\\\ 4777 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : \\(L_1 = L_1 - L_3 \\times -1.1059\\) \\(L_2 = L_2 - L_3 \\times 0.5294\\) Le syst\u00e8me devient alors : \\(\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} 4205\\\\ 158\\\\ 4777 \\end{pmatrix}\\) On trouve alors directement la solution du syst\u00e8me : \\(x = \\begin{pmatrix} 4205\\\\ 158\\\\ 4777 \\end{pmatrix}\\) Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : Exercice : En vous inspirant des fonctions Python pr\u00e9c\u00e9dentes, impl\u00e9mentez la m\u00e9thode de Gauss-Jordan avec pivot total, puis appliquez-la \u00e0 notre probl\u00e8me exemple. V\u00e9rifiez que vous retrouvez bien le r\u00e9sultat attendu.","title":"Exemple"},{"location":"Chap5_Systemes_lineaires/#methodes-directes-de-factorisation-decomposition","text":"","title":"M\u00e9thodes directes de factorisation / d\u00e9composition"},{"location":"Chap5_Systemes_lineaires/#equivalence-elimination-factorisation","text":"Les op\u00e9rations d'\u00e9limination : On remarque qu'appliquer l'op\u00e9ration \\(L_2 = L_2 - \\frac{a_{2,1}}{a_{1,1}} L_1\\) \u00e0 un syst\u00e8me \\(A x = b\\) de dimensions \\(n \\times n\\) revient \u00e0 multiplier \\(A\\) et \\(b\\) par la matrice : \\(M = \\begin{pmatrix} 1 & 0 & 0 \\\\ -a_{2,1}/a_{1,1} & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) Si on notre \\(A^{(k)}\\) et \\(b^{(k)}\\) les matrices avant l'op\u00e9ration, et \\(A^{(k+1)}\\) et \\(b^{(k+1)}\\) les matrices apr\u00e8s l'op\u00e9ration, on a : \\(A^{(k+1)} = M A^{(k)}\\) et \\(b^{(k+1)} = M b^{(k)}\\) Les permutations : On remarque que permutter les lignes \\(L_2\\) et \\(L_3\\) d'un syst\u00e8me \\(A x = b\\) de dimension \\(n \\times n\\) revient \u00e0 multiplier \\(A\\) et \\(b\\) par la matrice : \\(P = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix}\\) Si on notre \\(A^{(k)}\\) et \\(b^{(k)}\\) les matrices avant l'op\u00e9ration, et \\(A^{(k+1)}\\) et \\(b^{(k+1)}\\) les matrices apr\u00e8s l'op\u00e9ration, on a : \\(A^{(k+1)} = P A^{(k)}\\) et \\(b^{(k+1)} = P b^{(k)}\\) Equivalence Les algorithmes d'\u00e9limination de Gauss peuvent donc s'\u00e9crire comme une succession de multiplications par : - Des matrices triangulaires \\(M\\) . - Des matrices de permutation \\(P\\) . La repr\u00e9sentation matricelle de l'\u00e9limation de Gauss : On peut voir l'algorithme de l'\u00e9limination de Gauss comme la transformation d'un syst\u00e8me \\(A x = b\\) en un syst\u00e8me \\(U x = c\\) avec \\(U\\) une matrice triangulaire sup\u00e9rieure . On a vu qu'\u00e0 chaque \u00e9tape \\(k\\) , les op\u00e9rations de l'algorithme sont \u00e9quivalentes \u00e0 multiplier \\(A^{(k)}\\) et \\(b^{(k)}\\) par une matrice du type : \\(M_k = \\begin{pmatrix} 1 & 0 & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & 1 & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & m_{k+1,k} & 0 & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & m_{n,k} & 0 & \\cdots & 0 & 1 \\end{pmatrix}\\) avec les \\(m_{i,k} = - \\frac{a_{i,k}}{a_{k,k}}\\) pour \\(i>k\\) . A la 1\u00e8re \u00e9tape on a \\(A^{(1)}=A\\) , et \u00e0 la derni\u00e8re \u00e9tape on a \\(A^{(n)}=U\\) . On en d\u00e9duit que l'on peut exprimer \\(U\\) comme : \\(U = A^{(n)} = M_{n-1} A^{(n-1)} = M_{n-1} M_{n-2} A^{(n-2)} = ... = M_{n-1} M_{n-2} ... M_1 A^{(1)} = M A\\) en notant \\(M = M_{n-1} M_{n-2} ... M_1\\) Si on pose \\(L = M^{-1}\\) , alors on peut \u00e9crire \\(A = L U\\) avec \\(L\\) et \\(U\\) des matrices triangulaires inf\u00e9rieure (L) et sup\u00e9rieure (U) . Les matrices \\(M_k\\) sont inversibles, et leurs inverses sont les matrices triangulaires inf\u00e9rieures : \\(L_k = \\begin{pmatrix} 1 & 0 & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 1 & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & 1 & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & -m_{k+1,k} & 0 & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & -m_{n,k} & 0 & \\cdots & 0 & 1 \\end{pmatrix}\\) Il en r\u00e9sulte que \\(L = M^{-1} = L_{n-1} L_{n-2} ... L_1\\) . Dans le cadre des strat\u00e9gies avec pivot partiel ou total, on ajoute les permutations : \\(A = P L U\\) . D'o\u00f9 les d\u00e9compositions \"LU\" et \"PLU\" pr\u00e9sent\u00e9es dans la section suivante.","title":"Equivalence \u00e9limination - factorisation"},{"location":"Chap5_Systemes_lineaires/#decomposition-lu-et-plu","text":"","title":"D\u00e9composition LU et PLU"},{"location":"Chap5_Systemes_lineaires/#idee_2","text":"D\u00e9composition LU : On appelle d\u00e9composition LU (ou factorisation LU) d'une matrice carr\u00e9e \\(A\\) la recherche d'une matrice triangulaire inf\u00e9rieure \\(L\\) et d'une matrice triangulaire sup\u00e9rieure \\(U\\) telles que : \\(A = L U\\) Si \\(A\\) est inversible, alors \\(L\\) et \\(U\\) le sont aussi, et leurs termes diagonaux sont non-nuls. Alors, r\u00e9soudre \\(A x = b\\) revient \u00e0 r\u00e9soudre \\(L U x = b\\) , soit 2 syst\u00e8mes triangulaires : \\(L y = b\\) et \\(U x = y\\) Ces 2 syst\u00e8mes sont faciles \u00e0 r\u00e9soudre : \\(L y = b\\) peut \u00eatre r\u00e9solu avec un algorithme de descente : \\(\\begin{cases} y_1 = \\frac{1}{l_{1,1}} b_1\\\\ y_i = \\frac{1}{l_{i,i}} (b_i - \\displaystyle\\sum_{j=1}^{i-1} l_{i,j} y_j), i=2,...,n \\end{cases}\\) \\(U x = y\\) peut \u00eatre r\u00e9solu avec un algorithme de remont\u00e9e : \\(\\begin{cases} x_n = \\frac{1}{u_{n,n}} y_n\\\\ x_i = \\frac{1}{u_{i,i}} (y_i - \\displaystyle\\sum_{j=i+1}^{n} u_{i,j} x_j), i=n-1,...,1 \\end{cases}\\) Chacun de ces algorithmes n\u00e9cessite \\(n^2\\) op\u00e9rations. On devine alors un des grands avantages de la d\u00e9composition LU compar\u00e9e l'\u00e9limination de Gauss : si on a \\(n\\) syst\u00e8mes \\(A x_1 = b_1\\) , \\(A x_2 = b_2\\) , ... , \\(A x_n = b_n\\) \u00e0 r\u00e9soudre, il est inutile d'appliquer \\(n\\) fois l'\u00e9limination de Gauss. Une fois la d\u00e9composition LU obtenue pour un syst\u00e8me, il suffit d'appliquer les algorithmes de descente / remont\u00e9e aux autres, ce qui est beaucoup moins co\u00fbteux en calculs . La factorisation \\(L U\\) , quand elle existe, n'est pas unique : le syst\u00e8me \\(A = L U\\) est sous-d\u00e9termin\u00e9. On peut rendre la solution unique en donnant des conditions suppl\u00e9mentaires. Par exemple, on peut fixer les \u00e9l\u00e9ments diagonaux de \\(L\\) \u00e0 1. C'est que l'on appelle la factorisation de Gauss . La d\u00e9composition LU (sans pivotage) n'existe pas toujours , m\u00eame si \\(A\\) est inversible. Existence de la d\u00e9composition LU La d\u00e9composition LU existe si et seulement si : toutes les sous-matrices principales \\(A_k = (a_{i,j})_{1 \\leq i,j \\leq k}\\) d'ordre 1 \u00e0 \\(n-1\\) sont inversibles. Si toutes les sous-matrices principales d'ordre 1 \u00e0 \\(n\\) sont inversibles, elle est m\u00eame unique . Il est \u00e0 noter que le fait qu'une sous-matrice principale de \\(A\\) ne soit pas inversible ne signifie pas n\u00e9cessairement que \\(A\\) n'est pas inversible. Pour les matrice inversibles pour lesquelles il n'existe pas de d\u00e9composition LU, d'autres m\u00e9thodes de triangularisation existent, comme la d\u00e9composition PLU . D\u00e9composition PLU : Elle ajoute \u00e0 la d\u00e9composition LU les permutations (pivot partiel ou total), avec une matrice de permutation \\(P\\) : \\(P A = L U\\) Cette fois-ci, r\u00e9soudre \\(A x = b\\) revient \u00e0 r\u00e9soudre \\(P^{-1} L U x = b\\) , soit \\(L U x = P b\\) . On doit donc appliquer les algorithmes de descente et de remont\u00e9e aux syst\u00e8mes triangulaires : \\(L y = P b\\) et \\(U x = y\\) Existence de la d\u00e9composition PLU La d\u00e9composition PLU existe toujours si \\(A\\) est carr\u00e9e et r\u00e9guli\u00e8re (inversible). Autres int\u00e9r\u00eats de la d\u00e9composition LU : En plus de permettre la r\u00e9solution du syst\u00e8me d'\u00e9quation, les d\u00e9compositions LU et PLU permettent de calculer le d\u00e9terminant de \\(A\\) rapidement : \\(det(A) = det(LU) = \\displaystyle\\prod_{k=1}^{n} u_{k,k}\\) \\(det(A) = det(P^{-1}LU) = (-1)^p \\displaystyle\\prod_{k=1}^{n} u_{k,k}\\) avec \\(p\\) permutations La d\u00e9composition LU facilite aussi le calcul de l'inverse de \\(A\\) : En effet, recherche \\(X = A^{-1}\\) revient \u00e0 r\u00e9soudre \\(A X = I\\) , et donc \u00e0 r\u00e9soudre \\(L U X = I\\) . On peut alors trouver \\(A^{-1}\\) en r\u00e9solvant le syst\u00e8me : \\(\\begin{cases} L Y = I\\\\ U X = Y \\end{cases}\\) D\u00e9terminer les \u00e9l\u00e9ments de \\(L\\) et \\(U\\) par la factorisation de Gauss requiert \\(2 n^3/2\\) op\u00e9rations (sans compter les permutations). Il faut ensuite de l'ordre de \\(2 n^2\\) op\u00e9rations pour les algorithmes de remont\u00e9e et de descente.","title":"Id\u00e9e"},{"location":"Chap5_Systemes_lineaires/#algorithme_1","text":"Nous donnerons dans cette section les algorithmes pour les d\u00e9compositions LU puis PLU. Voici sous la forme d'une fonction Python l'algorithme de la d\u00e9composition LU. Cette fonction prend en entr\u00e9e un syst\u00e8me de Cramer : A la matrice des coefficients du syst\u00e8me. b le vecteur du second membre du syst\u00e8me. def decomposition_LU(A): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) : if (m!=n): raise ValueError(\"La matrice A n'est pas carr\u00e9e\") #Copier A pour ne pas modifier la matrice originale : U = np.copy(A) #Initialiser la matrice L comme la matrice identit\u00e9 (nxn) : L = np.eye(n) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur sur la diagonale de la j-\u00e8me colonne : pivot = U[j,j] #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Sauvegarde du coefficient d'\u00e9limination de Gauss dans L : L[k,j] = U[k,j]/pivot #Op\u00e9rations d'\u00e9limination de Gauss sur les lignes de A en #utilisant le pivot : U[k,:] = U[k,:] - U[j,:]*L[k,j] #Renvoyer les matrices L et U : return L,U Voici sous la forme d'une fonction Python l'algorithme de descente li\u00e9e \u00e0 la d\u00e9composition LU. Cette fonction prend en entr\u00e9e : L la matrice \\(L\\) obtenue par la d\u00e9composition LU de \\(A\\) . b le vecteur du second membre du syst\u00e8me. def descente(L,b): #R\u00e9cup\u00e9rer le nombre n d'\u00e9quations / inconnues du syst\u00e8me Ly=b : n = len(L) #Initialiser le vecteur qui contiendra les solutions du syst\u00e8me Ly=b : y = np.zeros(n,dtype=np.float64) #Boucle sur les lignes de la matrice L, de 1 \u00e0 n : for i in range(n): #D\u00e9termination de la i-\u00e8me inconnue : y[i] = (b[i]-sum(L[i,:i]*y[:i]))/L[i,i] #Renvoyer le vecteur contenant les solutions du syst\u00e8me Ly=b : return y Voici sous la forme d'une fonction Python l'algorithme de remont\u00e9e li\u00e9e \u00e0 la d\u00e9composition LU. Cette fonction prend en entr\u00e9e : U la matrice \\(U\\) obtenue par la d\u00e9composition LU de \\(A\\) . y le vecteur des solution du syst\u00e8me \\(L y = b\\) obtenue par l'algorithme de descente. def remontee(U,y): #R\u00e9cup\u00e9rer le nombre n d'\u00e9quations / inconnues du syst\u00e8me Ux=y : n = len(U) #Initialiser le vecteur qui contiendra les solutions du syst\u00e8me Ux=y : x = np.zeros(n,dtype=np.float64) #Boucle sur les lignes de la matrice U, de n \u00e0 1 : for i in range(n-1,-1,-1): #D\u00e9termination de la i-\u00e8me inconnue : x[i] = (y[i]-sum(U[i,i+1:n]*x[i+1:n]))/U[i,i] #Renvoyer le vecteur contenant les solutions du syst\u00e8me Ux=y : return x Voici maintenant sous la forme d'une fonction Python l'algorithme de la d\u00e9composition PLU. Comme pour la d\u00e9composition LU, cette fonction prend en entr\u00e9e un syst\u00e8me de Cramer : A la matrice des coefficients du syst\u00e8me. b le vecteur du second membre du syst\u00e8me. def decomposition_PLU(A): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) : if (m!=n): raise ValueError(\"La matrice A n'est pas carr\u00e9e\") #Copier A pour ne pas modifier la matrice originale : U = np.copy(A) #Initialiser la matrice L comme la matrice identit\u00e9 (nxn) : L = np.eye(n) #Initialiser la matrice de permutation P comme la matrice identit\u00e9 (nxn) : P = np.eye(n) #Boucle sur les colonnes de la matrice A, jusqu'\u00e0 l'avant-derni\u00e8re : for j in range(n-1): #S\u00e9lection du pivot comme \u00e9tant la valeur maximale en absolu sur la colonne, #sur la j-i\u00e8me ligne ou en dessous : idx_pivot = np.argmax(abs(U[j:,j]))+j #Indice de la ligne du pivot pivot = U[idx_pivot,j] #Valeur du pivot #On v\u00e9rifie que le pivot n'est pas nul : if pivot!=0: #Si le pivot n'est pas sur la j-i\u00e8me ligne, \u00e9changer la j-i\u00e8me et la #ligne du pivot : if idx_pivot!=j: U[[j,idx_pivot]] = U[[idx_pivot,j]] #Pour la matrice A P[[j,idx_pivot]] = P[[idx_pivot,j]] #Pour la matrice P #Boucle sur les lignes sous le pivot : for k in range(j+1,n): #Sauvegarde du coefficient d'\u00e9limination de Gauss dans L : L[k,j] = U[k,j]/pivot #Op\u00e9rations d'\u00e9limination de Gauss sur les lignes de A en #utilisant le pivot : U[k,:] = U[k,:] - U[j,:]*L[k,j] #Renvoyer les matrices P, L et U : return P,L,U Apr\u00e8s une d\u00e9composition PLU, il ne faudra pas oublier d'appliquer l'algorithme de descente \u00e0 \\(P b\\) au lieu de \\(b\\) .","title":"Algorithme"},{"location":"Chap5_Systemes_lineaires/#exemple_3","text":"Nous allons appliquer l'algorithme de d\u00e9composion LU, puis PLU \u00e0 notre probl\u00e8me exemple D\u00e9composition LU : Appliquons d'abord l'algorithme de d\u00e9composion LU. On rappelle que nous avons initialement le syst\u00e8me de Cramer \\(A x = b\\) suivant : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) Nous initialisons \\(L\\) et \\(U\\) de la mani\u00e8re suivante : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix}\\) 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant sur la diagonale de \\(U\\) : -5000. On r\u00e9alise les op\u00e9rations suivantes : On ajoute \\(\\frac{10000}{-5000} = -2\\) en ligne 2 dans \\(L\\) . On applique \\(L_2 = L_2 - L_1 \\times -2\\) sur \\(U\\) . On ajoute \\(\\frac{-4000}{-5000} = 0.8\\) en ligne 3 dans \\(L\\) . On applique \\(L_3 = L_3 - L_1 \\times 0.8\\) sur \\(U\\) . \\(L\\) et \\(U\\) deviennent alors : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0.8 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 0 & -34000 & -18000 \\\\ 0 & 26400 & -2800 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9l\u00e9ctionne le pivot comme \u00e9tant sur la diagonale de \\(U\\) : -34000. On r\u00e9alise les op\u00e9rations suivantes : On ajoute \\(\\frac{26400}{-34000} = -0.7765\\) en ligne 3 dans \\(L\\) . On applique \\(L_3 = L_3 - L_2 \\times -0.7765\\) sur \\(U\\) . \\(L\\) et \\(U\\) deviennent alors : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ -2 & 1 & 0 \\\\ 0.8 & -0.7764 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 0 & -34000 & -18000 \\\\ 0 & 0 & -16776.47 \\end{pmatrix}\\) On retrouve bien pour \\(U\\) la matrice triangulaire obtenue avec l'\u00e9limination de Gauss sans pivotage, et pour \\(L\\) les coefficients ayant servi \u00e0 l'\u00e9limination. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On peut v\u00e9rifier que \\(det(A) = -5000 \\times -34000 \\times -16776.47 \\approx -2852000000000\\) On d\u00e9duit les solutions du syst\u00e8me par les algorithmes : de descente pour \\(L y = b\\) \\(\\begin{cases} -42977000\\\\ -5404000 - (-2 \\times -42977000) = -91358000\\\\ -43586000 - (0.8 \\times -42977000) - (-0.7764 \\times -91358000) = -80141200 \\end{cases}\\) puis de remont\u00e9e pour \\(U x = y\\) \\(\\begin{cases} z_r = \\frac{-80141200}{-16776.47} = 4777\\\\ y_r = \\frac{1}{-34000} (-91358000 - (-18000 \\times z_r)) = 158\\\\ x_r = \\frac{1}{-5000} (-42977000 - (-4000 \\times y_r) - (-18000 \\times z_r)) = 4205 \\end{cases}\\) R\u00e9-utilisation de la d\u00e9composition LU : Imaginons maintenant qu'un r\u00e9cepteur situ\u00e9 au Beffroi de Lille, de coordonn\u00e9es ECEF approximatives \\((x_r,y_r,z_r) = (4048,217,4908)\\) , utilise les m\u00eames satellites GPS de m\u00eames coordonn\u00e9es ECEF pour se positionner. Le vecteur \\(b\\) devient alors : \\(\\begin{pmatrix} -43778000\\\\ -8166000\\\\ -43036000 \\end{pmatrix}\\) Les coefficients de la matrice \\(A\\) ne d\u00e9pendant que de la position des satellites GPS, ils restent inchang\u00e9s. On peut donc r\u00e9-utiliser la d\u00e9composition LU pr\u00e9c\u00e9dente pour r\u00e9soudre ce syst\u00e8me : \\(\\begin{cases} -43778000\\\\ -8166000 - (-2 \\times -43778000) = -95722000\\\\ -43036000 - (0.8 \\times -43778000) - (-0.7764 \\times -95722000) = -82338917.65 \\end{cases}\\) puis \\(\\begin{cases} z_r = \\frac{-82338917.65}{-16776.47} = 4048\\\\ y_r = \\frac{1}{-34000} (-95722000 - (-18000 \\times z_r)) = 217\\\\ x_r = \\frac{1}{-5000} (-43778000 - (-4000 \\times y_r) - (-18000 \\times z_r)) = 4908 \\end{cases}\\) On retrouve bien la position de notre r\u00e9cepteur lillois. D\u00e9composition PLU : Appliquons \u00e0 pr\u00e9sent au syst\u00e8me l'algorithme de d\u00e9composion PLU. On rappelle que nous avons initialement le syst\u00e8me de Cramer \\(A x = b\\) suivant : \\(\\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -42977000\\\\ -5404000\\\\ -43586000 \\end{pmatrix}\\) Nous initialisons \\(P\\) , \\(L\\) et \\(U\\) de la mani\u00e8re suivante : \\(P = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix}\\) 1\u00e8re it\u00e9ration : nous commen\u00e7ons par la colonne 1. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale de \\(U\\) : 10000. On \u00e9change la ligne 1 et la ligne 2 pour faire passer le pivot sur la diagonale. \\(U\\) et \\(P\\) deviennent alors : \\(P = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} 10000 & 2000 & -10000 \\\\ -5000 & -18000 & -4000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix}\\) On r\u00e9alise les op\u00e9rations suivantes : On ajoute \\(\\frac{-5000}{10000} = -0.5\\) en ligne 2 dans \\(L\\) . On applique \\(L_2 = L_2 - L_1 \\times -0.5\\) sur \\(U\\) . On ajoute \\(\\frac{-5000}{10000} = -0.4\\) en ligne 3 dans \\(L\\) . On applique \\(L_3 = L_3 - L_1 \\times -0.4\\) sur \\(U\\) . \\(L\\) et \\(U\\) deviennent alors : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ -0.5 & 1 & 0 \\\\ -0.4 & 0 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} 10000 & 2000 & -10000 \\\\ 0 & -17000 & -9000 \\\\ 0 & 12800 & -10000 \\end{pmatrix}\\) 2nde it\u00e9ration : nous continuons avec la colonne 2. On s\u00e9lectionne le pivot comme \u00e9tant le maximum en valeur absolue sur la colonne, sur ou sous la diagonale de \\(U\\) : -17000. On r\u00e9alise les op\u00e9rations suivantes : On ajoute \\(\\frac{12800}{-17000} = -0.7529\\) en ligne 3 dans \\(L\\) . On applique \\(L_3 = L_3 - L_2 \\times -0.7529\\) sur \\(U\\) . \\(L\\) et \\(U\\) deviennent alors : \\(L = \\begin{pmatrix} 1 & 0 & 0 \\\\ -0.5 & 1 & 0 \\\\ -0.4 & -0.7529 & 1 \\end{pmatrix}\\) \\(U = \\begin{pmatrix} 10000 & 2000 & -10000 \\\\ 0 & -17000 & -9000 \\\\ 0 & 0 & -16776.47 \\end{pmatrix}\\) On retrouve bien pour \\(U\\) la matrice triangulaire obtenue avec l'\u00e9limination de Gauss avec pivot partiel, et pour \\(L\\) les coefficients ayant servi \u00e0 l'\u00e9limination. Voici un r\u00e9sum\u00e9 des diff\u00e9rentes \u00e9tapes de l'algorithme sous la forme d'une animation : On peut v\u00e9rifier que \\(det(A) = (-1)^1 \\times 10000 \\times -17000 \\times -16776.47 \\approx -2852000000000\\) On d\u00e9duit les solutions du syst\u00e8me : Tout d'abord, on calcule \\(P b = \\begin{pmatrix} -5404000\\\\ -42977000\\\\ -43586000 \\end{pmatrix}\\) puis on applique \u00e0 \\(L y = P b\\) l' algorithme de descente \\(\\begin{cases} -5404000\\\\ -42977000 - (-0.5 \\times -5404000) = -45679000\\\\ -43586000 - (-0.4 \\times -5404000) - (-0.7529 \\times -45679000) = -80141200 \\end{cases}\\) et on applique l'algorithme de remont\u00e9e \u00e0 \\(U x =y\\) \\(\\begin{cases} z_r = \\frac{-80141200}{-16776.47} = 4777\\\\ y_r = \\frac{1}{-17000} (-45679000 - (-9000 \\times z_r)) = 158\\\\ x_r = \\frac{1}{10000} (-5404000 - (2000 \\times y_r) - (-10000 \\times z_r)) = 4205 \\end{cases}\\) Exercice : Imaginons maintenant qu'un r\u00e9cepteur situ\u00e9 au Cirque de Gavarnie, de coordonn\u00e9es ECEF approximatives \\((x_r,y_r,z_r) = (4695,0,4303)\\) , utilise les m\u00eames satellites GPS de m\u00eames coordonn\u00e9es ECEF pour se positionner. En vous servant des programmes Python pr\u00e9c\u00e9dents, d\u00e9terminez les nouvelles valeurs de \\(b\\) , puis utilisez la d\u00e9composition PLU obtenue pr\u00e9c\u00e9demment pour estimer la position du r\u00e9cepteur. Vous devriez retrouver la position ECEF du Cirque de Gavarnie.","title":"Exemple"},{"location":"Chap5_Systemes_lineaires/#autres-decompositions-qr-et-cholesky","text":"Il existe d'autres types de d\u00e9composition moins co\u00fbteux en temps de calcul, qui peuvent s'appliquer \u00e0 des syst\u00e8mes particulier. Nous pr\u00e9senterons rapidement ici les d\u00e9compositions QR et de Cholesky.","title":"Autres d\u00e9compositions (QR et Cholesky)"},{"location":"Chap5_Systemes_lineaires/#decomposition-qr","text":"Th\u00e9or\u00e8me Si \\(A\\) est une matrice r\u00e9elle inversible , il existe un unique couple \\((Q,R)\\) avec \\(Q\\) une matrice orthogonale (c'est-\u00e0-dire \\(QQ^T=Q^TQ=I\\) ) et \\(R\\) une matrice triangulaire sup\u00e9rieure dont les \u00e9l\u00e9ments diagonaux sont positifs, tel que \\(A = QR\\) La d\u00e9composition QR consiste \u00e0 d\u00e9composer la matrice \\(A\\) de cette fa\u00e7on : \\(A = QR\\) . R\u00e9soudre le syst\u00e8me \\(A x = b\\) revient alors \u00e0 r\u00e9soudre : \\(\\begin{cases} Q y = b\\\\ R x = y \\end{cases}\\) soit \\(\\begin{cases} y = Q^{-1} b = Q^T b\\\\ R x = y \\end{cases}\\)","title":"D\u00e9composition QR"},{"location":"Chap5_Systemes_lineaires/#decomposition-de-cholesky","text":"Th\u00e9or\u00e8me Si \\(A\\) est sym\u00e9trique d\u00e9finie positive , (c'est-\u00e0-dire une matrice carr\u00e9e \u00e9gale \u00e0 sa transpos\u00e9e, positive et inversible) il existe une matrice triangulaire inf\u00e9rieure \\(L\\) telle que \\(A = LL^T\\) La d\u00e9composition de Cholesky consiste \u00e0 d\u00e9composer la matrice \\(A\\) de cette fa\u00e7on : \\(A = LL^T\\) . On peut imposer que les \u00e9l\u00e9ments diagonaux de \\(L\\) soient positifs, pour obtenir l' unicit\u00e9 de la factorisation. R\u00e9soudre le syst\u00e8me \\(A x = b\\) revient alors \u00e0 appliquer les algorithmes de remont\u00e9e et de descente pour r\u00e9soudre \\(LL^T x = b\\) .","title":"D\u00e9composition de Cholesky"},{"location":"Chap5_Systemes_lineaires/#methodes-iteratives","text":"Les m\u00e9thodes directes donnent la solution du syst\u00e8me \\(A x = b\\) en un nombre fini d'op\u00e9rations, mais : Si la taille du syst\u00e8me est \u00e9lev\u00e9e, le nombre d'op\u00e9rations est important, ce qui augmente les erreurs. Si la taille du syst\u00e8me est \u00e9lev\u00e9e, le nombre de coefficients \u00e0 mettre en m\u00e9moire l'est aussi. Elles utilisent des propri\u00e9t\u00e9s math\u00e9matiques n\u00e9cessitant un calcul exact : il est donc difficile de tenir compte des erreurs de calculs. Les m\u00e9thodes it\u00e9ratives sont g\u00e9n\u00e9ralement plus rapides et n\u00e9cessitent moins de m\u00e9moire. L'id\u00e9e est de construire une suite \\((x^{(n)})_{n \\geq 0}\\) qui converge vers la solution \\(x\\) . Pour construire une telle suite, on va s'appuyer sur la lin\u00e9arit\u00e9 du probl\u00e8me en d\u00e9composant \\(A\\) en une matrice facilement inversible et un reste .","title":"M\u00e9thodes it\u00e9ratives"},{"location":"Chap5_Systemes_lineaires/#principe-et-convergence","text":"Les m\u00e9thodes it\u00e9ratives vont donc d\u00e9composer la matrice \\(A\\) du syst\u00e8me d'\u00e9quations de la mani\u00e8re suivante : \\(A = M-(M-A) = M-N\\) avec \\(M\\) facilement inversible . Alors, r\u00e9soudre \\(A x = b\\) revient \u00e0 r\u00e9soudre \\(M x = N x + b\\) . On calcule la suite de vecteurs \\((x^{(k)})_k\\) \u00e0 partir d'un vecteur de d\u00e9part \\(x^{(0)}\\) et de la relation de r\u00e9currence : \\(M x^{(k+1)} = N x^{(k)} + b\\) soit \\(x^{(k+1)} = M^{-1} N x^{(k)} + M^{-1} b\\) En posant \\(C = M^{-1} N\\) et \\(d = M^{-1} b\\) , la suite devient : \\(x^{(k+1)} = C x^{(k)} + d\\) La solution \\(x\\) est alors le point fixe de la fonction lin\u00e9aire \\(g(x) = C x + d\\) . Reste alors \u00e0 v\u00e9rifier sa convergence. Th\u00e9or\u00e8me Soit \\(C\\) une matrice carr\u00e9e de taille \\(n \\times n\\) , s'il existe une norme matricielle telle que \\(\\lVert C \\lVert < 1\\) , alors : - \\(g(x) = C x + d\\) admet un point fixe unique \\(x\\) . - La suite \\((x^{(k)})_k\\) telle que \\(x^{(k+1)} = g(x^{(k)})\\) converge vers ce point fixe quel que soit vecteur de d\u00e9part \\(x^{(0)}\\) . \\(C\\) est appel\u00e9e matrice d'it\u00e9ration . Le vecteur d'erreur absolue \u00e0 l'it\u00e9ration \\(k\\) est : \\(e^{(k)} = x^{(k)} - x\\) On en d\u00e9duit que : \\(e^{(k)} = x^{(k)} - x = (C x^{k-1} + d) - x = C x^{(k-1)} - C x = C (x^{(k-1)} - x) = C e^{(k-1)}\\) d'o\u00f9 \\(e^{(k)} = C^k e^{(0)}\\) On peut donc majorer l'erreur de la mani\u00e8re suivante : \\(\\|e^{(k)}\\| \\leq \\|C\\|^k \\|e^{(0)}\\|\\) d'o\u00f9 \\(\\lim\\limits_{k \\to \\infty} e^{(k)} = 0\\) et donc \\(\\lim\\limits_{k \\to \\infty} x^{(k)} = x\\) . Plus \\(\\|C\\|\\) est petit, moins il est n\u00e9cessaire d'effectuer des it\u00e9rations pour r\u00e9duire l'erreur initiale d'un facteur donn\u00e9. Convergence Pour \u00e9tablir la convergence de \\(x^{(k+1)} = C x^{(k)} + d\\) , il suffit de montrer que \\(C\\) v\u00e9rifie : \\(\\rho(C) = max_{1 \\leq i \\leq n} \\lVert \\lambda_i \\lVert < 1\\) Cette condition suffisante est aussi n\u00e9cessaire , car on a toujours \\(\\rho(C) \\leq \\|C\\|\\) . On peut calculer \u00e0 chaque it\u00e9ration le vecteur r\u00e9sidu : \\(r^{(k)} = b - A x^{(k)}\\) Voici alors 2 crit\u00e8res d'arr\u00eat possibles : \\(\\|r^{(k)}\\| \\leq \\epsilon \\|b\\|\\) ou \\(\\|x^{(k)}-x^{(k-1)}\\| \\leq \\epsilon \\|x^{(k-1)}\\|\\) L'avantage des m\u00e9thodes it\u00e9ratives est leur co\u00fbt : de l'ordre de \\(n^2\\) , \u00e0 comparer au \\(\\frac{2}{3} n^3\\) des m\u00e9thodes directes. Toute la difficult\u00e9 des m\u00e9thodes it\u00e9rative est dans leurs conditions de convergence , qui restreignent leur application \u00e0 certains syst\u00e8mes.","title":"Principe et convergence"},{"location":"Chap5_Systemes_lineaires/#methode-de-jacobi","text":"","title":"M\u00e9thode de Jacobi"},{"location":"Chap5_Systemes_lineaires/#idee_3","text":"La m\u00e9thode de Jacobi d\u00e9compose la matrice \\(A\\) en \\(A = M-N\\) et calcule la suite \\(x^{(k+1)} = M^{-1} N x^{(k)} + M^{-1} b\\) avec : \\(M = D\\) avec \\(D\\) la matrice des \u00e9l\u00e9ments diagonaux de \\(A\\) . \\(N = E+F\\) avec \\(-E\\) la matrice des \u00e9l\u00e9ments sous-diagonaux de \\(A\\) , et \\(-F\\) la matrice des \u00e9l\u00e9ments sur-diagonaux de \\(A\\) . On d\u00e9compose donc \\(A\\) en \\(A = D-E-F\\) avec \\(D\\) diagonale , avec \\(E\\) triangulaire inf\u00e9rieure , et avec \\(F\\) triangulaire sup\u00e9rieure . \\(D = \\begin{pmatrix} a_{1,1} & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & a_{2,2} & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & a_{3,3} & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & \\cdots & a_{n-1,n-1} & 0\\\\ 0 & 0 & 0 & \\cdots & 0 & a_{n,n} \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 & \\cdots & 0 & 0 & 0\\\\ a_{2,1} & 0 & 0 & \\cdots & 0 & 0 & 0\\\\ a_{3,1} & a_{3,2} & 0 & \\cdots & 0 & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ a_{n-2,1} & a_{n-2,2} & a_{n-2,3} & \\cdots & 0 & 0 & 0\\\\ a_{n-1,1} & a_{n-1,2} & a_{n-1,3} & \\cdots & a_{n-1,n-2} & 0 & 0\\\\ a_{n,1} & a_{n,2} & a_{n,3} & \\cdots & a_{n,n-2} & a_{n,n-1} & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & a_{1,2} & a_{1,3} & a_{1,4} & \\cdots & a_{1,n-1} & a_{1,n}\\\\ 0 & 0 & a_{2,3} & a_{2,4} & \\cdots & a_{2,n-1} & a_{2,n}\\\\ 0 & 0 & 0 & a_{3,4} & \\cdots & a_{3,n-1} & a_{3,n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & 0 & \\cdots & a_{n-2,n-1} & a_{n-2,n}\\\\ 0 & 0 & 0 & 0 & \\cdots & 0 & a_{n-1,n}\\\\ 0 & 0 & 0 & 0 & \\cdots & 0 & 0 \\end{pmatrix}\\) On appelle matrice de Jacobi : \\(J = M^{-1} N = D^{-1} (E+F)\\) La suite dont on cherche la limite est alors : \\(x^{(k+1)} = D^{-1} (E+F) x^{(k)} + D^{-1} b\\) On remarque que \\(D^{-1} (E+F) = D^{-1} (D-A) = I - D^{-1}A\\) \\(D^{-1} = \\begin{pmatrix} 1/a_{1,1} & 0 & 0 & \\cdots & 0 & 0\\\\ 0 & 1/a_{2,2} & 0 & \\cdots & 0 & 0\\\\ 0 & 0 & 1/a_{3,3} & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & \\cdots & 1/a_{n-1,n-1} & 0\\\\ 0 & 0 & 0 & \\cdots & 0 & 1/a_{n,n} \\end{pmatrix}\\) et \\(D^{-1} (E+F) = \\begin{pmatrix} 0 & -a_{1,2}/a_{1,1} & -a_{1,3}/a_{1,1} & \\cdots & -a_{1,n-1}/a_{1,1} & -a_{1,n}/a_{1,1}\\\\ -a_{2,1}/a_{2,2} & 0 & -a_{2,3}/a_{2,2} & \\cdots & -a_{2,n-1}/a_{2,2} & -a_{2,n}/a_{2,2}\\\\ -a_{3,1}/a_{3,3} & -a_{3,2}/a_{3,3} & 0 & \\cdots & -a_{3,n-1}/a_{3,3} & -a_{3,n}/a_{3,3}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ -a_{n-1,1}/a_{n-1,n-1} & -a_{n-1,2}/a_{n-1,n-1} & -a_{n-1,3}/a_{n-1,n-1} & \\cdots & 0 & -a_{n-1,n}/a_{n-1,n-1}\\\\ -a_{n,1}/a_{n,n} & -a_{n,2}/a_{n,n} & -a_{n,3}/a_{n,n} & \\cdots & -a_{n,n-1}/a_{n,n} & 0 \\end{pmatrix}\\) On peut facilement en d\u00e9duire qu'\u00e0 l'it\u00e9ration \\(k\\) , et pour chaque ligne \\(i\\) : \\(D^{-1} (E+F) x_i^{(k)} = - \\frac{\\displaystyle\\sum_{j=1,j \\neq i}^{n} a_{i,j} x_j^{(k)}}{a_{i,i}}\\) et \\(D^{-1} b_i = \\frac{b_i}{a_{i,i}}\\) D'o\u00f9 \\(x_i^{(k)} = \\frac{1}{a_{i,i}} (b_i - \\displaystyle\\sum_{j=1,j \\neq i}^{n} a_{i,j} x_j^{(k-1)})\\) Une condition n\u00e9cessaire et suffisante de convergence : \\(D\\) doit \u00eatre inversible (tous les \u00e9l\u00e9ments diagonaux de \\(A\\) doivent \u00eatre non-nuls). \\(\\rho(J) < 1\\) (la valeur propre de \\(J\\) de plus grand module est < 1). Dans la pratique, on peut utiliser le th\u00e9or\u00e8me suivant : Th\u00e9or\u00e8me de convergence La m\u00e9thode de Jacobi converge quelque soit \\(x^{(0)}\\) pour les syst\u00e8mes lin\u00e9aires dont \\(A\\) est \u00e0 diagonale strictement dominante , c'est-\u00e0-dire : \\(\\forall 1 \\leq i \\leq n\\) , \\(\\mid a_{i,i} \\mid > \\displaystyle\\sum_{j=1,j \\neq i}^{n} \\mid a_{i,j} \\mid\\) La m\u00e9thode de Jacobi converge lentement , mais on peut facilement la rendre plus rapide : c'est l'id\u00e9e de la m\u00e9thode de Gauss-Seidel, pr\u00e9sent\u00e9e dans la suite de ce chapitre.","title":"Id\u00e9e"},{"location":"Chap5_Systemes_lineaires/#algorithme_2","text":"Voici sous la forme d'une fonction Python l'algorithme de la m\u00e9thode de Jacobi. Elle prend en entr\u00e9e : A et b les matrices du syst\u00e8me lin\u00e9aire \u00e0 r\u00e9soudre. x_0 la valeur initiale de la suite convergeant vers la solution. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la solution du syst\u00e8me. r_n le r\u00e9sidu. def jacobi(A,b,x_0,n_max,e): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #R\u00e9cup\u00e9rer les valeurs sur la diagonale de A : A_diag = np.diag(A) #Cr\u00e9er la matrice diagonale D, ayant les m\u00eames valeurs que la diagonale de A : D = np.diag(A_diag) #Cr\u00e9er la matrice EF, ayant des z\u00e9ros sur sa diagonale et les m\u00eames valeurs #que A partout ailleurs : EF = A-D #V\u00e9rifier si la matrice A est \u00e0 diagonale strictement dominante : for i in range(n): if sum(abs(EF[:,i]))>=abs(A_diag[i]): print(\"Attention : la matrice A n'est pas \u00e0 diagonale strictement dominante\") #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n-1 x_n = (b-np.dot(EF,x_n_old))/A_diag #Estimation de la solution \u00e0 l'it\u00e9ration n r_n = np.dot(A,x_n)-b #R\u00e9sidu #It\u00e9rations de l'algorithme de Jacobi #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(np.linalg.norm(x_n-x_n_old,ord=2)>e)and(np.linalg.norm(r_n,ord=2)>e): #Mettre \u00e0 jour l'estimation de la solution : x_n_old = np.copy(x_n) #It\u00e9ration n x_n = (b-np.dot(EF,x_n_old))/A_diag #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Mettre \u00e0 jour le r\u00e9sidu : r_n = np.dot(A,x_n)-b #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : return x_n,r_n","title":"Algorithme"},{"location":"Chap5_Systemes_lineaires/#exemple_4","text":"On rappelle que la matrice \\(A\\) de notre probl\u00e8me exemple est : \\(A = \\begin{pmatrix} -5000 & -18000 & -4000 \\\\ 10000 & 2000 & -10000 \\\\ -4000 & 12000 & -6000 \\end{pmatrix}\\) Cette matrice n'est pas \u00e0 diagonale strictement dominante : \\(\\mid -5000 \\mid \\leq \\mid -18000 \\mid + \\mid -4000 \\mid\\) \\(\\mid 2000 \\mid \\leq \\mid 10000 \\mid + \\mid -10000 \\mid\\) \\(\\mid -6000 \\mid \\leq \\mid -4000 \\mid + \\mid 12000 \\mid\\) La convergence de la m\u00e9thode de Jacobi n'est donc pas assur\u00e9e pour une initialisation quelconque. Et en effet, en appliquant l'algorithme de Jacobi \u00e0 notre syst\u00e8me, on observe que la suite diverge. Ceci illustre bien la limite des m\u00e9thodes it\u00e9ratives : leurs conditions de convergence. Mais admettons que notre r\u00e9cepteur situ\u00e9 \u00e0 l'UFR des Sciences de l'UVSQ utilise les signaux provenant de 4 autres satellites, de positions ECEF : \\((x_{s1},y_{s1},z_{s1}) = (15000,13000,18000)\\) \\((x_{s2},y_{s2},z_{s2}) = (1000,6000,24000)\\) \\((x_{s3},y_{s3},z_{s3}) = (19000,2000,19000)\\) \\((x_{s4},y_{s4},z_{s4}) = (12000,12000,26000)\\) Le syst\u00e8me \u00e0 r\u00e9soudre devient alors : \\(\\begin{pmatrix} -14000 & -7000 & 6000 \\\\ 4000 & -11000 & 1000 \\\\ -3000 & -1000 & 8000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -31314000\\\\ 19859000\\\\ 25443000 \\end{pmatrix}\\) Cette fois-ci, \\(A\\) est \u00e0 diagonale strictement dominante : \\(\\mid -14000 \\mid > \\mid -7000 \\mid + \\mid 6000 \\mid\\) \\(\\mid -11000 \\mid > \\mid 4000 \\mid + \\mid 1000 \\mid\\) \\(\\mid 8000 \\mid > \\mid -3000 \\mid + \\mid -1000 \\mid\\) La convergence de la m\u00e9thode de Jacobi est donc assur\u00e9e pour une initialisation quelconque. Nous allons donc appliquer la m\u00e9thode \u00e0 ce syst\u00e8me. Nous choisirons : \\(x^{(0)} =\\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}\\) On d\u00e9compose la matrice \\(A = D-E-F\\) avec : \\(D = \\begin{pmatrix} -14000 & 0 & 0 \\\\ 0 & -11000 & 0 \\\\ 0 & 0 & 8000 \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 \\\\ 4000 & 0 & 0 \\\\ -3000 & -1000 & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & -7000 & 6000 \\\\ 0 & 0 & 1000 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) On a donc : \\(D^{-1} = \\begin{pmatrix} \\frac{1}{-14000} & 0 & 0 \\\\ 0 & \\frac{1}{-11000} & 0 \\\\ 0 & 0 & \\frac{1}{8000} \\end{pmatrix}\\) \\(E+F = \\begin{pmatrix} 0 & 7000 & -6000 \\\\ -4000 & 0 & -1000 \\\\ 3000 & 1000 & 0 \\end{pmatrix}\\) Soit : \\(D^{(-1)}(E+F) = \\begin{pmatrix} 0 & \\frac{7000}{-14000} & \\frac{-6000}{-14000} \\\\ \\frac{-4000}{-10000} & 0 & \\frac{-1000}{-10000} \\\\ \\frac{3000}{8000} & \\frac{1000}{8000} & 0 \\end{pmatrix}\\) La suite de la m\u00e9thode de Jacobi convergeant vers la solution est alors : \\(\\begin{cases} x_r^{(k+1)} = \\frac{1}{-14000} (-31314000 + 7000 y_r^{(k)} - 6000 z_r^{(k)})\\\\ y_r^{(k+1)} = \\frac{1}{-11000} (19859000 - 4000 x_r^{(k)} - 1000 z_r^{(k)})\\\\ z_r^{(k+1)} = \\frac{1}{8000} (25443000 + 3000 x_r^{(k)} + 1000 y_r^{(k)}) \\end{cases}\\) Pour atteindre une pr\u00e9cision de \\(10^{-3}\\) , on a besoin d'it\u00e9rer 10 fois la m\u00e9thode de Jacobi : It\u00e9ration \\(k\\) \\(x_r^{(k)}\\) \\(y_r^{(k)}\\) \\(z_r^{(k)}\\) 0 0.0000 0.0000 0.0000 1 2236.7143 -1805.3636 3180.3750 2 4502.4140 -702.8880 3793.4724 3 4213.9322 176.7389 4780.9192 4 4197.3102 161.6044 4782.6919 5 4205.6372 155.7212 4774.5669 6 4205.0967 158.0105 4776.9541 7 4204.9751 158.0310 4777.0376 8 4205.0006 157.9943 4776.9945 9 4205.0005 157.9997 4776.9995 10 4204.9999 158.0001 4777.0001 On obtient bien la solution recherch\u00e9e avec la pr\u00e9cision attendue. Exercice : Calculez la matrice de Jacobi correspondant \u00e0 notre probl\u00e8me exemple. Est-il attendu que la m\u00e9thode ne converge pas pour ce syst\u00e8me ? D\u00e9montrez-le.","title":"Exemple"},{"location":"Chap5_Systemes_lineaires/#methode-de-gauss-seidel","text":"","title":"M\u00e9thode de Gauss-Seidel"},{"location":"Chap5_Systemes_lineaires/#idee_4","text":"La m\u00e9thode de Gauss-Seidel d\u00e9compose la matrice \\(A\\) en \\(A = M-N\\) et calcule la suite \\(x^{(k+1)} = M^{-1} N x^{(k)} + M^{-1} b\\) avec : \\(M = D-E\\) avec \\(D\\) la matrice des \u00e9l\u00e9ments diagonaux de \\(A\\) , et \\(-E\\) la matrice des \u00e9l\u00e9ments sous-diagonaux de \\(A\\) . \\(N = F\\) avec \\(-F\\) la matrice des \u00e9l\u00e9ments sur-diagonaux de \\(A\\) . On appelle matrice de Gauss-Seidel : \\(G = (D-E)^{-1} F = M^{-1}N\\) La suite dont on cherche la limite est alors : \\(x^{(k+1)} = (D-E)^{-1} F x^{(k)} + (D-E)^{-1} b\\) On remarque que : \\((D-E) x^{(k+1)} = F x^{(k)} + b\\) avec \\(D-E = \\begin{pmatrix} a_{1,1} & 0 & 0 & \\cdots & 0 & 0\\\\ a_{2,1} & a_{2,2} & 0 & \\cdots & 0 & 0\\\\ a_{3,1} & a_{3,2} & a_{3,3} & \\cdots & 0 & 0\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ a_{n-1,1} & a_{n-1,2} & a_{n-1,3} & \\cdots & a_{n-1,n-1} & 0\\\\ a_{n,1} & a_{n,2} & a_{n,3} & \\cdots & a_{n,n-1} & a_{n,n} \\end{pmatrix}\\) et \\(F = \\begin{pmatrix} 0 & -a_{1,2} & -a_{1,3} & \\cdots & -a_{1,n-1} & -a_{1,n}\\\\ 0 & 0 & -a_{2,3} & \\cdots & -a_{2,n-1} & -a_{2,n}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ 0 & 0 & 0 & \\cdots & -a_{n-2,n-1} & -a_{n-2,n}\\\\ 0 & 0 & 0 & \\cdots & 0 & -a_{n-1,n}\\\\ 0 & 0 & 0 & \\cdots & 0 & 0 \\end{pmatrix}\\) On peut facilement en d\u00e9duire qu'\u00e0 l'it\u00e9ration \\(k\\) , et pour chaque ligne \\(i\\) : \\((D-E) x_i^{(k+1)} = \\displaystyle\\sum_{j=1}^{i} a_{i,j} x_j^{(k+1)} = a_{i,i} x_i^{(k+1)} + \\displaystyle\\sum_{j=1}^{i-1} a_{i,j} x_j^{(k+1)}\\) et \\(F x_i^{(k)} = -\\displaystyle\\sum_{j=i+1}^{n} a_{i,j} x_j^{(k)}\\) D'o\u00f9 \\(x_i^{(k)} = \\frac{1}{a_{i,i}} (b_i - \\displaystyle\\sum_{j=1}^{i-1} a_{i,j} x_j^{(k)} - \\displaystyle\\sum_{j=i+1}^{n} a_{i,j} x_j^{(k-1)})\\) Une condition n\u00e9cessaire et suffisante de convergence : \\(D-E\\) doit \u00eatre inversible (tous les \u00e9l\u00e9ments diagonaux de \\(A\\) doivent \u00eatre non-nuls). \\(\\rho(G) < 1\\) (la valeur propre de \\(G\\) de plus grand module est < 1). Dans la pratique, on peut utiliser les th\u00e9or\u00e8mes suivant : Th\u00e9or\u00e8me de convergence 1 La m\u00e9thode de Gauss-Seidel converge quelque soit \\(x^{(0)}\\) pour les syst\u00e8mes lin\u00e9aires dont \\(A\\) est \u00e0 diagonale strictement dominante , c'est-\u00e0-dire : \\(\\forall 1 \\leq i \\leq n\\) , \\(\\mid a_{i,i} \\mid > \\displaystyle\\sum_{j=1,j \\neq i}^{n} \\mid a_{i,j} \\mid\\) Th\u00e9or\u00e8me de convergence 2 Si \\(A\\) est une matrice sym\u00e9trique d\u00e9finie positive alors la m\u00e9thode de Gauss-Seidel converge. Contrairement \u00e0 la m\u00e9thode de Jacobi, la mise \u00e0 jour des composantes de \\(x^{(k)}\\) se fait de mani\u00e8re s\u00e9quentielle et non en parall\u00e8le. Ceci rend la convergence de la m\u00e9thode de Gauss-Seidel plus rapide que celle de Jacobi. Cependant, la m\u00e9thode de Gauss-Seidel peut osciller autour de la solution. Pour limiter ce ph\u00e9nom\u00e8ne et donc acc\u00e9l\u00e9rer la convergence, on peut utiliser les m\u00e9thodes de relaxation , d\u00e9crite dans la suite de ce chapitre.","title":"Id\u00e9e"},{"location":"Chap5_Systemes_lineaires/#algorithme_3","text":"Voici sous la forme d'une fonction Python l'algorithme de la m\u00e9thode de Gauss-Seidel. Elle prend en entr\u00e9e : A et b les matrices du syst\u00e8me lin\u00e9aire \u00e0 r\u00e9soudre. x_0 la valeur initiale de la suite convergeant vers la solution. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la solution du syst\u00e8me. r_n le r\u00e9sidu. def gauss_seidel(A,b,x_0,n_max,e): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #R\u00e9cup\u00e9rer les valeurs sur la diagonale de A : A_diag = np.diag(A) #Cr\u00e9er la matrice diagonale D, ayant les m\u00eames valeurs que la diagonale de A : D = np.diag(A_diag) #Cr\u00e9er la matrice A-D, ayant des z\u00e9ros sur sa diagonale et les m\u00eames valeurs #que A partout ailleurs : AD = A-D #V\u00e9rifier si la matrice A est \u00e0 diagonale strictement dominante : for i in range(n): if sum(abs(AD[:,i]))>=abs(A_diag[i]): print(\"Attention : la matrice A n'est pas \u00e0 diagonale strictement dominante\") break #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n-1 x_n = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n for i in range(len(b)): x_n[i] = (b[i]-np.dot(A[i,:i],x_n[:i])-np.dot(A[i,i+1:],x_n[i+1:]))/A[i,i] r_n = np.dot(A,x_n)-b #R\u00e9sidu #It\u00e9rations de l'algorithme de Gauss-Seidel #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(np.linalg.norm(x_n-x_n_old,ord=2)>e)and(np.linalg.norm(r_n,ord=2)>e): #Mettre \u00e0 jour l'estimation de la solution : x_n_old = np.copy(x_n) #It\u00e9ration n for i in range(len(b)): x_n[i] = (b[i]-np.dot(A[i,:i],x_n[:i])-np.dot(A[i,i+1:],x_n[i+1:]))/A[i,i] #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : r_n = np.dot(A,x_n)-b #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : return x_n,r_n","title":"Algorithme"},{"location":"Chap5_Systemes_lineaires/#exemple_5","text":"Nous allons cette fois-ci appliquer la m\u00e9thode de Gauss-Seidel au syst\u00e8me : \\(\\begin{pmatrix} -14000 & -7000 & 6000 \\\\ 4000 & -11000 & 1000 \\\\ -3000 & -1000 & 8000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -31314000\\\\ 19859000\\\\ 25443000 \\end{pmatrix}\\) Comme montr\u00e9 pr\u00e9c\u00e9demment, la matrice \\(A\\) est \u00e0 diagonale strictement dominante. La convergence de la m\u00e9thode de Gauss-Seidel est donc assur\u00e9e pour une initialisation quelconque. Nous allons donc appliquer la m\u00e9thode \u00e0 ce syst\u00e8me. Nous choisirons : \\(x^{(0)} =\\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}\\) On d\u00e9compose la matrice \\(A = D-E-F\\) avec : \\(D = \\begin{pmatrix} -14000 & 0 & 0 \\\\ 0 & -11000 & 0 \\\\ 0 & 0 & 8000 \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 \\\\ 4000 & 0 & 0 \\\\ -3000 & -1000 & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & -7000 & 6000 \\\\ 0 & 0 & 1000 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) On a donc : \\(D-E = \\begin{pmatrix} -14000 & 0 & 0 \\\\ 4000 & -11000 & 0 \\\\ -3000 & -1000 & 8000 \\end{pmatrix}\\) La suite de la m\u00e9thode de Gauss-Seidel convergeant vers la solution est alors : \\(\\begin{cases} x_r^{(k+1)} = \\frac{1}{-14000} (-31314000 + 7000 y_r^{(k)} - 6000 z_r^{(k)})\\\\ y_r^{(k+1)} = \\frac{1}{-11000} (19859000 - 4000 x_r^{(k+1)} - 1000 z_r^{(k)})\\\\ z_r^{(k+1)} = \\frac{1}{8000} (25443000 + 3000 x_r^{(k+1)} + 1000 y_r^{(k+1)}) \\end{cases}\\) Pour atteindre une pr\u00e9cision de \\(10^{-3}\\) , on a besoin d'it\u00e9rer 9 fois la m\u00e9thode de Gauss-Seidel : It\u00e9ration \\(k\\) \\(x_r^{(k)}\\) \\(y_r^{(k)}\\) \\(z_r^{(k)}\\) 0 0.0000 0.0000 0.0000 1 2236.7143 -992.0130 3895.1412 2 4402.0670 149.4918 4849.8366 3 4240.4698 177.5196 4792.7411 4 4201.9864 158.3352 4775.9118 5 4204.3660 157.6705 4776.7211 6 4205.0452 157.9911 4777.0158 7 4205.0112 158.0055 4777.0049 8 4204.9993 158.0002 4776.9998 9 4204.9998 157.9999 4776.9999 On obtient bien la solution recherch\u00e9e avec la pr\u00e9cision attendue, pour une it\u00e9ration de moins que la m\u00e9thode de Jacobi. Exercice : Essayez d'appliquer la m\u00e9thode Gauss-Seidel au syst\u00e8me d\u00e9finit dans le probl\u00e8me exemple. La m\u00e9thode converge-t-elle ? Ce r\u00e9sultat \u00e9tait-il pr\u00e9visible ? Prouvez-le.","title":"Exemple"},{"location":"Chap5_Systemes_lineaires/#methode-de-relaxation","text":"","title":"M\u00e9thode de relaxation"},{"location":"Chap5_Systemes_lineaires/#idee_5","text":"La m\u00e9thode de la relaxation est une variante de l'algorithme de Gauss-Seidel. L'id\u00e9e est de calculer un vecteur \\(x_G^{(k+1)}\\) \u00e0 l'aide de de la m\u00e9thode de Gauss-Seidel, puis de calculer \\(x^{(k+1)}\\) comme la moyenne pond\u00e9r\u00e9e : \\(x^{(k+1)} = \\omega x_G^{(k+1)} + (1-\\omega) x^{(k)}\\) avec \\(\\omega > 0\\) L'algorithme de relaxation d\u00e9compose donc la matrice \\(A = M-N\\) et calcule la suite \\(x^{(k+1)} = M^{-1} N x^{(k)} + M^{-1} b\\) avec : \\(M = \\frac{1}{\\omega} D - E\\) avec \\(D\\) la matrice des \u00e9l\u00e9ments diagonaux de \\(A\\) et \\(E\\) la matrice des \u00e9l\u00e9ments sous-diagonaux de \\(A\\) . \\(N = (\\frac{1}{\\omega}-1) D + F\\) avec \\(D\\) la matrice des \u00e9l\u00e9ments diagonaux de \\(A\\) et \\(F\\) la matrice des \u00e9l\u00e9ments sur-diagonaux de \\(A\\) . On appelle \\(\\omega\\) le param\u00e8tre de relaxation . Il y a diff\u00e9rents cas de figure suivant la valeur de \\(\\omega\\) choisie : Si \\(\\omega < 1\\) on parle de sous-relaxation : la m\u00e9thode est plus \"prudente\" que Gauss-Seidel. Si \\(\\omega > 1\\) on parle de sur-relaxation : la m\u00e9thode est plus \"plus ambitieuse\" que Gauss-Seidel. Si \\(\\omega = 1\\) on retrouve la m\u00e9thode de Gauss-Seidel. La matrice d'it\u00e9ration est ici : \\(C = (D - \\omega E)^{-1} ((1-\\omega) D + \\omega F)\\) On cherche \u00e0 optimiser \\(\\omega\\) pour que \\(\\rho(C)\\) soit minimal. On sait qu'\u00e0 l'it\u00e9ration \\(k\\) , et pour chaque ligne \\(i\\) : \\(x_i^{(k)} = \\omega x_Gi^{(k)} + (1-\\omega) x_i^{(k-1)}\\) On peut donc en d\u00e9duire que : \\(x_i^{(k)} = \\frac{\\omega}{a_{i,i}} (b_i - \\displaystyle\\sum_{j=1}^{i-1} a_{i,j} x_j^{(k)} - \\displaystyle\\sum_{j=i+1}^{n} a_{i,j} x_j^{(k-1)} - (\\frac{1}{\\omega}-1) a_{i,i} x_i^{(k-1)})\\) Voici un th\u00e9or\u00e8me utile pour v\u00e9rifier la convergence de la m\u00e9thode : Th\u00e9or\u00e8me - Si la m\u00e9thode de la relaxation converge, alors \\(0 < \\omega < 2\\) . - Si \\(A\\) est \u00e0 diagonale strictement dominante, la m\u00e9thode de la relaxation converge pour tout vecteur de d\u00e9part \\(x^{(0)}\\) si \\(0 < \\omega \\leq 1\\) . - Si \\(A\\) est sym\u00e9trique d\u00e9finie positive, la m\u00e9thode de relaxation converge pour tout vecteur de d\u00e9part \\(x^{(0)}\\) si \\(0 < \\omega < 2\\) .","title":"Id\u00e9e"},{"location":"Chap5_Systemes_lineaires/#algorithme_4","text":"Voici sous la forme d'une fonction Python l'algorithme de la m\u00e9thode de la relaxation. Elle prend en entr\u00e9e : A et b les matrices du syst\u00e8me lin\u00e9aire \u00e0 r\u00e9soudre. omega le param\u00e8tre de relaxation. x_0 la valeur initiale de la suite convergeant vers la solution. n_max le nombre maximum d'it\u00e9rations. e la pr\u00e9cision d\u00e9sir\u00e9e. On notera les variables \u00e0 l'it\u00e9ration n : x_n l'estimation de la solution du syst\u00e8me. r_n le r\u00e9sidu. def relaxation(A,b,omega,x_0,n_max,e): #R\u00e9cup\u00e9rer les dimensions de la matrice A : m,n = np.shape(A) #V\u00e9rification des dimensions de A (nxn) et b (n) : if (m!=n)or(len(b)!=n): raise ValueError(\"Le syst\u00e8me n'est pas de Cramer\") #R\u00e9cup\u00e9rer les valeurs sur la diagonale de A : A_diag = np.diag(A) #Cr\u00e9er la matrice diagonale D, ayant les m\u00eames valeurs que la diagonale de A : D = np.diag(A_diag) #Cr\u00e9er la matrice A-D, ayant des z\u00e9ros sur sa diagonale et les m\u00eames valeurs #que A partout ailleurs : AD = A-D #V\u00e9rifier si la matrice A est \u00e0 diagonale strictement dominante : for i in range(n): if sum(abs(AD[:,i]))>=abs(A_diag[i]): print(\"Attention : la matrice A n'est pas \u00e0 diagonale strictement dominante\") break #Initialisation des variables : n = 0 #Nombre d'it\u00e9rations x_n_old = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n-1 x_n = np.copy(x_0) #Estimation de la solution \u00e0 l'it\u00e9ration n for i in range(len(b)): x_n[i] = omega*(b[i]-np.dot(A[i,:i],x_n[:i])-np.dot(A[i,i+1:],x_n_old[i+1:]))/A[i,i] + (1-omega)*x_n_old[i] r_n = np.dot(A,x_n)-b #R\u00e9sidu #It\u00e9rations de l'algorithme de la relaxation #tant qu'une des conditions d'arr\u00eat n'est pas atteinte : while (n<n_max)and(np.linalg.norm(x_n-x_n_old,ord=2)>e)and(np.linalg.norm(r_n,ord=2)>e): #Mettre \u00e0 jour l'estimation de la solution : x_n_old = np.copy(x_n) #It\u00e9ration n for i in range(len(b)): x_n[i] = omega*(b[i]-np.dot(A[i,:i],x_n[:i])-np.dot(A[i,i+1:],x_n_old[i+1:]))/A[i,i] + (1-omega)*x_n_old[i] #Iteration n+1 #Incr\u00e9menter le nombre d'it\u00e9rations : n+=1 #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : r_n = np.dot(A,x_n)-b #Renvoyer l'estimation de la solution du syst\u00e8me et le r\u00e9sidu : return x_n,r_n","title":"Algorithme"},{"location":"Chap5_Systemes_lineaires/#exemple_6","text":"On peut d\u00e9montrer que pour le syst\u00e8me de notre probl\u00e8me-exemple, la m\u00e9thode de la relaxation diverge pour tout param\u00e8tre \\(\\omega\\) dans \\(]0,2]\\) . Consid\u00e9rons donc \u00e0 nouveau le syst\u00e8me : \\(\\begin{pmatrix} -14000 & -7000 & 6000 \\\\ 4000 & -11000 & 1000 \\\\ -3000 & -1000 & 8000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -31314000\\\\ 19859000\\\\ 25443000 \\end{pmatrix}\\) On a montr\u00e9 pr\u00e9c\u00e9demment que dans ce cas on peut d\u00e9composer la matrice \\(A = D-E-F\\) avec : \\(D = \\begin{pmatrix} -14000 & 0 & 0 \\\\ 0 & -11000 & 0 \\\\ 0 & 0 & 8000 \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 \\\\ 4000 & 0 & 0 \\\\ -3000 & -1000 & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & -7000 & 6000 \\\\ 0 & 0 & 1000 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) On peut alors d\u00e9terminer pour diff\u00e9rentes valeurs du param\u00e8tre de relaxation \\(\\omega\\) sur \\(]0,2]\\) le rayon spectral de la matrice d'it\u00e9ration \\(C = (D - \\omega E)^{-1} ((1-\\omega) D + \\omega F)\\) : On observe que le rayon spectral est minimal pour \\(\\omega \\approx 1\\) . On en d\u00e9duit que la m\u00e9thode de Gauss-Seidel est optimale pour la r\u00e9solution de ce syst\u00e8me. Mais ce n'est pas toujours le cas ! Mettons que la position ECEF du satellite 1 ne soit pas \\((x_{s1},y_{s1},z_{s1}) = (15000,13000,18000)\\) , mais plut\u00f4t \\((x_{s1},y_{s1},z_{s1}) = (23000,7000,20000)\\) . Le syst\u00e8me \u00e0 r\u00e9soudre devient alors : \\(\\begin{pmatrix} -22000 & -1000 & 4000 \\\\ -4000 & -5000 & -1000 \\\\ -11000 & 5000 & 6000 \\end{pmatrix} \\begin{pmatrix} x_r\\\\ y_r\\\\ z_r \\end{pmatrix} = \\begin{pmatrix} -73560000\\\\ -22387000\\\\ -16803000 \\end{pmatrix}\\) On remarque que \\(A\\) n'est pas \u00e0 diagonale strictement dominante. Il faudra donc v\u00e9rifier que le rayon spectral de la matrice d'it\u00e9ration choisie est inf\u00e9rieur \u00e0 1 pour assurer la convergence de la m\u00e9thode de la relaxation. On d\u00e9compose la matrice \\(A = D-E-F\\) avec : \\(D = \\begin{pmatrix} -22000 & 0 & 0 \\\\ 0 & -5000 & 0 \\\\ 0 & 0 & 6000 \\end{pmatrix}\\) \\(-E = \\begin{pmatrix} 0 & 0 & 0 \\\\ -4000 & 0 & 0 \\\\ -11000 & 5000 & 0 \\end{pmatrix}\\) \\(-F = \\begin{pmatrix} 0 & -1000 & 4000 \\\\ 0 & 0 & -1000 \\\\ 0 & 0 & 0 \\end{pmatrix}\\) On a donc : \\(D-E = \\begin{pmatrix} -22000 & 0 & 0 \\\\ -4000 & -5000 & 0 \\\\ -11000 & 5000 & 6000 \\end{pmatrix}\\) Si nous d\u00e9terminons pour diff\u00e9rentes valeurs de \\(\\omega\\) sur \\(]0,2]\\) le rayon spectral de la matrice d'it\u00e9ration \\(C = (D - \\omega E)^{-1} ((1-\\omega) D + \\omega F)\\) de ce syst\u00e8me, nous obtenons : Cette fois-ci le rayon spectral est minimal pour \\(\\omega \\approx 1.25\\) . On en d\u00e9duit que la formule de r\u00e9currence optimale pour r\u00e9soudre ce syst\u00e8me est : \\(x^{(k+1)} = 1.25 \\times x_G^{(k+1)} - 0.25 \\times x^{(k)}\\) Ce qui donne : \\(\\begin{cases} x_r^{(k+1)} = \\frac{1.25}{-22000} (-73560000 + 1000 y_r^{(k)} - 4000 z_r^{(k)}) - 0.25 x_r^{(k)}\\\\ y_r^{(k+1)} = \\frac{1.25}{-4000} (-22387000 + 4000 x_r^{(k+1)} + 1000 z_r^{(k)}) - 0.25 y_r^{(k)}\\\\ z_r^{(k+1)} = \\frac{1.25}{-11000} (-16803000 + 11000 x_r^{(k+1)} - 5000 y_r^{(k+1)}) - 0.25 z_r^{(k)} \\end{cases}\\) On peut v\u00e9rifier que cette formule converge plus rapidement que la m\u00e9thode de Gauss-Seidel ( \\(\\omega = 1\\) ). Nous choisirons encore : \\(x^{(0)} =\\begin{pmatrix} 0\\\\ 0\\\\ 0 \\end{pmatrix}\\) Dans le cas de Gauss-Seidel ( \\(\\omega = 1\\) ), pour obtenir la solution avec une pr\u00e9cision de \\(10^{-3}\\) , 39 it\u00e9rations sont n\u00e9cessaires : It\u00e9ration \\(k\\) \\(x_r^{(k)}\\) \\(y_r^{(k)}\\) \\(z_r^{(k)}\\) 0 0.0000 0.0000 0.0000 1 3343.6364 1802.4909 1827.4242 2 3593.9639 1236.7440 2757.8138 3 3788.8414 894.7641 3400.0725 4 3921.1603 660.4573 3837.9128 5 4011.4179 500.6831 4136.5302 6 4072.9744 391.7144 4340.1911 7 4114.9568 317.3963 4479.0906 8 4143.5894 266.7104 4573.8218 9 4163.1171 232.1419 4638.4298 10 4176.4353 208.5658 4682.4933 11 4185.5185 192.4865 4712.5452 12 4191.7134 181.5203 4733.0410 13 4195.9383 174.0411 4747.0194 14 4198.8198 168.9403 4756.5528 15 4200.7850 165.4614 4763.0547 16 4202.1253 163.0888 4767.4892 17 4203.0394 161.4706 4770.5135 18 4203.6629 160.3670 4772.5761 19 4204.0881 159.6143 4773.9828 20 4204.3780 159.1010 4774.9423 21 4204.5758 158.7509 4775.5966 22 4204.7107 158.5121 4776.0429 23 4204.8027 158.3493 4776.3472 24 4204.8654 158.2382 4776.5548 25 4204.9082 158.1625 4776.6964 26 4204.9374 158.1108 4776.7929 27 4204.9573 158.0756 4776.8588 28 4204.9709 158.0515 4776.9037 29 4204.9801 158.0351 4776.9343 30 4204.9865 158.0240 4776.9552 31 4204.9908 158.0163 4776.9694 32 4204.9937 158.0112 4776.9792 33 4204.9957 158.0076 4776.9858 34 4204.9971 158.0052 4776.9903 35 4204.9980 158.0035 4776.9934 36 4204.9986 158.0024 4776.9955 37 4204.9991 158.0016 4776.9969 38 4204.9994 158.0011 4776.9979 39 4204.9996 158.0008 4776.9986 Pour atteindre la m\u00eame pr\u00e9cision sur la solution avec notre formule optimale, seules 15 it\u00e9rations sont n\u00e9cessaires : It\u00e9ration \\(k\\) \\(x_r^{(k)}\\) \\(y_r^{(k)}\\) \\(z_r^{(k)}\\) 0 0.0000 0.0000 0.0000 1 4179.5455 1417.2045 4601.2453 2 4099.8737 -7.7361 4752.6660 3 4235.1679 175.3496 4834.1459 4 4209.4599 134.9162 4796.9799 5 4209.7375 154.0385 4786.9883 6 4206.3108 155.1825 4780.4417 7 4205.6146 157.2294 4778.3508 8 4205.1971 157.6578 4777.4705 9 4205.0771 157.8908 4777.1728 10 4205.0262 157.9579 4777.0607 11 4205.0096 157.9857 4777.0218 12 4205.0034 157.9948 4777.0077 13 4205.0012 157.9982 4777.0027 14 4205.0004 157.9993 4777.0010 15 4205.0002 157.9998 4777.0003 On peut v\u00e9rifier pour diff\u00e9rentes valeurs de \\(\\omega\\) qu'il s'agit bien du nombre d'it\u00e9rations minimum possible. Exercice : Pour le dernier cas pr\u00e9sent\u00e9 dans cet exemple, calculez le rayon spectral de la matrice de convergence pour \\(\\omega = 1\\) et \\(\\omega = 1.25\\) . Calculez le ratio entre ces 2 valeurs. On a trouv\u00e9 pr\u00e9c\u00e9demment que pour \\(\\omega = 1.25\\) , la m\u00e9thode converge environ 2.7 fois plus rapidement que pour Gauss-Seidel. Qu'en concluez vous ?","title":"Exemple"},{"location":"Chap5_Systemes_lineaires/#conclusion","text":"En fonction du rang , et du d\u00e9terminant de la matrice \\(A\\) , un syst\u00e8me lin\u00e9aire \\(A x = b\\) admet une unique, z\u00e9ro ou une infinit\u00e9 de solutions . M\u00eame si un syst\u00e8me admet une solution, elle peut-\u00eatre tr\u00e8s sensible aux perturbations de \\(A\\) ou \\(b\\) . La stabilit\u00e9 d'une solution peut \u00eatre estim\u00e9e en calculant le conditionnement de \\(A\\) . La r\u00e8gle de Cramer donne une expression de explicite de la solution d'un syst\u00e8me, mais elle est inapplicable au-del\u00e0 de 4 inconnues. Les m\u00e9thodes directes d'\u00e9limination transforment la matrice \\(A\\) par une suite d'op\u00e9rations arithm\u00e9tiques afin de se ramener \u00e0 un syst\u00e8me plus simple \u00e0 r\u00e9soudre : triangulaire ou diagonal . Les m\u00e9thodes directes de factorisation d\u00e9composent la matrice \\(A\\) en le produit d'une matrice triangulaire sup\u00e9rieure et une matrice triangulaire inf\u00e9rieure , afin de rendre la r\u00e9solution plus simple. Les m\u00e9thodes it\u00e9ratives construisent une suite convergeant vers la solution \\(x\\) \u00e0 partir de \\(A\\) et \\(b\\) pour n'importe quelle initialisation. Les m\u00e9thodes directes donnent une solution pour tout syst\u00e8me de Cramer, mais sont lourdes en calculs / m\u00e9moire et les erreurs peuvent \u00eatre amplifi\u00e9es \u00e0 chaque op\u00e9ration. Les m\u00e9thodes it\u00e9ratives requi\u00e8rent un nombre fini d'op\u00e9rations, mais elles sont donc soumises aux erreurs de troncature, et les conditions de leur convergence ne sont pas \u00e9videntes.","title":"Conclusion"}]}